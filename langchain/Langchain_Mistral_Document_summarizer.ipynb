{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/langchain/Langchain_Mistral_Document_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3LFaHkF9AgK",
        "outputId": "185be883-8b50-4e04-e938-8ce777ce0707"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq torch==2.1 --progress-bar off\n",
        "!pip install -qqq transformers==4.34.0 --progress-bar off\n",
        "!pip install -qqq accelerate==0.23.0 --progress-bar off\n",
        "!pip install -qqq bitsandbytes==0.41.1 --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veaIAYHJQJCv",
        "outputId": "27094c79-1349-4450-9cbd-5c0a78101c00"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers spacy langchain trl datasets pypdf -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip_uhu9L9kof",
        "outputId": "83b1cd8f-47bd-4a8e-c7fa-8c3f9e493ca6"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
        "\n",
        "splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6rUNtCzKsyR",
        "outputId": "f2be111a-0e5b-4ab9-af73-3508c1edd72a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxPVavEf9klX"
      },
      "outputs": [],
      "source": [
        "file = \"/content/drive/MyDrive/data/text.txt\"\n",
        "with open(file) as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZyth_yb9kiL"
      },
      "outputs": [],
      "source": [
        "count_start_and_stop_tokens = 2\n",
        "text_token_count = splitter.count_tokens(text=text) - count_start_and_stop_tokens\n",
        "\n",
        "token_multiplier = splitter.maximum_tokens_per_chunk // text_token_count + 1\n",
        "text_to_split = text * token_multiplier\n",
        "\n",
        "text_chunks = splitter.split_text(text=text_to_split)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBG9-dhi9guK",
        "outputId": "3393b5c7-dc7c-4284-c206-61b0600d9c05"
      },
      "outputs": [],
      "source": [
        "text_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOzbh9bSM4zJ"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import transformers\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "um-qU1-KRO6e",
        "outputId": "fbc9a137-5b5f-49cf-99a4-b47450da7c56"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "30ebcfd7cb3441189e10fe4a9f2316df",
            "a9750c6b07cb41eb8f2ded32275b59d3",
            "df7cbe3e208d497ba45fe873822a4a79",
            "6eeda88d227c4866b3df661630519d97",
            "5c27c11ce3e24bc289aab687757dca7d",
            "8331a92a465041f2b9520175b58b999b",
            "132cb06650fa471b806b68e8aec5dde0",
            "18164d2c929442c3bea02018b8534990",
            "e2eda8da65544d25b22221d53d5269b6",
            "0c0553ee65a34cb0bb122bbca79f472d",
            "41980d5d6e23457f8e5c07337f409dd4"
          ]
        },
        "id": "tQiwrBYFM76m",
        "outputId": "b1e66739-412c-4b1d-a10a-a64ef4c634fb"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", load_in_4bit=True, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6nR9TgSRD8y",
        "outputId": "78c9e898-8d63-49cf-9c2b-cac0f0fd28b7"
      },
      "outputs": [],
      "source": [
        "# Test prompt 1\n",
        "vegeterian_recipe_prompt = \"\"\"### Instruction: Act as a gourmet chef.\n",
        "I have a friend coming over who is a vegetarian.\n",
        "I want to impress my friend with a special vegetarian dish.\n",
        "What do you recommend?\n",
        "\n",
        "Give me two options, along with the whole recipe for each.\n",
        "\n",
        " ### Answer:\n",
        " \"\"\"\n",
        "\n",
        "encoded_instruction = tokenizer(vegeterian_recipe_prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "model_inputs = encoded_instruction.to(device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRnI6iovRD5O"
      },
      "outputs": [],
      "source": [
        "PROMPT= \"\"\" ### Instruction: Act as a currency expert.\n",
        "\n",
        "### Question:\n",
        "Explain to me how Digital Currency works. Assume that I am a 5-year-old child.\n",
        "\n",
        " ### Answer:\n",
        " \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwAT--D1RDzk",
        "outputId": "c3a9a4f2-585e-47e3-fea2-56dcaa67ca82"
      },
      "outputs": [],
      "source": [
        "encoded_instruction = tokenizer(PROMPT,  return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "model_inputs = encoded_instruction.to(device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIJRqFKpRDwp"
      },
      "outputs": [],
      "source": [
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=1000,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKXgZs0bTPB7"
      },
      "outputs": [],
      "source": [
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6esCQRsTO-5"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "### Instruction: Act as a LARGE Language Models Expert. I will ask you a QUESTION and give you an AUDIENCE PERSONA, and you will respond with an answer easily understandable by the AUDIENCE PERSONA.\n",
        "\n",
        "### AUDIENCE PERSONA:\n",
        "{audience_persona}\n",
        "\n",
        "### QUESTION:\n",
        "{question}\n",
        "\n",
        " ### Answer:\n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g71vPUVcTO7w"
      },
      "outputs": [],
      "source": [
        "# Create prompt from prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"audience_persona\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "# Create llm chain\n",
        "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxsh7MaSTO4j",
        "outputId": "83dbb6e8-0133-4ead-9675-df903c423b48"
      },
      "outputs": [],
      "source": [
        "# Input query to LLM\n",
        "input_text = {\n",
        "    \"audience_persona\": \"A 5 year-old child\",\n",
        "    \"question\": \"Explain how Large Language Models work.\"\n",
        "}\n",
        "\n",
        "# Generate and print LLM response\n",
        "response = llm_chain(input_text)\n",
        "\n",
        "\n",
        "#print(response_text['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "4JfI7ctWTiDv",
        "outputId": "ca51b1fe-2dc3-4ff3-a6e1-75f63d266854"
      },
      "outputs": [],
      "source": [
        "response['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M48uBi6kVgKJ",
        "outputId": "f5567ab5-95ca-41a0-ca81-fdfc3e8fd27b"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-large\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcvV2EzKV7vM"
      },
      "outputs": [],
      "source": [
        "text_summarization_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"summarization\",\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "      max_new_tokens=1000,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "\n",
        "\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=text_summarization_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1YwAPe_WGVB"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Please provide a summary of the following text\n",
        "\n",
        "TEXT:\n",
        "Philosophy (from Greek: φιλοσοφία, philosophia, 'love of wisdom') \\\n",
        "is the systematized study of general and fundamental questions, \\\n",
        "such as those about existence, reason, knowledge, values, mind, and language. \\\n",
        "Some sources claim the term was coined by Pythagoras (c. 570 – c. 495 BCE), \\\n",
        "although this theory is disputed by some. Philosophical methods include questioning, \\\n",
        "critical discussion, rational argument, and systematic presentation.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe0xpHrfWGSK",
        "outputId": "8c367867-2537-4d05-c22e-6bc5104bb59c"
      },
      "outputs": [],
      "source": [
        "num_tokens = llm.get_num_tokens(prompt)\n",
        "print (f\"Our prompt has {num_tokens} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDINUCLWWGPP",
        "outputId": "a1e4379d-1bd0-4099-d3dc-45b62b1cc165"
      },
      "outputs": [],
      "source": [
        "output = llm(prompt)\n",
        "print (output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seHN-zL6WGL0"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-A9W_TDaigM"
      },
      "outputs": [],
      "source": [
        "# Load the book\n",
        "loader = PyPDFLoader(\"/content/drive/MyDrive/data/IntoThinAirBook.pdf\")\n",
        "pages = loader.load()\n",
        "\n",
        "# Cut out the open and closing parts\n",
        "pages = pages[26:277]\n",
        "\n",
        "# Combine the pages, and replace the tabs with spaces\n",
        "text = \"\"\n",
        "\n",
        "for page in pages:\n",
        "    text += page.page_content\n",
        "\n",
        "text = text.replace('\\t', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXwsYrpgctjI",
        "outputId": "f2f2f3e8-a594-4f03-db5a-cf5ea830ffd8"
      },
      "outputs": [],
      "source": [
        "num_tokens = llm.get_num_tokens(text)\n",
        "\n",
        "print (f\"This book has {num_tokens} tokens in it\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWw5tszAaidJ"
      },
      "outputs": [],
      "source": [
        "# Loaders\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Splitters\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Model\n",
        "\n",
        "\n",
        "# Embedding Support\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Summarizer we'll use for Map Reduce\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "# Data Science\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m2NyppqaiaF"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \"\\t\"], chunk_size=10000, chunk_overlap=3000)\n",
        "\n",
        "docs = text_splitter.create_documents([text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-05Q_Up1aiWq",
        "outputId": "dc1263fb-6841-4f7d-e13c-06be247acced"
      },
      "outputs": [],
      "source": [
        "num_documents = len(docs)\n",
        "\n",
        "print (f\"Now our book is split up into {num_documents} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu1UAOQRc8ZF"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "vectors = embeddings.embed_documents([x.page_content for x in docs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAO8olPDdnx1",
        "outputId": "419beb4b-c25b-40bb-c5fe-cbd172be8058"
      },
      "outputs": [],
      "source": [
        "embeddings.Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwEouf2hdCt4",
        "outputId": "f8debbba-533b-436f-a105-b3a5401317b3"
      },
      "outputs": [],
      "source": [
        "len(vectors[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwzwlQKddCqJ",
        "outputId": "8c61ef59-3e37-4a2f-f4ac-5b37694bbd6a"
      },
      "outputs": [],
      "source": [
        "# Assuming 'embeddings' is a list or array of 1536-dimensional embeddings\n",
        "\n",
        "# Choose the number of clusters, this can be adjusted based on the book's content.\n",
        "# I played around and found ~10 was the best.\n",
        "# Usually if you have 10 passages from a book you can tell what it's about\n",
        "num_clusters = 11\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgH9qi4ydCmz",
        "outputId": "96c76f8d-781b-4090-fd86-877e48390611"
      },
      "outputs": [],
      "source": [
        "kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "OZGydweGeAlv",
        "outputId": "441a6941-76a8-4c4e-a502-804f237cba41"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Taking out the warnings\n",
        "import warnings\n",
        "from warnings import simplefilter\n",
        "\n",
        "# Filter out FutureWarnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Perform t-SNE and reduce to 2 dimensions\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "reduced_data_tsne = tsne.fit_transform(np.array(vectors))\n",
        "\n",
        "# Plot the reduced data\n",
        "plt.scatter(reduced_data_tsne[:, 0], reduced_data_tsne[:, 1], c=kmeans.labels_)\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.title('Book Embeddings Clustered')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6DOKSe1eRaF"
      },
      "outputs": [],
      "source": [
        "# Find the closest embeddings to the centroids\n",
        "\n",
        "# Create an empty list that will hold your closest points\n",
        "closest_indices = []\n",
        "\n",
        "# Loop through the number of clusters you have\n",
        "for i in range(num_clusters):\n",
        "\n",
        "    # Get the list of distances from that particular cluster center\n",
        "    distances = np.linalg.norm(vectors - kmeans.cluster_centers_[i], axis=1)\n",
        "\n",
        "    # Find the list position of the closest one (using argmin to find the smallest distance)\n",
        "    closest_index = np.argmin(distances)\n",
        "\n",
        "    # Append that position to your closest indices list\n",
        "    closest_indices.append(closest_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5KD4VvreZiQ",
        "outputId": "91e3609e-b7f2-4958-a2ff-47ef2ff91f81"
      },
      "outputs": [],
      "source": [
        "selected_indices = sorted(closest_indices)\n",
        "selected_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7VRN1gJeZfK"
      },
      "outputs": [],
      "source": [
        "map_prompt = \"\"\"\n",
        "You will be given a single passage of a book. This section will be enclosed in triple backticks (```)\n",
        "Your goal is to give a summary of this section so that a reader will have a full understanding of what happened.\n",
        "Your response should be at least three paragraphs and fully encompass what was said in the passage.\n",
        "\n",
        "```{text}```\n",
        "FULL SUMMARY:\n",
        "\"\"\"\n",
        "map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyS0OUyLeZb8"
      },
      "outputs": [],
      "source": [
        "map_chain = load_summarize_chain(llm=llm,\n",
        "                             chain_type=\"stuff\",\n",
        "                             prompt=map_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0DVEitpevuN"
      },
      "outputs": [],
      "source": [
        "selected_docs = [docs[doc] for doc in selected_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJER-0Y5evrs",
        "outputId": "63fcce22-4a68-4f74-af46-6e67d197897c"
      },
      "outputs": [],
      "source": [
        "# Make an empty list to hold your summaries\n",
        "summary_list = []\n",
        "\n",
        "# Loop through a range of the lenght of your selected docs\n",
        "for i, doc in enumerate(selected_docs):\n",
        "\n",
        "    # Go get a summary of the chunk\n",
        "    chunk_summary = map_chain.run([doc])\n",
        "\n",
        "    # Append that summary to your list\n",
        "    summary_list.append(chunk_summary)\n",
        "\n",
        "    print (f\"Summary #{i} (chunk #{selected_indices[i]}) - Preview: {chunk_summary[:250]} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jZZILejevoi",
        "outputId": "c7cc200b-6143-4310-af81-bbd961014561"
      },
      "outputs": [],
      "source": [
        "summaries = \"\\n\".join(summary_list)\n",
        "\n",
        "# Convert it back to a document\n",
        "summaries = Document(page_content=summaries)\n",
        "\n",
        "print (f\"Your total summary has {llm.get_num_tokens(summaries.page_content)} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RB2eHR0khaRN"
      },
      "outputs": [],
      "source": [
        "text_summarization_pipeline2 = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"summarization\",\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "      max_new_tokens=1000,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "\n",
        ")\n",
        "llm2 = HuggingFacePipeline(pipeline=text_summarization_pipeline2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGymo8I-evlc"
      },
      "outputs": [],
      "source": [
        "combine_prompt = \"\"\"\n",
        "You will be given a series of summaries from a book. The summaries will be enclosed in triple backticks (```)\n",
        "Your goal is to give a verbose summary of what happened in the story.\n",
        "The reader should be able to grasp what happened in the book.\n",
        "\n",
        "```{text}```\n",
        "VERBOSE SUMMARY:\n",
        "\"\"\"\n",
        "combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGygLaEphyB5"
      },
      "outputs": [],
      "source": [
        "reduce_chain = load_summarize_chain(llm=llm2,\n",
        "                             chain_type=\"stuff\",\n",
        "                             prompt=combine_prompt_template,\n",
        "                             verbose=True # Set this to true if you want to see the inner workings\n",
        "                                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbc9kBYDhx95",
        "outputId": "7f429e04-7e71-4461-8d32-03f6a4411d39"
      },
      "outputs": [],
      "source": [
        "output = reduce_chain.run([summaries])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdEWkrN9hx6l",
        "outputId": "765e8188-18ea-4867-f484-e272384757a3"
      },
      "outputs": [],
      "source": [
        "print (output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN+pZJgwFZO5grsXLAMD21h",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "mount_file_id": "1eZ5X_ZyoqE3tO1WRAGZkIoojBLchKWar",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c0553ee65a34cb0bb122bbca79f472d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "132cb06650fa471b806b68e8aec5dde0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18164d2c929442c3bea02018b8534990": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30ebcfd7cb3441189e10fe4a9f2316df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9750c6b07cb41eb8f2ded32275b59d3",
              "IPY_MODEL_df7cbe3e208d497ba45fe873822a4a79",
              "IPY_MODEL_6eeda88d227c4866b3df661630519d97"
            ],
            "layout": "IPY_MODEL_5c27c11ce3e24bc289aab687757dca7d"
          }
        },
        "41980d5d6e23457f8e5c07337f409dd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c27c11ce3e24bc289aab687757dca7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eeda88d227c4866b3df661630519d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c0553ee65a34cb0bb122bbca79f472d",
            "placeholder": "​",
            "style": "IPY_MODEL_41980d5d6e23457f8e5c07337f409dd4",
            "value": " 2/2 [00:08&lt;00:00,  3.82s/it]"
          }
        },
        "8331a92a465041f2b9520175b58b999b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9750c6b07cb41eb8f2ded32275b59d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8331a92a465041f2b9520175b58b999b",
            "placeholder": "​",
            "style": "IPY_MODEL_132cb06650fa471b806b68e8aec5dde0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "df7cbe3e208d497ba45fe873822a4a79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18164d2c929442c3bea02018b8534990",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2eda8da65544d25b22221d53d5269b6",
            "value": 2
          }
        },
        "e2eda8da65544d25b22221d53d5269b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
