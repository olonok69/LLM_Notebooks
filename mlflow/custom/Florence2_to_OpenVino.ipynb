{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9660a88-d5d3-4f43-9fd0-50722ca1fbef",
   "metadata": {},
   "source": [
    "# Florence-2: Open Source Vision Foundation Model\n",
    "Florence-2 is a lightweight vision-language foundation model developed by Microsoft Azure AI and open-sourced under the MIT license. It aims to achieve a unified, prompt-based representation for diverse vision and vision-language tasks, including captioning, object detection, grounding, and segmentation. Despite its compact size, Florence-2 rivals much larger models like Kosmos-2 in performance. Florence-2 represents a significant advancement in vision-language models by combining lightweight architecture with robust capabilities, making it highly accessible and versatile. Its unified representation approach, supported by the extensive FLD-5B dataset, enables it to excel in multiple vision tasks without the need for separate models. This efficiency makes Florence-2 a strong contender for real-world applications, particularly on devices with limited resources.\n",
    "# Paper \n",
    "https://arxiv.org/pdf/2311.06242\n",
    "# Models\n",
    "https://huggingface.co/microsoft/Florence-2-large\n",
    "# Dataset\n",
    "Training set (FLD-5B) of 126M images, more than 500M text annotations, 1.3B region-text annotations, and 3.6B textphrase-region annotations. Each image is annotated with text, region-text pairs, and text-phrase-region triplets and each annotation type has multiple instances varying in diverse granularity\n",
    "\n",
    "\n",
    "# Openvino (Open Visual Inference and Neural network Optimization)\n",
    "OpenVINO is an open-source toolkit for optimizing and deploying deep learning models from cloud to edge. It accelerates deep learning inference across various use cases, such as generative AI, video, audio, and language with models from popular frameworks like PyTorch, TensorFlow, ONNX, and more. Convert and optimize models, and deploy across a mix of IntelÂ® hardware and environments, on-premises and on-device, in the browser or in the cloud.\n",
    "\n",
    "- https://docs.openvino.ai/2024/index.html\n",
    "- https://docs.openvino.ai/2024/documentation/openvino-ir-format.html (Intermediate Representation)\n",
    "# MLFLOW\n",
    "\n",
    "MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible.\n",
    "\n",
    "https://mlflow.org/docs/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdecbb-71af-41f3-9be2-f181647b3921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "%pip install -q \"openvino>=2024.3.0\" \"einops\" \"torch>2.1\" \"torchvision\" \"timm>=0.9.8\" \"transformers>=4.41\" \"pillow\" \"gradio>=4.19\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "if platform.system() != \"Windows\":\n",
    "    %pip install -q \"matplotlib>=3.4\"\n",
    "else:\n",
    "    %pip install -q \"matplotlib>=3.4,<3.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb0325a-4516-4278-9bc8-56f4729087b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! pip install mlflow ov_helpers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c673aae-df51-4b36-85bd-e40a93625031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86bb6d8-08c7-4f9f-93b7-af9a0c907b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not Path(\"ov_florence2_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/florence2/ov_florence2_helper.py\")\n",
    "    open(\"ov_florence2_helper.py\", \"w\", encoding=\"utf-8\").write(r.text)\n",
    "\n",
    "\n",
    "if not Path(\"gradio_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/florence2/gradio_helper.py\")\n",
    "    open(\"gradio_helper.py\", \"w\", encoding=\"utf-8\").write(r.text)\n",
    "\n",
    "if not Path(\"notebook_utils.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\")\n",
    "    open(\"notebook_utils.py\", \"w\" ,encoding=\"utf-8\").write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abb0b3-3a61-4da7-a81d-22fe42882142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ov_helpers.ov_florence2_helper import convert_florence2, get_model_selector\n",
    "\n",
    "model_selector = get_model_selector()\n",
    "\n",
    "model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d474a1-ea74-455b-8f92-0036b13bac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = model_selector.value\n",
    "model_path = Path(model_id.split(\"/\")[-1])\n",
    "\n",
    "# Uncomment the line to see conversion code\n",
    "#??convert_florence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bb3625-6436-454f-a79d-1ee22d7ffb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_florence2(model_id, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0cf659-8908-43be-a088-bd8ee6742d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "device = device_widget()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9256ede-8850-4be0-8e47-0ff2408e4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ov_helpers.ov_florence2_helper import OVFlorence2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bcc0b3-21f7-456b-9b19-29789d37b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelp = os.path.join(os.getcwd(), model_path.name)\n",
    "modelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db51120c-e557-44b8-82bd-8132269e74e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OVFlorence2Model(modelp, device.value)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "354ed43e-b369-4dbb-b7f0-54e3f4879031",
   "metadata": {},
   "source": [
    "# Caption\n",
    "- task_prompt = \"<CAPTION>\"\n",
    "- task_prompt = \"<DETAILED_CAPTION>\"\n",
    "- task_prompt = \"<MORE_DETAILED_CAPTION>\"\n",
    "# Object detection\n",
    "OD results format: {'<OD>': { 'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['label1', 'label2', ...] } }\n",
    "- task_prompt = \"<OD>\"\n",
    "# Dense region caption\n",
    "Dense region caption results format: {'<DENSE_REGION_CAPTION>': {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['label1', 'label2', ...]}}\n",
    "- task_prompt = \"<DENSE_REGION_CAPTION>\"\n",
    "\n",
    "# Region proposal\n",
    "Region proposal results format: {'<REGION_PROPOSAL>' : {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['', '', ...]}}\n",
    "- task_prompt = \"<REGION_PROPOSAL>\"\n",
    "\n",
    "task_prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\n",
    "\n",
    "task_prompt = \"<REFERRING_EXPRESSION_SEGMENTATION>\"\n",
    "\n",
    "task_prompt = \"<REGION_TO_SEGMENTATION>\"\n",
    "\n",
    "task_prompt = \"<OPEN_VOCABULARY_DETECTION>\"\n",
    "\n",
    "task_prompt = \"<REGION_TO_CATEGORY>\"\n",
    "\n",
    "task_prompt = \"<REGION_TO_DESCRIPTION>\"\n",
    "\n",
    "task_prompt = \"<OCR>\"\n",
    "\n",
    "task_prompt = \"<OCR_WITH_REGION>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e41b2-b58a-48e4-9bd1-8f86499839b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "prompt = \"<OD>\"\n",
    "\n",
    "# url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "path = \"../images/buildings.jpg\"\n",
    "image = Image.open(path)\n",
    "#image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c04b23-4865-44a9-bd3c-d33a71100e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ef42b-6309-4600-b412-4b6befe4dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processor.image_processor.crop_size = {'height': 1024, 'width': 1024}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f362b3d4-264b-45eb-a77b-7a499e31db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2615e8-e8bf-41c9-83bb-d60f20342677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48bcf77-2670-44d2-ac6c-caf55a6f0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee4725-cfae-4f91-873a-68a003308a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc8645-d7f8-4bd5-87d6-206a59c5baef",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93206eb1-ec9d-4387-96fa-a519b83de34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(input_ids=inputs[\"input_ids\"], pixel_values=inputs[\"pixel_values\"], max_new_tokens=1024, do_sample=False, num_beams=3)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "parsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(image.width, image.height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ec383-4e08-4818-aedb-d7991ba22efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5c5de-ceaf-49a2-9647-3bfa0cc6d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import copy\n",
    "import random\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218fa4d1-84c2-4e04-9565-e32cb3e6acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bbox(image, data):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "    for bbox, label in zip(data[\"bboxes\"], data[\"labels\"]):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(x1, y1, label, color=\"white\", fontsize=8, bbox=dict(facecolor=\"red\", alpha=0.5))\n",
    "    ax.axis(\"off\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1e1d2-e3fc-4f84-81ea-935433bf24f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gradio_helper import plot_bbox\n",
    "\n",
    "fig = plot_bbox(image, parsed_answer[\"<OD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c172b-ed43-4b69-81ce-0db2ebdfe7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<MORE_DETAILED_CAPTION>\"\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2de878-b7a2-4625-b6bc-5f75d6929c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(input_ids=inputs[\"input_ids\"], pixel_values=inputs[\"pixel_values\"], max_new_tokens=1024, do_sample=False, num_beams=3)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "parsed_answer = processor.post_process_generation(generated_text, task=\"<MORE_DETAILED_CAPTION>\", image_size=(image.width, image.height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11b278e-5aa8-4321-9bcd-1dd5a48c982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_answer.get('<MORE_DETAILED_CAPTION>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf4fc0f-b8c2-4b90-b336-5c09da0dc0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers.utils import logging\n",
    "ROOT_DIR = os.getcwd()\n",
    "load_dotenv(os.path.join(ROOT_DIR,\".env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81ffd5-3fcb-4b18-bb9d-21aade307b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import mlflow\n",
    "config = dotenv_values(os.path.join(ROOT_DIR,\".env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63731e71-4820-4178-b70d-1dad4a497acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfd86d6-22f0-43d8-9154-c8143d5dbd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_URI=config.get('MLFLOW_TRACKING_URI')\n",
    "# Specify the workspace hostname and token\n",
    "DATABRICKS_HOST=config.get('DATABRICKS_HOST')\n",
    "DATABRICKS_TOKEN=config.get('DATABRICKS_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc71a4-cd2a-4ed6-b7fc-1b1437d6669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI\n",
    "\n",
    "os.environ[\"DATABRICKS_HOST\"] = DATABRICKS_HOST\n",
    "\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = DATABRICKS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f145032d-fd89-45e0-9c04-41f35d8db47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATABRICKS_HOST\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d62241-17ec-4fcb-b620-e98c4ad332b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee3d080-6491-40ba-8df0-40947f66dfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(f\"{config.get('USER_DATABRICKS')}/Florence2_captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50656ba8-bfcb-4a19-b2bb-576a8a6a22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.pyfunc import PythonModel\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f690a-55fd-4185-8a02-9113e9a8791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378579e6-f8f7-4d66-84da-aec461e681de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Florence2_Captioner(PythonModel):\n",
    "  def load_context(self, context):\n",
    "        \"\"\"\n",
    "        This method initializes the tokenizer and language model\n",
    "        using the specified model snapshot directory.\n",
    "        \"\"\"\n",
    "        from ov_helpers.ov_florence2_helper import OVFlorence2Model\n",
    "        from transformers import AutoProcessor\n",
    "\n",
    "        self.model = OVFlorence2Model(model_dir=context.artifacts[\"snapshot\"], device=\"AUTO\")\n",
    "        self.processor =  AutoProcessor.from_pretrained(context.artifacts[\"snapshot\"], trust_remote_code=True)\n",
    "\n",
    "\n",
    "\n",
    "  def predict(self, context, model_input, params=None):\n",
    "        \"\"\"\n",
    "        This method generates prediction for the given input.\n",
    "        \"\"\"\n",
    "        # Parameters\n",
    "        task = params.get(\"task\", '<MORE_DETAILED_CAPTION>') if params else '<MORE_DETAILED_CAPTION>'\n",
    "        max_new_tokens = params.get(\"max_new_tokens\", 1024) if params else 1024\n",
    "        num_beams = params.get(\"num_beams\", 3) if params else 3\n",
    "        # get Image\n",
    "        image_path  = model_input[\"path_image\"][0]\n",
    "        raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "        # process image\n",
    "        inputs = self.processor(text=task, images=raw_image, return_tensors=\"pt\")\n",
    "        # conditional image captioning\n",
    "        generated_ids = self.model.generate(input_ids=inputs[\"input_ids\"], pixel_values=inputs[\"pixel_values\"], max_new_tokens=max_new_tokens, do_sample=False, num_beams=num_beams)\n",
    "        generated_text = self.processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "        \n",
    "        parsed_answer = self.processor.post_process_generation(generated_text, task=task,image_size=(raw_image.width, raw_image.height))\n",
    "        return  {task: [parsed_answer.get(task)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d68af2a-8d9f-4aa8-b364-41cc065c83b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import ColSpec, DataType, ParamSchema, ParamSpec, Schema\n",
    "\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "# model_output= [{'<MORE_DETAILED_CAPTION>': \"\"\"This is an image of a city. The city is filled with buildings. The buildings are very tall. The building in the middle is made of glass and metal.\n",
    "# The sky is gray and cloudy. There are mountains in the background. The mountains are brown and gray. The trees in the foreground are green and healthy. There is a street light by \n",
    "# the buildings.\"\"\"}]\n",
    "\n",
    "model_output = Schema([ColSpec(DataType.string, \"task\")])\n",
    "\n",
    "model_input = Schema(\n",
    "    [\n",
    "        ColSpec(DataType.string, \"path_image\"),\n",
    "    ]\n",
    ")\n",
    "parameters = ParamSchema(\n",
    "    [\n",
    "        ParamSpec(\"temperature\", DataType.float, np.float32(0.1), None),\n",
    "        ParamSpec(\"max_new_tokens\", DataType.integer, np.int32(1024), None),\n",
    "        ParamSpec(\"num_beams\", DataType.integer, np.int32(3), None),\n",
    "        ParamSpec(\"task\", DataType.string, \"<MORE_DETAILED_CAPTION>\", None),\n",
    "    ]\n",
    ")\n",
    "\n",
    "signature = ModelSignature(inputs=model_input,outputs=model_output, params=parameters)\n",
    "\n",
    "# Define input example\n",
    "\n",
    "input_example = pd.DataFrame({\"path_image\": [\"D:\\\\repos\\\\openvino\\\\images\\\\buildings.jpg\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a4af9-82a8-4557-9506-9fb8dc9553e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b868f7c9-0aa7-4f1e-8a3b-4a5d37f78ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "now.strftime(\"%Y-%m-%d_%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb6066-e6bc-4350-b2a2-8101941c37f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "# Get the current base version of torch that is installed, without specific version modifiers\n",
    "torch_version = torch.__version__.split(\"+\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01813fda-6375-4c8a-a002-7d3627a3ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR']=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23cbd0-48c7-4695-b6fb-da73048391e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start an MLflow run context and log the Florence model wrapper along with the param-included signature to\n",
    "# allow for overriding parameters at inference time\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "description= \"\"\"Log Florence2 \n",
    "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks\n",
    "https://huggingface.co/microsoft/Florence-2-large\n",
    "```\n",
    "### Caption\n",
    "- task_prompt = \"<CAPTION>\"\n",
    "- task_prompt = \"<DETAILED_CAPTION>\"\n",
    "- task_prompt = \"<MORE_DETAILED_CAPTION>\"\n",
    "\n",
    "### Object detection\n",
    "OD results format: {'<OD>': { 'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['label1', 'label2', ...] } }\n",
    "- task_prompt = \"<OD>\"\n",
    "\n",
    "### Dense region caption\n",
    "Dense region caption results format: {'<DENSE_REGION_CAPTION>': {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['label1', 'label2', ...]}}\n",
    "- task_prompt = \"<DENSE_REGION_CAPTION>\"\n",
    "\n",
    "### Region proposal\n",
    "\n",
    "Region proposal results format: {'<REGION_PROPOSAL>' : {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['', '', ...]}}\n",
    "- task_prompt = \"<REGION_PROPOSAL>\"\n",
    "\n",
    "task_prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\n",
    "\n",
    "task_prompt = \"<REFERRING_EXPRESSION_SEGMENTATION>\"\n",
    "\n",
    "task_prompt = \"<REGION_TO_SEGMENTATION>\"\n",
    "\n",
    "task_prompt = \"<OPEN_VOCABULARY_DETECTION>\"\n",
    "\n",
    "task_prompt = \"<REGION_TO_CATEGORY>\"\n",
    "\n",
    "task_prompt = \"<REGION_TO_DESCRIPTION>\"\n",
    "\n",
    "task_prompt = \"<OCR>\"\n",
    "\n",
    "task_prompt = \"<OCR_WITH_REGION>\"\n",
    "```\n",
    "\"\"\"\n",
    "with mlflow.start_run(run_name=f\"florence2_captioner_log_{now.strftime('%Y-%m-%d_%H:%M:%S')}\", description=description) as run:\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        \"captioner\",\n",
    "        python_model=Florence2_Captioner(),\n",
    "        # NOTE: the artifacts dictionary mapping is critical! This dict is used by the load_context()\n",
    "        artifacts={\"snapshot\": modelp},\n",
    "\n",
    "        pip_requirements=[\n",
    "            \"torch>2.4.1\",\n",
    "            f\"transformers=={transformers.__version__}\",\n",
    "            \"pillow\",\n",
    "            \"openvino>=2024.3.0\" ,\n",
    "            \"einops\",\n",
    "             \"torchvision\",\n",
    "            \"timm>=0.9.8\",\n",
    "            \"ov_helpers\",\n",
    "            \"nncf\"\n",
    "            \n",
    "\n",
    "\n",
    "        ],\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9188b5-e35f-4464-ac43-c9aad4029335",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.to_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8347d0c7-5aa9-4c7e-ab17-0792c89b8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info.signature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0b439-c00d-4c91-85f5-92b4fcf380f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info.model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ca3f4e-0f3f-4019-9b86-1c201637fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77534d9e-8f3b-4362-94bc-a55469a72df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f544f766-b199-40b1-a670-afef5399581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time1=  datetime.datetime.now()\n",
    "input_image =  pd.DataFrame({\"path_image\": [\"D:\\\\repos\\\\openvino\\\\images\\\\buildings.jpg\"]})\n",
    "response = loaded_model.predict(input_image, params={\"task\": \"<MORE_DETAILED_CAPTION>\" })\n",
    "time2=  datetime.datetime.now()\n",
    "print(time2-time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f710f42-65ee-45b3-a4cf-e1c59ee6f422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pprint.pprint(response[\"<MORE_DETAILED_CAPTION>\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4849c813-1262-41b7-8b45-4f7c101c1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = mlflow.register_model(\n",
    "    model_info.model_uri, \"florence2_captioner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d61e79-3181-4cea-840f-14f3ef2d3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7a70f0-03f4-4c85-b565-8e7ef533d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "model_name = \"florence2_captioner\"\n",
    "model_version = 1\n",
    "\n",
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{model_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a31726-dd76-46ad-ab56-46b0dfcc9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.predict(input_image, params={\"task\": \"<MORE_DETAILED_CAPTION>\" })\n",
    "pprint.pprint(response[\"<MORE_DETAILED_CAPTION>\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840c1c7-24f4-4014-b9cd-74ec1c03df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.predict(input_image, params={\"task\": \"<OD>\" })\n",
    "pprint.pprint(response[\"<OD>\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc010d8-8d3f-4f6c-abed-dd4e68fc0cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"models:/{model_name}/{model_version}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4078e-ce2f-4fda-9e42-eba9542bbd15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ov)",
   "language": "python",
   "name": "openvino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
