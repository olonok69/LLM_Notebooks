{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f41b39ba-c5a2-43f7-a6bf-30d8179c45f8",
   "metadata": {},
   "source": [
    "# Langchain <--> Elastic Search\n",
    "\n",
    "Elasticsearch is an open source distributed, RESTful search and analytics engine, scalable data store, and vector database capable of addressing a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data for lightning-fast search, fineâ€‘tuned relevancy, and powerful analytics that scale with ease.\n",
    "Elasticsearch can store and index a variety of data, including structured and unstructured text, numerical data, and geospatial data. It's known for its ability to find queries in large-scale unstructured data\n",
    "Elasticsearch uses a search index, which is similar to an index in the back of a book, to map content to its location in a document. This allows users to quickly find information without scanning through an entire document\n",
    "\n",
    "- https://www.elastic.co/search-labs/blog/langchain-collaboration\n",
    "- https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html\n",
    "- https://python.langchain.com/docs/integrations/vectorstores/elasticsearch/\n",
    "- https://www.elastic.co/blog/elasticsearch-is-open-source-again\n",
    "- https://www.elastic.co/search-labs/blog/category/generative-ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54202443-41a9-4d26-9345-3dae7d117266",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341825-1534-4d50-9a2d-c3e28bbb444e",
   "metadata": {},
   "source": [
    "# Install ELastic Search Docker\n",
    "\n",
    "- docker network create elastic\n",
    "- docker pull docker.elastic.co/elasticsearch/elasticsearch:8.15.3\n",
    "- docker run --name es01 --net elastic -p 9200:9200 -it -m 1GB docker.elastic.co/elasticsearch/elasticsearch:8.15.3\n",
    "\n",
    "# Docker compose\n",
    "To INstall all components in Containers, Ollama, ELastic and Setup ELastic Component\n",
    "\n",
    "- docker compose -f docker-compose.yml up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce3ca41-6a91-4b55-961b-26e3d1eaa2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1086070-63a7-41a8-80ab-b54a398764a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\"./keys/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f338483-7888-4520-8b59-c77d5d033043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tempfile\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import dotenv_values\n",
    "import json\n",
    "import vertexai\n",
    "\n",
    "import itertools\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f269ec-271e-461e-a4db-03423bfa41a3",
   "metadata": {},
   "source": [
    "# SETUP ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7433e972-08db-4a8b-9037-b66eac9ffc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\"./keys/.env\")\n",
    "with open(\"./keys/complete-tube-421007-208a4862c992.json\") as source:\n",
    "    info = json.load(source)\n",
    "\n",
    "vertex_credentials = service_account.Credentials.from_service_account_info(info)\n",
    "vertexai.init(\n",
    "    project=config[\"PROJECT\"],\n",
    "    location=config[\"REGION\"],\n",
    "    credentials=vertex_credentials,\n",
    ")\n",
    "google_api_key = config[\"GEMINI-API-KEY\"]\n",
    "os.environ[\"GEMINI_API_KEY\"] = google_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aedd986-e586-4539-86d0-0d62509caccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT= os.getcwd()\n",
    "ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb42462-13ce-47a0-ab5f-04b8bd144e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "                    model=\"gemini-1.5-pro-001\", credentials=vertex_credentials\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd95317-ac09-457f-bc1c-ebdddd189918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GoogleAIClient:\n",
    "    def __init__(self, model, credentials):\n",
    "        self.credentials = credentials\n",
    "        self.model = model\n",
    "        self.client = ChatGoogleGenerativeAI(\n",
    "            model=self.model, credentials=self.credentials)\n",
    "\n",
    "    def generate_streaming_response(self, prompt):\n",
    "        \"\"\"\n",
    "        Generates a streaming completion using LangChain ChatGoogleGenerativeAI.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The prompt for the model.\n",
    "            model (str): The model to use (e.g., \"gemini-pro\").\n",
    "\n",
    "        Yields:\n",
    "            str: Chunks of the generated completion.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            chat = ChatGoogleGenerativeAI(model=self.model)\n",
    "            messages = [HumanMessage(content=prompt)]\n",
    "            for chunk in chat.stream(messages):\n",
    "                yield chunk.content\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            yield f\"Error: {e}\" # yield error message to the stream.\n",
    "\n",
    "    def generate_non_streaming_response(self, prompt, system_prompt=\"\"):\n",
    "        try:\n",
    "            chat = self.client\n",
    "            messages = []\n",
    "            if system_prompt:\n",
    "                messages.append(SystemMessage(content=system_prompt))\n",
    "            messages.append(HumanMessage(content=prompt))\n",
    "            response = chat.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "        \n",
    "LLM = GoogleAIClient(model=\"gemini-1.5-pro-001\", credentials=vertex_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd83ee-e054-4761-9734-31317294de24",
   "metadata": {},
   "source": [
    "# GENERATE FAKE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9d15e-53a7-43ee-ae95-b1c825135bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import string\n",
    "from pprint import pprint\n",
    "import traceback\n",
    "import uuid\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def generate_random_email():\n",
    "    # Generate a random username length between 5 and 12 characters\n",
    "    username_length = random.randint(5, 12)\n",
    "    \n",
    "    # Generate a random username using lowercase letters and digits\n",
    "    username = ''.join(random.choices(string.ascii_lowercase + string.digits, k=username_length))\n",
    "    \n",
    "    # List of common email domains\n",
    "    domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com', 'icloud.com']\n",
    "    \n",
    "    # Choose a random domain\n",
    "    domain = random.choice(domains)\n",
    "    \n",
    "    # Construct and return the email\n",
    "    return f\"{username}@{domain}\"\n",
    "\n",
    "\n",
    "def generate_chinese_name():\n",
    "    chinese_surnames = [\n",
    "        \"Tan\", \"Lim\", \"Lee\", \"Ng\", \"Ong\", \"Wong\", \"Goh\", \"Chua\", \"Koh\", \"Teo\", \"Chan\", \"Yeo\", \"Ang\", \"Chong\", \"Leong\", \"Foo\", \"Sim\", \"Tay\", \"Ho\", \"Low\",\n",
    "        \"Chen\", \"Lin\", \"Huang\", \"Zhang\", \"Li\", \"Wang\", \"Liu\", \"Wu\", \"Yang\", \"Zhou\", \"Xu\", \"Sun\", \"Ma\", \"Zhu\", \"Hu\", \"Guo\", \"He\", \"Gao\", \"Lin\", \"Luo\",\n",
    "        \"Zheng\", \"Liang\", \"Xie\", \"Song\", \"Tang\", \"Xu\", \"Han\", \"Feng\", \"Deng\", \"Xiao\", \"Cheng\", \"Cao\", \"Peng\", \"Zeng\", \"Xue\", \"Lu\", \"Su\", \"Pan\", \"Jiang\", \"Bai\",\n",
    "        \"Du\", \"Yin\", \"Mei\", \"Fang\", \"Fu\", \"Yuan\", \"Cai\", \"Jia\", \"Gu\", \"Xiong\", \"Hao\", \"Shao\", \"Meng\", \"Long\", \"Wei\", \"Wan\", \"Duan\", \"Qiu\", \"Jiang\", \"Qin\",\n",
    "        \"Chu\", \"Yu\", \"Shen\", \"Qi\", \"Cui\", \"Ren\", \"Tian\", \"Xia\", \"Shi\", \"Hou\", \"Yan\", \"Jin\", \"Kong\", \"Wei\", \"Xiang\", \"Yao\", \"Yan\", \"Sheng\", \"Zu\", \"Qian\"\n",
    "    ]\n",
    "\n",
    "    chinese_given_names = [\n",
    "        \"Wei\", \"Hui\", \"Xin\", \"Yi\", \"Ying\", \"Jie\", \"Ling\", \"Zhi\", \"Qiang\", \"Mei\", \"Jun\", \"Xiang\", \"Hao\", \"Chen\", \"Ming\", \"Feng\", \"Yang\", \"Cheng\", \"Yong\", \"Tian\",\n",
    "        \"Jing\", \"Yan\", \"Fei\", \"Yu\", \"Xiuying\", \"Guiying\", \"Chunmei\", \"Xiaohong\", \"Xiulan\", \"Guilan\", \"Huifang\", \"Xiuzhen\", \"Yumei\", \"Xiumei\", \"Guirong\", \"Shulan\", \"Guizhi\", \"Xiuyun\", \"Huiying\", \"Jinlan\",\n",
    "        \"Qing\", \"Xuan\", \"Zhen\", \"Rui\", \"Kai\", \"Sheng\", \"Hong\", \"Xiong\", \"Lei\", \"Hua\", \"Bin\", \"Heng\", \"Xiaowei\", \"Xiaojun\", \"Xiaofeng\", \"Xiaogang\", \"Xiaoming\", \"Xiaohua\", \"Xiaohui\", \"Xiaolin\",\n",
    "        \"An\", \"Bao\", \"Bo\", \"Chang\", \"Chao\", \"Da\", \"Dong\", \"En\", \"Gang\", \"Guo\", \"Hai\", \"Han\", \"Jian\", \"Jiao\", \"Jin\", \"Kang\", \"Lang\", \"Li\", \"Liang\", \"Miao\",\n",
    "        \"Nan\", \"Peng\", \"Ping\", \"Qi\", \"Qian\", \"Rong\", \"Ru\", \"Shan\", \"Shu\", \"Tai\", \"Tao\", \"Wen\", \"Wu\", \"Xia\", \"Xian\", \"Xiao\", \"Xue\", \"Yao\", \"Yi\", \"Yin\",\n",
    "        \"Yu\", \"Yuan\", \"Yun\", \"Zhan\", \"Zhe\", \"Zhong\", \"Zi\", \"Ai\", \"Bi\", \"Cai\", \"Can\", \"Ce\", \"Cui\", \"Di\", \"E\", \"Fu\", \"Gai\", \"Gan\", \"Huan\", \"Jia\",\n",
    "        \"Jiu\", \"Ju\", \"Kui\", \"Lan\", \"Lian\", \"Meng\", \"Nian\", \"Ning\", \"Nu\", \"Pin\", \"Qiu\", \"Quan\", \"Sha\", \"Shi\", \"Si\", \"Song\", \"Su\", \"Ti\", \"Tong\", \"Wai\",\n",
    "        \"Xi\", \"Xiu\", \"Xu\", \"Ya\", \"Yan\", \"Ye\", \"Ying\", \"You\", \"Zai\", \"Ze\", \"Zeng\", \"Zhi\", \"Zhuo\", \"Zi\", \"Zong\", \"Zou\"\n",
    "    ]\n",
    "    \n",
    "    surname = random.choice(chinese_surnames)\n",
    "    given_name = random.choice(chinese_given_names)\n",
    "    \n",
    "    # Sometimes add a second character to the given name\n",
    "    if random.random() < 0.5:  # 50% chance for a two-character given name\n",
    "        given_name += ' '+random.choice(chinese_given_names)\n",
    "    \n",
    "    return f\"{surname} {given_name}\"\n",
    "\n",
    "\n",
    "def generate_malay_name():\n",
    "    malay_given_names = [\n",
    "        \"Abdullah\", \"Abdul Rahman\", \"Abdul Rahim\", \"Abdul Aziz\", \"Abdul Kadir\", \"Abdul Latif\", \"Abdul Malik\", \"Abdul Razak\", \"Abu Bakar\", \"Adam\", \"Adil\", \"Adnan\", \"Ahmad\", \"Aiman\", \"Aizat\", \"Akmal\", \"Ali\", \"Amin\", \"Amir\", \"Ammar\",\n",
    "        \"Anuar\", \"Arif\", \"Ashraf\", \"Asraf\", \"Azhar\", \"Aziz\", \"Azlan\", \"Azman\", \"Azmi\", \"Badrul\", \"Baharudin\", \"Bakri\", \"Borhan\", \"Burhanuddin\", \"Che\", \"Danial\", \"Daud\", \"Dzulkifli\", \"Edzham\", \"Fadil\", \"Fahmi\", \"Faisal\", \"Faizal\", \"Farid\", \"Faris\",\n",
    "        \"Fauzi\", \"Fuad\", \"Ghazali\", \"Hadi\", \"Hafiz\", \"Hakim\", \"Halim\", \"Hamid\", \"Hamzah\", \"Hanafi\", \"Haris\", \"Harith\", \"Haron\", \"Hasan\", \"Hashim\", \"Hassan\", \"Haziq\", \"Helmi\", \"Hisham\", \"Husain\", \"Hussein\", \"Ibrahim\", \"Idris\", \"Ihsan\",\n",
    "        \"Imran\", \"Irfan\", \"Isa\", \"Ismail\", \"Izwan\", \"Jafar\", \"Jamal\", \"Jamil\", \"Johari\", \"Kamal\", \"Kamarul\", \"Kamaruzaman\", \"Khairil\", \"Khairuddin\", \"Khalid\", \"Lokman\", \"Lutfi\", \"Mahathir\", \"Mahmud\", \"Majid\", \"Malik\", \"Mansor\", \"Mas\", \"Mat\", \"Megat\",\n",
    "        \"Mizan\", \"Mohamad\", \"Mohamed\", \"Mohammad\", \"Mohammed\", \"Mohd\", \"Muhamad\", \"Muhammad\", \"Muhsin\", \"Mukhriz\", \"Munir\", \"Mustafa\", \"Muthu\", \"Nasir\", \"Nasrudin\", \"Nazri\", \"Nik\", \"Nizam\", \"Noor\", \"Nor\", \"Nordin\", \"Omar\", \"Osman\", \"Othman\",\n",
    "        \"Radzi\", \"Rafiq\", \"Rahimi\", \"Rahim\", \"Rahman\", \"Rashid\", \"Razak\", \"Razali\", \"Redza\", \"Redzuan\", \"Riduan\", \"Rizal\", \"Roslan\", \"Ruslan\", \"Saad\", \"Sabri\", \"Saffuan\", \"Saiful\", \"Saleh\", \"Salleh\", \"Samad\", \"Shafiq\", \"Shah\", \"Shahrul\", \"Shamsudin\",\n",
    "        \"Shamsul\", \"Sharif\", \"Sulaiman\", \"Syed\", \"Syukri\", \"Tarmizi\", \"Taufiq\", \"Tengku\", \"Umar\", \"Wan\", \"Yusof\", \"Yusoff\", \"Yusri\", \"Zafran\", \"Zainal\", \"Zakaria\", \"Zaki\", \"Zamri\", \"Zikri\", \"Zulkifli\",\n",
    "        \"Adibah\", \"Adila\", \"Adina\", \"Afiqah\", \"Aida\", \"Aishah\", \"Aisyah\", \"Alya\", \"Amalina\", \"Amelia\", \"Amira\", \"Amirah\", \"Aminah\", \"Anisah\", \"Aqilah\", \"Arissa\", \"Asma\", \"Asmah\", \"Atiqah\", \"Azizah\",\n",
    "        \"Azlina\", \"Azwa\", \"Balqis\", \"Dalila\", \"Dayang\", \"Eliana\", \"Emilia\", \"Farah\", \"Farhana\", \"Farhanah\", \"Fariha\", \"Faridah\", \"Farihah\", \"Fasihah\", \"Fatimah\", \"Fatin\", \"Fazlin\", \"Hafizah\", \"Halimatun\", \"Hamidah\",\n",
    "        \"Hanisah\", \"Hasmah\", \"Hasnah\", \"Haziqah\", \"Hazwani\", \"Hidayah\", \"Humaira\", \"Izzati\", \"Jamilah\", \"Khadijah\", \"Khairunnisa\", \"Laila\", \"Latifah\", \"Lina\", \"Madiha\", \"Maisarah\", \"Mariam\", \"Maryam\", \"Mas\", \"Mastura\",\n",
    "        \"Mawar\", \"Nabila\", \"Nabilah\", \"Nadiah\", \"Nadirah\", \"Nafeesa\", \"Najwa\", \"Nasyitah\", \"Natasha\", \"Nazifah\", \"Nazirah\", \"Nik\", \"Noor\", \"Noorul\", \"Nor\", \"Nora\", \"Noraini\", \"Norashikin\", \"Norazlina\", \"Norhayati\",\n",
    "        \"Noriah\", \"Norizan\", \"Norlia\", \"Normah\", \"Norziana\", \"Nur\", \"Nurain\", \"Nuraina\", \"Nuraliya\", \"Nuramira\", \"Nurassyifa\", \"Nurdiana\", \"Nurfadilah\", \"Nurfaizah\", \"Nurfarahana\", \"Nurhafizah\", \"Nurhaliza\", \"Nurhayati\", \"Nurhidayah\", \"Nurin\",\n",
    "        \"Nurliyana\", \"Nurmala\", \"Nurshahira\", \"Nursyafiqah\", \"Nursyahirah\", \"Nurul\", \"Puteri\", \"Qistina\", \"Rabiatul\", \"Rabiatuladawiyah\", \"Radin\", \"Rahmah\", \"Raja\", \"Rashidah\", \"Rosmah\", \"Rossita\", \"Rozita\", \"Safiah\", \"Safinah\", \"Safiyyah\",\n",
    "        \"Saleha\", \"Salina\", \"Salma\", \"Saodah\", \"Sarah\", \"Shafiqah\", \"Sharifah\", \"Siti\", \"Sofia\", \"Sofiah\", \"Sofiyah\", \"Sumaiyah\", \"Suraya\", \"Syafiqah\", \"Syahirah\", \"Syairah\", \"Syakila\", \"Syamimi\", \"Syaza\", \"Syazwani\",\n",
    "        \"Tengku\", \"Ummi\", \"Umi\", \"Wan\", \"Yasmin\", \"Yusrina\", \"Zainab\", \"Zainal\", \"Zainun\", \"Zakiah\", \"Zaleha\", \"Zalina\", \"Zanariah\", \"Zarina\", \"Zulaika\", \"Zulaikha\", \"Zulaikhah\", \"Zulfah\"\n",
    "    ]\n",
    "\n",
    "    malay_surnames = [\"bin\", \"binti\"]\n",
    "    \n",
    "    first_name = random.choice(malay_given_names)\n",
    "    middle_name = random.choice(malay_given_names)\n",
    "    surname = random.choice(malay_surnames)\n",
    "    \n",
    "    # Sometimes add a title or honorific\n",
    "    titles = [\"Haji\", \"Hajjah\", \"Tan Sri\", \"Puan Sri\", \"Datuk\", \"Datin\", \"Tun\", \"Toh Puan\"]\n",
    "    if random.random() < 0.1:  # 10% chance to add a title\n",
    "        title = random.choice(titles)\n",
    "        return f\"{title} {first_name} {surname} {middle_name}\"\n",
    "    \n",
    "    return f\"{first_name} {surname} {middle_name}\"\n",
    "\n",
    "def generate_indian_name():\n",
    "    indian_given_names = [\n",
    "        \"Aadhav\", \"Aadit\", \"Aaditya\", \"Aakash\", \"Aalam\", \"Aalok\", \"Aamir\", \"Aanjaneya\", \"Aarav\", \"Aarnav\", \"Aarush\", \"Aayush\", \"Abha\", \"Abhai\", \"Abhay\", \"Abhijat\", \"Abhijeet\", \"Abhimanyu\", \"Abhinav\", \"Abhishek\",\n",
    "        \"Abishek\", \"Aditi\", \"Aditya\", \"Advik\", \"Agastya\", \"Agni\", \"Aishwarya\", \"Ajay\", \"Ajeet\", \"Akash\", \"Akhil\", \"Akshay\", \"Akshita\", \"Alok\", \"Amal\", \"Aman\", \"Amar\", \"Amarnath\", \"Amey\", \"Amish\",\n",
    "        \"Amit\", \"Amita\", \"Amitabh\", \"Amolak\", \"Amrita\", \"Anand\", \"Anandi\", \"Anamika\", \"Ananth\", \"Ananya\", \"Anarya\", \"Anay\", \"Anaya\", \"Aniket\", \"Anil\", \"Aniruddha\", \"Anish\", \"Anit\", \"Anita\", \"Anjali\",\n",
    "        \"Anjana\", \"Anjan\", \"Anjney\", \"Ankit\", \"Ankita\", \"Ankur\", \"Anmol\", \"Ansh\", \"Anshika\", \"Anshul\", \"Anuj\", \"Anupam\", \"Anushka\", \"Anurag\", \"Aparna\", \"Apoorva\", \"Arav\", \"Arjun\", \"Arka\", \"Arnav\",\n",
    "        \"Arohi\", \"Arpit\", \"Artha\", \"Arun\", \"Aruna\", \"Arundhati\", \"Arushi\", \"Arya\", \"Asha\", \"Ashok\", \"Ashwin\", \"Asim\", \"Astha\", \"Atharv\", \"Ati\", \"Atiksh\", \"Atishay\", \"Atul\", \"Aum\", \"Avani\",\n",
    "        \"Avantika\", \"Avichal\", \"Avinash\", \"Ayaan\", \"Ayush\", \"Ayushi\", \"Bala\", \"Balaji\", \"Bharat\", \"Bharath\", \"Bhargav\", \"Bhargavi\", \"Bhaskar\", \"Bhavana\", \"Bhavesh\", \"Bhavya\", \"Bhoomi\", \"Bijay\", \"Bina\", \"Bindu\",\n",
    "        \"Chandan\", \"Chandra\", \"Chandran\", \"Charu\", \"Chetan\", \"Chetana\", \"Chirag\", \"Chitrangada\", \"Darshan\", \"Daya\", \"Deepa\", \"Deepak\", \"Deepika\", \"Dev\", \"Deva\", \"Devdan\", \"Devendra\", \"Devi\", \"Devika\", \"Dhairya\",\n",
    "        \"Dhananjay\", \"Dharma\", \"Dharmendra\", \"Dhruv\", \"Dilip\", \"Disha\", \"Divya\", \"Diya\", \"Durga\", \"Esha\", \"Ekta\", \"Gauri\", \"Gautam\", \"Gayathri\", \"Geeta\", \"Girish\", \"Gita\", \"Gitanjali\", \"Gopal\", \"Gopinath\",\n",
    "        \"Govind\", \"Gowri\", \"Gulshan\", \"Gunjan\", \"Guru\", \"Harsh\", \"Harsha\", \"Harshad\", \"Harshita\", \"Hema\", \"Hemant\", \"Himani\", \"Hira\", \"Hiren\", \"Indira\", \"Indra\", \"Indu\", \"Ira\", \"Ishan\", \"Isha\",\n",
    "        \"Ishaan\", \"Ishani\", \"Ishita\", \"Jai\", \"Jatin\", \"Jaya\", \"Jayant\", \"Jayanti\", \"Jayin\", \"Jhanvi\", \"Jitendra\", \"Jiya\", \"Jyoti\", \"Kabir\", \"Kalindi\", \"Kalpana\", \"Kalyani\", \"Kanak\", \"Karan\", \"Karthik\",\n",
    "        \"Kartik\", \"Karuna\", \"Kaustubh\", \"Kavita\", \"Kavya\", \"Keerthi\", \"Keshav\", \"Ketan\", \"Khushi\", \"Kiara\", \"Kiran\", \"Kirti\", \"Krishna\", \"Krish\", \"Kriti\", \"Kritika\", \"Kshitij\", \"Kunal\", \"Kushal\", \"Lakshmi\",\n",
    "        \"Lalit\", \"Lalita\", \"Lavanya\", \"Laxmi\", \"Leela\", \"Madhav\", \"Madhavi\", \"Madhur\", \"Mahendra\", \"Mahesh\", \"Mahima\", \"Mahi\", \"Mallika\", \"Manasi\", \"Manish\", \"Manju\", \"Manjula\", \"Manoj\", \"Manohar\", \"Maya\",\n",
    "        \"Mayank\", \"Meena\", \"Meera\", \"Megha\", \"Mehul\", \"Mira\", \"Mitali\", \"Mohit\", \"Mridula\", \"Mukesh\", \"Mukta\", \"Muskaan\", \"Nachiket\", \"Naman\", \"Namita\", \"Nandini\", \"Narayan\", \"Naren\", \"Naveen\", \"Navin\",\n",
    "        \"Neela\", \"Neelam\", \"Neeti\", \"Neha\", \"Nidhi\", \"Nikhil\", \"Nikita\", \"Nilam\", \"Nilesh\", \"Nilima\", \"Nimesh\", \"Nirmal\", \"Nirmala\", \"Nirupama\", \"Nisha\", \"Nishant\", \"Nishtha\", \"Nitesh\", \"Niti\", \"Nitya\",\n",
    "        \"Om\", \"Ojas\", \"Omkar\", \"Pankaj\", \"Parag\", \"Paras\", \"Parth\", \"Parvati\", \"Pooja\", \"Prabhat\", \"Prachi\", \"Pradip\", \"Pragya\", \"Prakash\", \"Pramod\", \"Pranav\", \"Praney\", \"Pranita\", \"Prasad\", \"Pratap\",\n",
    "        \"Pratibha\", \"Pratik\", \"Praveen\", \"Prem\", \"Prerna\", \"Preeti\", \"Priya\", \"Priyanka\", \"Puja\", \"Puneet\", \"Purvi\", \"Pushpa\", \"Rachana\", \"Radha\", \"Radhika\", \"Raghu\", \"Rahul\", \"Raj\", \"Raja\", \"Rajat\",\n",
    "        \"Rajeev\", \"Rajendra\", \"Rajesh\", \"Raju\", \"Rakesh\", \"Ram\", \"Rama\", \"Ramesh\", \"Rani\", \"Ranjana\", \"Ranjit\", \"Rashmi\", \"Ravi\", \"Ravindra\", \"Rekha\", \"Renuka\", \"Reva\", \"Richa\", \"Riddhi\", \"Riddhima\",\n",
    "        \"Rishabh\", \"Rishi\", \"Rita\", \"Ritesh\", \"Ritika\", \"Rohan\", \"Rohit\", \"Roopa\", \"Ruchi\", \"Rudra\", \"Rupal\", \"Rupali\", \"Rushil\", \"Sachin\", \"Sahil\", \"Sakshi\", \"Sameer\", \"Samir\", \"Sandeep\", \"Sandya\",\n",
    "        \"Sanjay\", \"Sanjiv\", \"Sankar\", \"Santosh\", \"Saras\", \"Sarika\", \"Sarthak\", \"Satish\", \"Satyam\", \"Saurabh\", \"Savar\", \"Seema\", \"Shailesh\", \"Shalu\", \"Shanta\", \"Shantanu\", \"Sharad\", \"Sharmila\", \"Shashi\", \"Shekhar\",\n",
    "        \"Shilpa\", \"Shiva\", \"Shivani\", \"Shraddha\", \"Shreeya\", \"Shreya\", \"Shri\", \"Shriram\", \"Shubha\", \"Shubham\", \"Shweta\", \"Siddharth\", \"Simar\", \"Simran\", \"Smita\", \"Smriti\", \"Sneha\", \"Soham\", \"Sohini\", \"Sonam\",\n",
    "        \"Sonia\", \"Srijan\", \"Srinivas\", \"Subhash\", \"Suchitra\", \"Sudhir\", \"Sujata\", \"Sukanya\", \"Suman\", \"Sumati\", \"Sumit\", \"Sundar\", \"Sundari\", \"Sunil\", \"Sunita\", \"Supriya\", \"Suraj\", \"Suresh\", \"Surya\", \"Sushil\",\n",
    "        \"Sushma\", \"Swapna\", \"Swapnil\", \"Swati\", \"Tanisha\", \"Tanmay\", \"Tanuj\", \"Tanvi\", \"Tanya\", \"Tarun\", \"Tej\", \"Tejas\", \"Tejashri\", \"Tina\", \"Trisha\", \"Triveni\", \"Tuhina\", \"Tushar\", \"Udai\", \"Uday\",\n",
    "        \"Ujjwal\", \"Uma\", \"Umang\", \"Upasana\", \"Urvi\", \"Usha\", \"Uttam\", \"Vaibhav\", \"Vaishnavi\", \"Varun\", \"Varsha\", \"Vasant\", \"Vasudha\", \"Vedant\", \"Vidhi\", \"Vidya\", \"Vijay\", \"Vimal\", \"Vinay\", \"Vineet\",\n",
    "        \"Vinod\", \"Vipul\", \"Viraj\", \"Vishal\", \"Vishnu\", \"Vivek\", \"Yash\", \"Yashoda\", \"Yogesh\", \"Yuvraj\"\n",
    "    ]\n",
    "\n",
    "    indian_surnames = [\n",
    "        \"Acharya\", \"Agarwal\", \"Aggarwal\", \"Ahluwalia\", \"Ahuja\", \"Arora\", \"Anand\", \"Awasthi\", \"Babu\", \"Badal\", \"Bajaj\", \"Bajwa\", \"Bakshi\", \"Balakrishnan\", \"Balan\", \"Balasubramanian\", \"Banerjee\", \"Banik\", \"Bansal\", \"Basu\",\n",
    "        \"Batra\", \"Bhagat\", \"Bhalla\", \"Bhandari\", \"Bhardwaj\", \"Bhargava\", \"Bhasin\", \"Bhat\", \"Bhatia\", \"Bhatt\", \"Bhattacharya\", \"Bhavsar\", \"Bedi\", \"Bhojwani\", \"Bose\", \"Buch\", \"Chauhan\", \"Chadha\", \"Chakrabarti\", \"Chakraborty\",\n",
    "        \"Chandra\", \"Chatterjee\", \"Chaturvedi\", \"Chauhan\", \"Chawla\", \"Cherian\", \"Chokshi\", \"Chopra\", \"Choudhary\", \"Choudhury\", \"D'Souza\", \"Dalmia\", \"Das\", \"Dasgupta\", \"Datta\", \"Dave\", \"Dayal\", \"Desai\", \"Deshmukh\", \"Deshpande\",\n",
    "        \"Devan\", \"Dewan\", \"Dhar\", \"Dhawan\", \"Dhillon\", \"Dixit\", \"Doshi\", \"Dua\", \"Dube\", \"Dubey\", \"Dugar\", \"Dutt\", \"Dutta\", \"Dwivedi\", \"Fernandes\", \"Gandhi\", \"Ganesh\", \"Ganguly\", \"Garg\", \"George\", \"Ghosh\", \"Gokhale\", \"Goel\",\n",
    "        \"Goswami\", \"Gour\", \"Goyal\", \"Guha\", \"Gulati\", \"Gupta\", \"Halder\", \"Handa\", \"Hans\", \"Hegde\", \"Hora\", \"Iyengar\", \"Iyer\", \"Jain\", \"Jaiswal\", \"Jani\", \"Jayaraman\", \"Jha\", \"Jhaveri\", \"Johar\", \"Joshi\", \"Kakkar\", \"Kala\",\n",
    "        \"Kale\", \"Kalra\", \"Kanda\", \"Kannan\", \"Kapoor\", \"Kapur\", \"Kar\", \"Karnik\", \"Kashyap\", \"Kaul\", \"Kaur\", \"Khatri\", \"Khanna\", \"Khandelwal\", \"Kher\", \"Khosla\", \"Khurana\", \"Kohli\", \"Kochhar\", \"Kothari\", \"Krishna\", \"Krishnamurthy\",\n",
    "        \"Krishnan\", \"Kulkarni\", \"Kumar\", \"Kumari\", \"Kurian\", \"Kuruvilla\", \"Lal\", \"Lalla\", \"Lamba\", \"Lobo\", \"Madhavan\", \"Mahajan\", \"Mahalingam\", \"Maheshwari\", \"Majumdar\", \"Malhotra\", \"Malik\", \"Manikandan\", \"Mani\", \"Manna\",\n",
    "        \"Mathew\", \"Mathur\", \"Mehra\", \"Mehrotra\", \"Mehta\", \"Menon\", \"Mirchandani\", \"Mishra\", \"Misra\", \"Mistry\", \"Mitra\", \"Modi\", \"Mohan\", \"Mohanty\", \"Mukherjee\", \"Mukhopadhyay\", \"Nagar\", \"Nagarajan\", \"Nair\", \"Nambiar\",\n",
    "        \"Nambudiripad\", \"Nanda\", \"Narang\", \"Narayan\", \"Narayanan\", \"Nath\", \"Nayak\", \"Nayar\", \"Nazareth\", \"Nigam\", \"Nimbkar\", \"Oak\", \"Om\", \"Padmanabhan\", \"Pai\", \"Pal\", \"Palan\", \"Pande\", \"Pandey\", \"Pandit\", \"Pant\", \"Parekh\",\n",
    "        \"Parikh\", \"Patel\", \"Pathak\", \"Patil\", \"Patnaik\", \"Patra\", \"Pillai\", \"Prabhakar\", \"Prabhu\", \"Pradhan\", \"Prakash\", \"Prasad\", \"Prashad\", \"Puri\", \"Purohit\", \"Radhakrishnan\", \"Ragavan\", \"Raghavan\", \"Rai\", \"Raj\", \"Raja\",\n",
    "        \"Rajan\", \"Rajagopalan\", \"Raju\", \"Ram\", \"Rama\", \"Raman\", \"Ramanathan\", \"Ramaswamy\", \"Ramachandran\", \"Ramakrishnan\", \"Rangan\", \"Ranganathan\", \"Rao\", \"Rastogi\", \"Ratta\", \"Rattan\", \"Ratti\", \"Rau\", \"Raval\", \"Ravindran\",\n",
    "        \"Ray\", \"Reddy\", \"Roy\", \"Sabharwal\", \"Sachdev\", \"Sachdeva\", \"Sagar\", \"Saha\", \"Sahni\", \"Saini\", \"Salvi\", \"Samarth\", \"Sampath\", \"Sampat\", \"Samuel\", \"Sandhu\", \"Sane\", \"Sanghi\", \"Sanghvi\", \"Sankar\", \"Sankaran\", \"Sant\",\n",
    "        \"Saraf\", \"Sarin\", \"Sarkar\", \"Sarma\", \"Sarna\", \"Sastry\", \"Sathe\", \"Savant\", \"Sawhney\", \"Saxena\", \"Sebastian\", \"Sehgal\", \"Sen\", \"Sengupta\", \"Sequeira\", \"Seth\", \"Sethi\", \"Setty\", \"Shah\", \"Shankar\", \"Sharma\", \"Shenoy\",\n",
    "        \"Sheth\", \"Shetty\", \"Shroff\", \"Shukla\", \"Sinha\", \"Sodhi\", \"Solanki\", \"Som\", \"Soman\", \"Somani\", \"Soni\", \"Sood\", \"Sridhar\", \"Srinivas\", \"Srinivasan\", \"Srivastava\", \"Subramaniam\", \"Subramanian\", \"Sundaram\", \"Sur\", \"Suri\",\n",
    "        \"Swaminathan\", \"Swamy\", \"Tagore\", \"Talwar\", \"Tandon\", \"Tata\", \"Tella\", \"Thakkar\", \"Thakur\", \"Thomas\", \"Tiwari\", \"Trivedi\", \"Upadhyay\", \"Upadhyaya\", \"Vaidya\", \"Varghese\", \"Varkey\", \"Varma\", \"Varman\", \"Vasa\", \"Venkataraman\",\n",
    "        \"Venkatesh\", \"Verma\", \"Vijayakumar\", \"Virk\", \"Viswanathan\", \"Vohra\", \"Vora\", \"Vyas\", \"Wable\", \"Wadhwa\", \"Wagle\", \"Wahi\", \"Walia\", \"Walla\", \"Warrior\", \"Wason\", \"Yadav\", \"Yogi\", \"Zaveri\", \"Zachariah\"\n",
    "    ]\n",
    "    \n",
    "    given_name = random.choice(indian_given_names)\n",
    "    surname = random.choice(indian_surnames)\n",
    "    \n",
    "    return f\"{given_name} {surname}\"\n",
    "\n",
    "def generate_name():\n",
    "    name_generators = [generate_chinese_name, generate_malay_name, generate_indian_name]\n",
    "    chosen_generator = random.choice(name_generators)\n",
    "    return chosen_generator(), chosen_generator\n",
    "\n",
    "\n",
    "\n",
    "def generate_nric():\n",
    "    digits = ''.join(random.choices(string.digits, k=8))\n",
    "    checksum = random.choice(string.ascii_uppercase)\n",
    "    return f\"S{digits}{checksum}\"\n",
    "\n",
    "def generate_phone_number():\n",
    "    return f\"+65 {random.randint(8000, 9999)} {random.randint(1000, 9999)}\"\n",
    "\n",
    "def generate_drivers_license():\n",
    "    return f\"S{random.randint(1000000, 9999999):07d}{random.choice(string.ascii_uppercase)}\"\n",
    "\n",
    "def generate_cpf_number():\n",
    "    return f\"S{random.randint(1000000, 9999999):07d}{random.choice(string.ascii_uppercase)}\"\n",
    "\n",
    "def calculate_age(dob):\n",
    "    today = datetime.now()\n",
    "    return today.year - dob.year - ((today.month, today.day) < (dob.month, dob.day))\n",
    "\n",
    "def generate_passport_number():\n",
    "    return f\"K{random.randint(1000000, 9999999)}{random.choice(string.ascii_uppercase)}\"\n",
    "\n",
    "\n",
    "\n",
    "def generate_date(start_date, end_date):\n",
    "    time_between_dates = end_date - start_date\n",
    "    days_between_dates = time_between_dates.days\n",
    "    random_number_of_days = random.randrange(days_between_dates)\n",
    "    return start_date + timedelta(days=random_number_of_days)\n",
    "\n",
    "def generate_profile():\n",
    "    countries = [\"Singapore\", \"Malaysia\", \"China\", \"India\", \"Indonesia\", \"Philippines\", \"Vietnam\", \"Thailand\", \"Myanmar\", \"Cambodia\", \"Laos\", \"Brunei\", \"Japan\", \"South Korea\", \"North Korea\", \"Taiwan\", \"Hong Kong\", \"Macau\", \"Bangladesh\", \"Sri Lanka\", \"Nepal\", \"Bhutan\", \"Pakistan\", \"Afghanistan\", \"Iran\", \"Iraq\", \"Saudi Arabia\", \"UAE\", \"Oman\", \"Yemen\", \"Qatar\", \"Kuwait\", \"Bahrain\", \"Jordan\", \"Lebanon\", \"Syria\", \"Israel\", \"Palestine\", \"Turkey\", \"Cyprus\", \"Greece\", \"Italy\", \"Spain\", \"Portugal\", \"France\", \"Germany\", \"United Kingdom\", \"Ireland\", \"Netherlands\", \"Belgium\", \"Luxembourg\", \"Switzerland\", \"Austria\", \"Czech Republic\", \"Slovakia\", \"Hungary\", \"Poland\", \"Romania\", \"Bulgaria\", \"Serbia\", \"Croatia\", \"Bosnia and Herzegovina\", \"Montenegro\", \"North Macedonia\", \"Albania\", \"Kosovo\", \"Slovenia\", \"United States\", \"Canada\", \"Mexico\", \"Brazil\", \"Argentina\", \"Chile\", \"Peru\", \"Colombia\", \"Venezuela\", \"Ecuador\", \"Bolivia\", \"Paraguay\", \"Uruguay\", \"Guyana\", \"Suriname\", \"French Guiana\", \"Australia\", \"New Zealand\", \"Papua New Guinea\", \"Fiji\", \"Solomon Islands\", \"Vanuatu\", \"New Caledonia\", \"Egypt\", \"Libya\", \"Tunisia\", \"Algeria\", \"Morocco\", \"Sudan\", \"South Sudan\", \"Ethiopia\", \"Eritrea\", \"Djibouti\", \"Somalia\", \"Kenya\", \"Uganda\", \"Tanzania\", \"Rwanda\", \"Burundi\", \"Congo\", \"Democratic Republic of the Congo\", \"Angola\", \"Zambia\", \"Zimbabwe\", \"Mozambique\", \"Malawi\", \"South Africa\", \"Namibia\", \"Botswana\", \"Lesotho\", \"Eswatini\"]\n",
    "\n",
    "    occupations = [\"Teacher\", \"Engineer\", \"Doctor\", \"Lawyer\", \"Accountant\", \"Nurse\", \"Salesperson\", \"Manager\", \"Chef\", \"Artist\", \"Software Developer\", \"Data Scientist\", \"Architect\", \"Pharmacist\", \"Dentist\", \"Veterinarian\", \"Police Officer\", \"Firefighter\", \"Paramedic\", \"Pilot\", \"Flight Attendant\", \"Electrician\", \"Plumber\", \"Carpenter\", \"Mechanic\", \"Hairdresser\", \"Beautician\", \"Photographer\", \"Journalist\", \"Writer\", \"Editor\", \"Translator\", \"Interpreter\", \"Psychologist\", \"Counselor\", \"Social Worker\", \"Financial Advisor\", \"Insurance Agent\", \"Real Estate Agent\", \"Marketing Specialist\", \"Public Relations Specialist\", \"Human Resources Manager\", \"Graphic Designer\", \"Web Designer\", \"UX Designer\", \"Product Manager\", \"Project Manager\", \"Business Analyst\", \"Systems Analyst\", \"Network Administrator\", \"Database Administrator\", \"Cybersecurity Specialist\", \"Librarian\", \"Museum Curator\", \"Zoologist\", \"Marine Biologist\", \"Environmental Scientist\", \"Geologist\", \"Meteorologist\", \"Astronomer\", \"Physicist\", \"Chemist\", \"Biologist\", \"Mathematician\", \"Statistician\", \"Economist\", \"Political Scientist\", \"Sociologist\", \"Anthropologist\", \"Archaeologist\", \"Historian\", \"Philosopher\", \"Theologian\", \"Actor\", \"Musician\", \"Dancer\", \"Choreographer\", \"Film Director\", \"Producer\", \"Screenwriter\", \"Fashion Designer\", \"Interior Designer\", \"Landscape Architect\", \"Urban Planner\", \"Civil Engineer\", \"Mechanical Engineer\", \"Electrical Engineer\", \"Chemical Engineer\", \"Aerospace Engineer\", \"Biomedical Engineer\", \"Environmental Engineer\", \"Nuclear Engineer\", \"Petroleum Engineer\", \"Agricultural Engineer\", \"Food Scientist\", \"Nutritionist\", \"Dietitian\", \"Personal Trainer\", \"Sports Coach\", \"Athlete\", \"Referee\", \"Umpire\", \"Tour Guide\", \"Travel Agent\", \"Hotel Manager\", \"Restaurant Manager\", \"Bartender\", \"Waiter/Waitress\", \"Housekeeping Staff\", \"Janitor\", \"Security Guard\", \"Locksmith\", \"Tailor\", \"Seamstress\", \"Jeweler\", \"Watchmaker\", \"Optician\", \"Optometrist\", \"Audiologist\", \"Speech Therapist\", \"Occupational Therapist\", \"Physical Therapist\", \"Massage Therapist\", \"Chiropractor\", \"Acupuncturist\", \"Naturopath\", \"Homeopath\", \"Midwife\", \"Doula\", \"Farmer\", \"Rancher\", \"Fisherman\", \"Forester\", \"Gardener\", \"Florist\", \"Botanist\", \"Zoologist\", \"Entomologist\", \"Paleontologist\", \"Geographer\", \"Cartographer\", \"Surveyor\", \"Air Traffic Controller\", \"Ship Captain\", \"Train Conductor\", \"Bus Driver\", \"Taxi Driver\", \"Truck Driver\", \"Courier\", \"Postal Worker\", \"Librarian\", \"Archivist\", \"Curator\", \"Conservator\", \"Restorer\", \"Printer\", \"Bookbinder\", \"Engraver\", \"Sculptor\", \"Painter\", \"Illustrator\", \"Animator\", \"Game Designer\", \"Voice Actor\", \"Stunt Performer\", \"Magician\", \"Circus Performer\", \"Street Performer\", \"Busker\", \"Tattoo Artist\", \"Piercer\", \"Makeup Artist\", \"Special Effects Artist\", \"Prosthetics Technician\", \"Orthodontist\", \"Endodontist\", \"Periodontist\", \"Oral Surgeon\", \"Radiologist\", \"Anesthesiologist\", \"Surgeon\", \"Cardiologist\", \"Neurologist\", \"Oncologist\", \"Pediatrician\", \"Geriatrician\", \"Psychiatrist\", \"Dermatologist\", \"Gynecologist\", \"Urologist\", \"Ophthalmologist\", \"Otolaryngologist\", \"Podiatrist\", \"Nephrologist\", \"Pulmonologist\", \"Rheumatologist\", \"Gastroenterologist\", \"Endocrinologist\", \"Hematologist\", \"Immunologist\", \"Pathologist\", \"Forensic Scientist\", \"Toxicologist\", \"Biochemist\", \"Microbiologist\", \"Virologist\", \"Geneticist\", \"Embryologist\", \"Ecologist\", \"Oceanographer\", \"Seismologist\", \"Volcanologist\", \"Hydrologist\", \"Glaciologist\", \"Climatologist\", \"Astronaut\", \"Cosmonaut\", \"Taikonaut\"]\n",
    "\n",
    "    languages = [\"English\", \"Mandarin\", \"Malay\", \"Tamil\", \"Hokkien\", \"Cantonese\", \"Teochew\", \"Hakka\", \"Hainanese\", \"Hindi\", \"Bengali\", \"Urdu\", \"Punjabi\", \"Gujarati\", \"Malayalam\", \"Telugu\", \"Kannada\", \"Marathi\", \"Tagalog\", \"Indonesian\", \"Vietnamese\", \"Thai\", \"Burmese\", \"Khmer\", \"Lao\", \"Japanese\", \"Korean\", \"Arabic\", \"Persian\", \"Turkish\", \"Russian\", \"French\", \"German\", \"Spanish\", \"Portuguese\", \"Italian\", \"Greek\", \"Dutch\", \"Swedish\", \"Norwegian\", \"Danish\", \"Finnish\", \"Polish\", \"Czech\", \"Slovak\", \"Hungarian\", \"Romanian\", \"Bulgarian\", \"Serbian\", \"Croatian\", \"Bosnian\", \"Albanian\", \"Macedonian\", \"Slovenian\", \"Ukrainian\", \"Belarusian\", \"Lithuanian\", \"Latvian\", \"Estonian\", \"Georgian\", \"Armenian\", \"Azerbaijani\", \"Kazakh\", \"Uzbek\", \"Turkmen\", \"Kyrgyz\", \"Tajik\", \"Mongolian\", \"Tibetan\", \"Nepali\", \"Sinhala\", \"Dzongkha\", \"Tetum\", \"Fijian\", \"Samoan\", \"Tongan\", \"Maori\", \"Hawaiian\", \"Swahili\", \"Zulu\", \"Xhosa\", \"Afrikaans\", \"Amharic\", \"Somali\", \"Yoruba\", \"Igbo\", \"Hausa\", \"Wolof\", \"Fulani\", \"Oromo\", \"Hebrew\", \"Yiddish\", \"Latin\", \"Ancient Greek\", \"Sanskrit\", \"Classical Chinese\", \"Esperanto\"]\n",
    "\n",
    "    blood_types = [\"A+\", \"A-\", \"B+\", \"B-\", \"O+\", \"O-\", \"AB+\", \"AB-\"]\n",
    "\n",
    "    ns_ranks = [\"Private\", \"Lance Corporal\", \"Corporal\", \"Sergeant\", \"Staff Sergeant\", \"2nd Lieutenant\", \"Lieutenant\", \"Captain\"]\n",
    "\n",
    "    marital_statuses = [\"Single\", \"Married\", \"Divorced\", \"Separated\", \"Widowed\", \"Civil Partnership\", \"Domestic Partnership\", \"Engaged\", \"Annulled\"]\n",
    "\n",
    "    religions = [\"Buddhism\", \"Christianity\", \"Islam\", \"Hinduism\", \"Taoism\", \"No Religion\"]\n",
    "    \n",
    "    dob = generate_date(datetime(1950, 1, 1), datetime(2024, 12, 31))\n",
    "\n",
    "    towns = [\n",
    "    \"Ang Mo Kio\", \"Bedok\", \"Tampines\", \"Woodlands\", \"Jurong West\", \"Sengkang\", \"Punggol\",\n",
    "    \"Yishun\", \"Hougang\", \"Jurong East\", \"Choa Chu Kang\", \"Bukit Batok\", \"Toa Payoh\",\n",
    "    \"Serangoon\", \"Bukit Merah\", \"Pasir Ris\", \"Clementi\", \"Bishan\", \"Queenstown\",\n",
    "    \"Bukit Panjang\", \"Kallang\", \"Geylang\", \"Marine Parade\", \"Novena\", \"Tanjong Pagar\",\n",
    "    \"Bukit Timah\", \"Sembawang\", \"Central Area\", \"Rochor\", \"Orchard\", \"Newton\",\n",
    "    \"Outram\", \"River Valley\", \"Downtown Core\", \"Marina South\", \"Straits View\"\n",
    "    ]\n",
    "\n",
    "    streets = [\n",
    "        \"Orchard Road\", \"Serangoon Road\", \"Shenton Way\", \"Raffles Place\", \"Boon Lay Way\",\n",
    "        \"Jurong Gateway Road\", \"Ang Mo Kio Avenue 3\", \"Tampines Avenue 5\", \"Woodlands Avenue 1\",\n",
    "        \"Bedok North Street 1\", \"Punggol Central\", \"Sengkang East Way\", \"Yishun Ring Road\",\n",
    "        \"Hougang Avenue 7\", \"Choa Chu Kang Avenue 4\", \"Bukit Batok East Avenue 3\",\n",
    "        \"Toa Payoh Lorong 1\", \"Bishan Street 22\", \"Clementi Avenue 3\", \"Marine Parade Road\",\n",
    "        \"Pasir Ris Drive 1\", \"Upper Serangoon Road\", \"Upper Thomson Road\", \"Bukit Timah Road\",\n",
    "        \"Jalan Besar\", \"Victoria Street\", \"North Bridge Road\", \"South Bridge Road\",\n",
    "        \"New Upper Changi Road\", \"Eu Tong Sen Street\", \"Telok Blangah Road\", \"Alexandra Road\",\n",
    "        \"Joo Chiat Road\", \"Geylang Road\", \"Kallang Way\", \"Lavender Street\", \"Beach Road\",\n",
    "        \"Balestier Road\", \"Bartley Road\", \"Braddell Road\", \"Bukit Panjang Ring Road\",\n",
    "        \"Commonwealth Avenue\", \"Holland Road\", \"East Coast Road\", \"Sembawang Road\",\n",
    "        \"Mandai Road\", \"Changi Road\", \"Upper East Coast Road\", \"Loyang Avenue\",\n",
    "        \"Admiralty Drive\", \"Yio Chu Kang Road\", \"Lorong Chuan\", \"Kovan Road\", \"Simei Street 1\"\n",
    "    ]\n",
    "    work_pass_types = [\"Employment Pass\", \"S Pass\", \"Work Permit\", \"Dependent's Pass\", \"Long Term Visit Pass\"]\n",
    "    language_proficiencies = [\"Basic\", \"Conversational\", \"Fluent\", \"Native\"]\n",
    "\n",
    "    races = [\"Chinese\", \"Malay\", \"Indian\", \"Others\"]\n",
    "    religions = [\"Buddhism\", \"Christianity\", \"Islam\", \"Hinduism\", \"Taoism\", \"No Religion\"]\n",
    "    towns = [\"Ang Mo Kio\", \"Bedok\", \"Tampines\", \"Woodlands\", \"Jurong West\", \"Sengkang\", \"Punggol\"]\n",
    "    email_providers = [\"gmail.com\", \"hotmail.com\", \"yahoo.com\", \"outlook.com\"]\n",
    "    sg_qualifications = [\"PSLE\", \"N-Levels\", \"O-Levels\", \"A-Levels\", \"Diploma\", \"Bachelor's\", \"Master's\", \"PhD\"]\n",
    "    language_proficiencies = [\"Basic\", \"Conversational\", \"Fluent\", \"Native\"]\n",
    "    ns_statuses = [\"Pre-enlistee\", \"NSF\", \"NSman\", \"Exempted\"]\n",
    "    ns_ranks = [\"Private\", \"Lance Corporal\", \"Corporal\", \"Sergeant\", \"Staff Sergeant\", \"2nd Lieutenant\", \"Lieutenant\", \"Captain\"]\n",
    "    work_pass_types = [\"Employment Pass\", \"S Pass\", \"Work Permit\", \"Dependent's Pass\", \"Long Term Visit Pass\"]\n",
    "\n",
    "    citizenship=random.choice([\"Singapore Citizen\", \"Singapore PR\", \"Foreigner\"])\n",
    "    name, name_generator=generate_name()\n",
    "    profile = {\n",
    "        \"nric\": generate_nric(),\n",
    "        \"name\": name_generator(),  # Assume this function generates culturally appropriate names\n",
    "        \"race\": random.choice(races),\n",
    "        \"gender\": random.choice([\"Male\", \"Female\"]),\n",
    "        \"date_of_birth\": (datetime.now() - timedelta(days=random.randint(6570, 36500))).strftime(\"%Y-%m-%d\"),\n",
    "        \"age\": 0,  # Will be calculated later\n",
    "        \"country_of_birth\": random.choice(countries),\n",
    "        \"citizenship\": citizenship,\n",
    "        \"religion\": random.choice(religions),\n",
    "        \"marital_status\": random.choice(marital_statuses),\n",
    "        \"address\": {\n",
    "            \"block\": f\"{random.randint(1, 999)}\",\n",
    "            \"street No.\": f\"{random.randint(1, 999)}\",\n",
    "            \"street\": f\"{random.choice(streets)}\",\n",
    "            \"unit\": f\"#{random.randint(1, 30)}-{random.randint(1, 999):03d}\",\n",
    "            \"town\": random.choice(towns),\n",
    "            \"postal_code\": f\"{random.randint(100000, 999999)}\"\n",
    "        },\n",
    "        \"phone_number\": generate_phone_number(),\n",
    "        \"email\": generate_random_email(),\n",
    "        \"occupation\": random.choice(occupations),\n",
    "        \"cpf_number\": generate_cpf_number(),\n",
    "        \"education\": {\n",
    "            \"highest_qualification\": random.choice(sg_qualifications),\n",
    "            \"institution\": random.choice([\"NUS\", \"NTU\", \"SMU\", \"SUTD\", \"Local Polytechnic\", \"Local JC\", \"Others\"])\n",
    "        },\n",
    "        \"languages\": {\n",
    "            \"spoken\": {lang: random.choice(language_proficiencies) for lang in random.sample(languages, random.randint(1, 3))},\n",
    "            \"written\": {lang: random.choice(language_proficiencies) for lang in random.sample(languages, random.randint(1, 3))}\n",
    "        },\n",
    "        \"height_cm\": random.randint(150, 190),\n",
    "        \"weight_kg\": random.randint(45, 100),\n",
    "        \"blood_type\": random.choice(blood_types),\n",
    "        \"passport_number\": generate_passport_number(),\n",
    "        \"drivers_license_number\": generate_drivers_license(),\n",
    "        \"national_service\": {\n",
    "            \"status\": None,\n",
    "            \"rank\": None\n",
    "        },\n",
    "        \"immigration_status\": None,  # Will be filled for non-citizens\n",
    "        \"emergency_contact\": {\n",
    "            \"name\": name_generator(),\n",
    "            \"relationship\": random.choice([\"Parent\", \"Sibling\", \"Spouse\", \"Friend\"]),\n",
    "            \"phone_number\": generate_phone_number()\n",
    "        },\n",
    "        \"deceased\": random.choice([True, False])\n",
    "    }\n",
    "\n",
    "    # Calculate age\n",
    "    profile[\"age\"] = calculate_age(datetime.strptime(profile[\"date_of_birth\"], \"%Y-%m-%d\"))\n",
    "\n",
    "    # Set NS status for males\n",
    "    if profile[\"gender\"] == \"Male\" and profile[\"citizenship\"] in [\"Singapore Citizen\", \"Singapore PR\"]:\n",
    "        if profile[\"age\"] < 18:\n",
    "            profile[\"national_service\"][\"status\"] = \"Pre-enlistee\"\n",
    "        elif 18 <= profile[\"age\"] <= 20:\n",
    "            profile[\"national_service\"][\"status\"] = \"NSF\"\n",
    "            profile[\"national_service\"][\"rank\"] = random.choice(ns_ranks[:5])  # Lower ranks for NSF\n",
    "        elif profile[\"age\"] > 20:\n",
    "            profile[\"national_service\"][\"status\"] = \"NSman\"\n",
    "            profile[\"national_service\"][\"rank\"] = random.choice(ns_ranks)\n",
    "\n",
    "    # Set immigration status for non-citizens\n",
    "    if profile[\"citizenship\"] == \"Foreigner\":\n",
    "        profile[\"immigration_status\"] = random.choice(work_pass_types)\n",
    "\n",
    "    # Set date of death if deceased\n",
    "    if profile[\"deceased\"]:\n",
    "        # max_days = (datetime.now() - datetime.strptime(profile[\"date_of_birth\"], \"%Y-%m-%d\")).days\n",
    "        # profile[\"date_of_death\"] = (datetime.now() - timedelta(days=random.randint(0, max_days))).strftime(\"%Y-%m-%d\")\n",
    "        dob = datetime.strptime(profile[\"date_of_birth\"], \"%Y-%m-%d\")\n",
    "        age_in_days = profile[\"age\"] * 365  # This is an approximation, not accounting for leap years\n",
    "        date_of_death = dob + timedelta(days=age_in_days)\n",
    "        profile[\"date_of_death\"] = date_of_death.strftime(\"%Y-%m-%d\")\n",
    "    return profile\n",
    "\n",
    "def generate_profiles(n):\n",
    "    return [generate_profile() for _ in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23166769-bcb2-4203-9d55-2b1efcf4dd14",
   "metadata": {},
   "source": [
    "# UPLOAD DATA TO ELASTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15795c-3424-43d9-8ec1-04a72374e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def bulk_upload_pickle_to_elasticsearch(file_path, index_name, es, batch_size=1000):\n",
    "    \n",
    "    total_uploaded = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    def create_action(doc):\n",
    "        # doc=merge_nested_dictionaries(doc, default_template)\n",
    "        return {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": uuid.uuid4(),\n",
    "            \"_source\": doc\n",
    "        }\n",
    "\n",
    "    def read_and_upload_batch(data):\n",
    "        batch = []\n",
    "        for doc in data:\n",
    "            batch.append(create_action(doc))\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if batch:\n",
    "            yield batch\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data=pickle.load(f)\n",
    "        for batch in read_and_upload_batch(data):\n",
    "            try:\n",
    "                success, failed = helpers.bulk(es, batch, raise_on_error=False)\n",
    "                if type(failed) is list: \n",
    "                    failed=len(failed)\n",
    "                total_uploaded += success\n",
    "                total_failed += failed\n",
    "                print(f\"Uploaded {success} documents, Failed {failed} documents\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during bulk upload: {str(e)}\")\n",
    "                total_failed += len(batch)\n",
    "\n",
    "    return total_uploaded, total_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd485616-05cd-4373-941c-4b98c2a8d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d983ead-9c87-4f2d-9535-72f9d3c78fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    es_endpoint =\"http://127.0.0.1:9200\"\n",
    "    es_client = Elasticsearch(\n",
    "        es_endpoint,\n",
    "        #api_key=os.environ.get(\"ELASTIC_API_KEY\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"No Client\")\n",
    "    es_client=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a34c0e-e703-4de2-b6bf-4fa673aaf666",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"langchain-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a6b350-cc98-490e-bf55-44c405b8f4f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runs=200\n",
    "profile_batch_size=1000\n",
    "\n",
    "for i in range(runs):\n",
    "    profiles = generate_profiles(profile_batch_size)\n",
    "    filename='./data/personal_info.pkl'\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(profiles, file)\n",
    "    try:\n",
    "        bulk_upload_pickle_to_elasticsearch(filename, \"langchain-demo\", es_client)\n",
    "    except Exception as e:\n",
    "        print(traceback.print_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad4a33-c433-43e6-a056-07351f800d38",
   "metadata": {},
   "source": [
    "# GET ELASTIC SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af18c49-eac7-4306-92ff-5f3127a685cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Replace with your Elasticsearch host and index name\n",
    "es_host = \"http://localhost:9200\"\n",
    "index_name = \"langchain-demo\"\n",
    "\n",
    "# Make the request to get the mapping\n",
    "response = requests.get(f\"{es_host}/{index_name}/_mapping\")\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    mapping = response.json()\n",
    "    with open(\"langchain-demo-mapping.json\", \"w\") as f:\n",
    "        json.dump(mapping, f, indent=4)\n",
    "\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d5fd68-fbe2-48db-b816-ff15de839158",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad91c9-1b89-4242-b110-94012c52d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping['langchain-demo']['mappings']['properties'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c36c2-e076-4006-b7c2-4943306adcc3",
   "metadata": {},
   "source": [
    "# QUERY ELASTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56587fdd-5692-40f5-ae31-368ffc69c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {'query': {'bool': {'minimum_should_match': 3,\n",
    "                    'should': [{'match': {'gender': 'female'}},\n",
    "                               {'match': {'deceased': True}},\n",
    "                               {'match': {'blood_type': 'o-'}},\n",
    "                               {'match': {'country_of_birth': 'singapore'}}]}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19745057-12fd-4fc4-b9a0-90149710c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = es_client.search(index=index, body=query)\n",
    "\n",
    "total_hits = search_results['hits']['total']['value']\n",
    "print(f\"Total matches: {total_hits}\")\n",
    "\n",
    "for hit in search_results['hits']['hits']:\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Name: {hit['_source']['name']}\")\n",
    "    print(f\"Blood Type: {hit['_source']['blood_type']}\")\n",
    "    print(f\"Gender: {hit['_source']['gender']}\")\n",
    "    print(f\"Country of Birth: {hit['_source']['country_of_birth']}\")\n",
    "    print(f\"Deceased: {hit['_source']['deceased']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2962012b-f4c2-458d-b77a-e106ca57823b",
   "metadata": {},
   "source": [
    "# BUILD RAG COMPONENT AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cdd2a-3742-4778-ad93-9d042c7c4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='''\n",
    "You are an AI assistant specialized in converting natural language queries into Elasticsearch queries. Your task is to interpret user questions about personal profiles and generate the appropriate Elasticsearch query in JSON format.\n",
    "\n",
    "The document schema for the profiles is as follows:\n",
    "\n",
    "{\n",
    "  \"nric\": \"string\",\n",
    "  \"name\": \"string\",\n",
    "  \"race\": \"string\",\n",
    "  \"gender\": \"string\",\n",
    "  \"date_of_birth\": \"date\",\n",
    "  \"age\": \"integer\",\n",
    "  \"country_of_birth\": \"string\",\n",
    "  \"citizenship\": \"string\",\n",
    "  \"religion\": \"string\" [\"Buddhism\", \"Christianity\", \"Islam\", \"Hinduism\", \"Taoism\", \"No Religion\"],\n",
    "  \"marital_status\": \"string\" [\"Single\", \"Married\", \"Divorced\", \"Separated\", \"Widowed\", \"Civil Partnership\", \"Domestic Partnership\", \"Engaged\", \"Annulled\"],\n",
    "  \"address\": {\n",
    "    \"block\": \"string\",\n",
    "    \"street_no\": \"string\",\n",
    "    \"street\": \"string\",\n",
    "    \"unit\": \"string\",\n",
    "    \"town\": \"string\",\n",
    "    \"postal_code\": \"string\"\n",
    "  },\n",
    "  \"phone_number\": \"string\",\n",
    "  \"email\": \"string\",\n",
    "  \"occupation\": \"string\",\n",
    "  \"cpf_number\": \"string\",\n",
    "  \"education\": {\n",
    "    \"highest_qualification\": \"string\",\n",
    "    \"institution\": \"string\"\n",
    "  },\n",
    "  \"languages\": {\n",
    "    \"spoken\": {\"language\":\"fluency\" [\"Basic\", \"Conversational\", \"Fluent\", \"Native\"]},\n",
    "    \"written\": {\"language\":\"fluency\" [\"Basic\", \"Conversational\", \"Fluent\", \"Native\"]},\n",
    "  },\n",
    "  \"height_cm\": \"integer\",\n",
    "  \"weight_kg\": \"integer\",\n",
    "  \"blood_type\": \"string\" [\"A+\", \"A-\", \"B+\", \"B-\", \"O+\", \"O-\", \"AB+\", \"AB-\"],\n",
    "  \"passport_number\": \"string\",\n",
    "  \"drivers_license_number\": \"string\",\n",
    "  \"national_service\": {\n",
    "    \"status\": \"string\",\n",
    "    \"rank\": \"string\"\n",
    "  },\n",
    "  \"immigration_status\": \"string\",\n",
    "  \"emergency_contact\": {\n",
    "    \"name\": \"string\",\n",
    "    \"relationship\": \"string\",\n",
    "    \"phone_number\": \"string\"\n",
    "  },\n",
    "  \"deceased\": \"boolean\",\n",
    "  \"date_of_death\": \"date\"\n",
    "}\n",
    "-----------------------------------------------------------------------------------\n",
    "Example query 1:\n",
    "User: Find all male Singapore citizens between 25 and 30 years old who work as software developers and speak fluent English.\n",
    "\n",
    "Your response should be:\n",
    "\n",
    "{\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"should\": [\n",
    "        { \"match\": { \"gender\": \"Male\" } },\n",
    "        { \"match\": { \"citizenship\": \"Singapore Citizen\" } },\n",
    "        { \"range\": { \"age\": { \"gte\": 25, \"lte\": 30 } } },\n",
    "        { \"match\": { \"occupation\": \"Software Developer\" } },\n",
    "        {\n",
    "          \"match\": {\n",
    "            \"languages.spoken.English\": {\n",
    "              \"query\": \"Fluent\",\n",
    "              \"fuzziness\": \"AUTO\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ],\n",
    "      \"minimum_should_match\": 2\n",
    "    }\n",
    "  }\n",
    "}\n",
    "-------------------------------\n",
    "Example query 2:\n",
    "User: All non-Singaporean men over the age of 25 who are software people living in woodlands \n",
    "\n",
    "Your response should be:\n",
    "{\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "            \"minimum_should_match\": 4,\n",
    "            \"should\": [{\n",
    "                \"bool\": {\n",
    "                  \"must_not\": [\n",
    "                  {\"match\": {\"citizenship\": \"singapore citizen\"}}\n",
    "                             ]\n",
    "                            }\n",
    "                        },\n",
    "                               {\"match\": {\"gender\": \"male\"}},\n",
    "                               {\"range\": {\"age\": {\"gte\": 25}}},\n",
    "                               {\"multi_match\": {\"fields\": [\"occupation\",\n",
    "                                                           \"job_title\",\n",
    "                                                           \"role\"],\n",
    "                                                \"fuzziness\": \"AUTO\",\n",
    "                                                \"query\": \"software\",\n",
    "                                                \"type\": \"best_fields\"}},\n",
    "                               {\"match\": {\"address.town\": {\"fuzziness\": \"AUTO\",\n",
    "                                                           \"query\": \"woodlands\"}}}]}}}\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    "Consider using multi_match for fields that might contain the value in different subfields:\n",
    "{\n",
    "  \"multi_match\": {\n",
    "    \"query\": \"Software Developer\",\n",
    "    \"fields\": [\"occupation\", \"job_title\", \"role\"],\n",
    "    \"type\": \"best_fields\",\n",
    "    \"fuzziness\": \"AUTO\"\n",
    "  }\n",
    "}\n",
    "\n",
    "For names or other fields where word order matters, you might want to use match_phrase with slop:\n",
    "{\n",
    "  \"match_phrase\": {\n",
    "    \"full_name\": {\n",
    "      \"query\": \"John Doe\",\n",
    "      \"slop\": 1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "when using minimum_should_match, try to guaranty that the minumum number of clauses satisface the user query in Natural Language\n",
    "\n",
    "When dealing with queries that involve categories, groups, or regions (such as language families, geographical areas, or professional fields), expand the search to include all relevant specific instances. \n",
    "For example, if asked about Slavic languages, include searches for Russian, Polish, Czech, etc. If asked about people from Europe, include searches for various European countries.\n",
    "\n",
    "\n",
    "Generate a JSON query for Elasticsearch. Provide only the raw JSON without any surrounding tags or markdown formatting, because we need to convert your response to an object. \n",
    "Use a lenient approach with 'should' clauses instead of strict 'must' clauses. Include a 'minimum_should_match' parameter to ensure some relevance while allowing flexibility. Avoid using 'must' clauses entirely.\n",
    "All queries must be lowercase.\n",
    "\n",
    "Use 'match' queries instead of 'term' queries to allow for partial matches and spelling variations. Where appropriate, include fuzziness parameters to further increase tolerance for spelling differences. \n",
    "For name fields or other phrases where word order matters, consider using 'match_phrase' with a slop parameter. Use 'multi_match' for fields that might contain the value in different subfields.\n",
    "\n",
    "Try to create a query which satisfaces most closely what the user is requesting.\n",
    "let's think step by step\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376888a2-18d7-4006-b7ac-1b35ac3ebc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELastic Query Model\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=\"llama3.2:latest\", format=\"json\", temperature=0.1, top_p=.9, tok_k=30, num_ctx=8192)\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> {prompt}\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"prompt\"],\n",
    ")\n",
    "\n",
    "elastic_llm = prompt2 | llm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f0d3af-71d5-4cb8-9b5a-0f49e7d91889",
   "metadata": {},
   "source": [
    "# LLama 3.2 3Billions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b0ae88-bcf3-4967-9f41-3ff7620d333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"All men over the age of 35, who  are living in Tanjong Pagar\"\n",
    "\n",
    "response =elastic_llm.invoke({\"question\": question, \"prompt\": prompt})\n",
    "\n",
    "es_query=json.loads(response.content)\n",
    "pprint(es_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287d09a-b88c-4b6a-94ac-2cc93e433795",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = es_client.search(index=index, body=es_query)\n",
    "\n",
    "total_hits = search_results['hits']['total']['value']\n",
    "print(f\"Total matches: {total_hits}\")\n",
    "\n",
    "for hit in search_results['hits']['hits']:\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Name: {hit['_source']['name']}\")\n",
    "    print(f\"Age: {hit['_source']['age']}\")\n",
    "    print(f\"Gender: {hit['_source']['gender']}\")\n",
    "    print(f\"Citizenship: {hit['_source']['citizenship']}\")\n",
    "    print(f\"Occupation: {hit['_source']['occupation']}\")\n",
    "    print(f\"Address: {hit['_source']['address']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbb814-c72a-4dfa-ae34-ef4d597fd00a",
   "metadata": {},
   "source": [
    "# Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8e0c6-569b-43d9-8e06-65fb5c97b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=  \"All men over the age of 35, who  are living in Tanjong Pagar\"\n",
    "response=LLM.generate_non_streaming_response(query, system_prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca785f3-af03-4bd5-bd35-8339561944c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3028c05-1544-4744-815b-052c40dea794",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_query=json.loads(response)\n",
    "pprint(es_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87be801b-04dc-4baf-bdaf-51ef944d96ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = es_client.search(index=index, body=es_query)\n",
    "\n",
    "total_hits = search_results['hits']['total']['value']\n",
    "print(f\"Total matches: {total_hits}\")\n",
    "\n",
    "for hit in search_results['hits']['hits']:\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Name: {hit['_source']['name']}\")\n",
    "    print(f\"Age: {hit['_source']['age']}\")\n",
    "    print(f\"Gender: {hit['_source']['gender']}\")\n",
    "    print(f\"Citizenship: {hit['_source']['citizenship']}\")\n",
    "    print(f\"Occupation: {hit['_source']['occupation']}\")\n",
    "    print(f\"Address: {hit['_source']['address']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b8196-aa2c-4516-9aa3-edc6038dc704",
   "metadata": {},
   "source": [
    "# LLama 3.2 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388bdf6-13da-44dc-853b-c9398381f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Women who are not alive currently, who are universal blood donors born in singapore\"\n",
    "\n",
    "response =elastic_llm.invoke({\"question\": question, \"prompt\": prompt})\n",
    "\n",
    "es_query=json.loads(response.content)\n",
    "pprint(es_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d71ba-ea08-4df2-819e-a2acc4af69d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = es_client.search(index=index, body=es_query)\n",
    "\n",
    "total_hits = search_results['hits']['total']['value']\n",
    "print(f\"Total matches: {total_hits}\")\n",
    "\n",
    "for hit in search_results['hits']['hits']:\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Name: {hit['_source']['name']}\")\n",
    "    print(f\"Blood Type: {hit['_source']['blood_type']}\")\n",
    "    print(f\"Gender: {hit['_source']['gender']}\")\n",
    "    print(f\"Country of Birth: {hit['_source']['country_of_birth']}\")\n",
    "    print(f\"Deceased: {hit['_source']['deceased']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec18609-8c73-4da0-8151-0c8b00d67649",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Women who are not alive currently, who are universal blood donors born in singapore\" \n",
    "\n",
    "response=LLM.generate_non_streaming_response(query, system_prompt=prompt)\n",
    "es_query=json.loads(response)\n",
    "pprint(es_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2adb7c-8538-493b-87d5-54e0df6f9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = es_client.search(index=index, body=es_query)\n",
    "\n",
    "total_hits = search_results['hits']['total']['value']\n",
    "print(f\"Total matches: {total_hits}\")\n",
    "\n",
    "for hit in search_results['hits']['hits']:\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Name: {hit['_source']['name']}\")\n",
    "    print(f\"Blood Type: {hit['_source']['blood_type']}\")\n",
    "    print(f\"Gender: {hit['_source']['gender']}\")\n",
    "    print(f\"Country of Birth: {hit['_source']['country_of_birth']}\")\n",
    "    print(f\"Deceased: {hit['_source']['deceased']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4af11b-c60f-478d-95c0-bc408162ea4a",
   "metadata": {},
   "source": [
    "# LLama 3.2 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209854e6-e087-4ba9-8eab-1296c7ff65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"People with height equal to 175 centimeters\" \n",
    "\n",
    "response =elastic_llm.invoke({\"question\": question, \"prompt\": prompt})\n",
    "\n",
    "es_query=json.loads(response.content)\n",
    "pprint(es_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6191c-bd5c-4e50-91f1-d06c3a6f447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = es_client.search(index=index, body=es_query)\n",
    "\n",
    "total_hits = search_results['hits']['total']['value']\n",
    "print(f\"Total matches: {total_hits}\")\n",
    "\n",
    "for hit in search_results['hits']['hits']:\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Name: {hit['_source']['name']}\")\n",
    "    print(f\"languages: {hit['_source']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4211d645-e221-43a8-8057-86087220bc2f",
   "metadata": {},
   "source": [
    "# Gemini 1.5 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5added-9e5b-495b-b4d4-b400d7d12192",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"People which height is equal to 175 centimeters\" \n",
    "\n",
    "response=LLM.generate_non_streaming_response(query, system_prompt=prompt)\n",
    "es_query=json.loads(response)\n",
    "pprint(es_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1192ce56-c938-4a3d-b897-ce85b7d40181",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = es_client.search(index=index, body=es_query)\n",
    "\n",
    "total_hits = search_results['hits']['total']['value']\n",
    "print(f\"Total matches: {total_hits}\")\n",
    "\n",
    "for hit in search_results['hits']['hits']:\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Name: {hit['_source']['name']}\")\n",
    "    print(f\"languages: {hit['_source']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65ccc08-8f17-4391-afa7-3c81a6b1d83e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
