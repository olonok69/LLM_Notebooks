{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c889268-9935-4cd6-ba86-a78b39ef49bf",
   "metadata": {},
   "source": [
    "# Multi-modal RAG with Google Cloud\n",
    "\n",
    "\n",
    "### MultiVector Retriever\n",
    "It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base MultiVectorRetriever which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the MultiVectorRetriever.\n",
    "\n",
    "The methods to create multiple vectors per document include:\n",
    "\n",
    "- Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever).\n",
    "- Summary: create a summary for each document, embed that along with (or instead of) the document.\n",
    "- Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the documen\n",
    "\n",
    "![image.png](multi.png)\n",
    "\n",
    "- We will use Unstructured to parse images, text, and tables from documents (PDFs).\n",
    "- We will use the multi-vector retriever with Chroma to store raw text and images along with their summaries for retrieval.\n",
    "- We will use Gemini-pro 1.5 fast for both image summarization (for retrieval) as well as final answer synthesis from join review of images and texts (or tables).\n",
    "\n",
    "#  LangChain Google\n",
    "https://github.com/langchain-ai/langchain-google\n",
    "\n",
    "# Choma\n",
    "https://github.com/chroma-core/chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b061fa1-8006-45ea-9ea8-69064c1a130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8109b5a-9b12-4c0f-aed4-bb60e4dff294",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\"./keys/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec73f0-4287-4ac0-90ee-8ecc761539b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tempfile\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import dotenv_values\n",
    "import json\n",
    "import vertexai\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import chromadb\n",
    "import uuid  \n",
    "import itertools\n",
    "import time\n",
    "import logging\n",
    "import zipfile\n",
    "import requests\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0c15d-4ea2-4036-897f-9de50c14023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\"./keys/.env\")\n",
    "with open(\"./keys/complete-tube-421007-208a4862c992.json\") as source:\n",
    "    info = json.load(source)\n",
    "\n",
    "vertex_credentials = service_account.Credentials.from_service_account_info(info)\n",
    "vertexai.init(\n",
    "    project=config[\"PROJECT\"],\n",
    "    location=config[\"REGION\"],\n",
    "    credentials=vertex_credentials,\n",
    ")\n",
    "google_api_key = config[\"GEMINI-API-KEY\"]\n",
    "os.environ[\"GEMINI_API_KEY\"] = google_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e935408a-a875-4596-af4b-a9a7a478076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT= os.getcwd()\n",
    "ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54410c5-726a-4c0f-a31c-db568be4ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First download\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "data_url = \"https://storage.googleapis.com/benchmarks-artifacts/langchain-docs-benchmarking/cj.zip\"\n",
    "result = requests.get(data_url)\n",
    "filename = \"zips/cj.zip\"\n",
    "with open(filename, \"wb\") as file:\n",
    "    file.write(result.content)\n",
    "\n",
    "with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276540b9-b224-4cd7-b151-36ceda5e5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./cj/cj.pdf\", extract_images=True)\n",
    "docs = loader.load()\n",
    "tables = []\n",
    "texts = [d.page_content for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e28b7-d049-4a82-aadc-ec7112b35789",
   "metadata": {},
   "source": [
    "# Multi-vector retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391180a-0940-46b8-88a7-5aba32ec2d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_text)\n",
    "    empty_response = RunnableLambda(\n",
    "        lambda x: AIMessage(content=\"Error processing document\")\n",
    "    )\n",
    "    # Text summary chain\n",
    "    model = VertexAI(\n",
    "        temperature=0, model_name=\"gemini-1.5-flash-002\", max_tokens=1024, credentials=vertex_credentials,\n",
    "    ).with_fallbacks([empty_response])\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 1})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "# Get text, table summaries\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "    texts, tables, summarize_texts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b1d41-eea6-4e06-9564-105a08312587",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_summaries), len(table_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08fc13a-149f-46f9-b75e-784d123c1d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b63008-d12e-4eb6-8c8f-f1a3a620555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be3ced-51cb-42ac-991a-10754d71ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04c65b-eeb0-4811-9ce7-77f44d0a1670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    model = ChatVertexAI(model=\"gemini-1.5-flash-002\", max_tokens=1024,credentials=vertex_credentials)\n",
    "\n",
    "    msg = model.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n",
    "\n",
    "# Image summaries\n",
    "img_base64_list, image_summaries = generate_img_summaries(\"./cj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adca777-07a3-4831-acdd-4412ddf93c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_base64_list[0][:10], image_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f618c9-4bc7-4940-bc4a-765a4fdf1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c4b97-3faa-4217-99d8-2b0a12cd1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409cc399-9e14-40c6-8494-8f239ae8f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_vector_retriever(\n",
    "    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "#The vectorstore to use to index the summaries\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"mm_rag\",\n",
    "    embedding_function=VertexAIEmbeddings(model_name=\"textembedding-gecko@latest\", credentials=vertex_credentials),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb1ca7-f7e6-4fc8-9632-672fa4d5ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries,\n",
    "    texts,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a591b-7290-453b-b4a8-6c5889199273",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_multi_vector_img.vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec4035-bfa2-48ea-85a4-06f4ea16a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Display base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    if len(b64_images) > 0:\n",
    "        return {\"images\": b64_images[:1], \"texts\": []}\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are financial analyst tasking with providing investment advice.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "            \"Use this information to provide investment advice related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatVertexAI(temperature=0, model_name=\"gemini-1.5-flash-002\", max_tokens=1024,credentials=vertex_credentials)\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d07d7-aa4c-4970-8f73-b40c8d984b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c848944-f155-4d03-a811-81ce96c9eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the EV / NTM and NTM rev growth for MongoDB, Cloudflare, and Datadog?\"\n",
    "docs = retriever_multi_vector_img.invoke(query, limit=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7203f6c4-5140-4984-97cc-8e3bd980128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_img_base64(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293faec-7cbb-4e9f-9fe5-36d587d75f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_multimodal_rag.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf30f8d2-8bbb-4309-8d00-38de1ccb0650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f271e34-fa08-4523-9602-e319417348aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
