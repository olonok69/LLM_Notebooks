{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc2879f-d33a-408b-8b75-93637f14fa4b",
   "metadata": {},
   "source": [
    "# NVIDIA NIMs\n",
    "The langchain-nvidia-ai-endpoints package contains LangChain integrations building applications with models on NVIDIA NIM inference microservice. NIM supports models across domains like chat, embedding, and re-ranking models from the community as well as NVIDIA. These models are optimized by NVIDIA to deliver the best performance on NVIDIA accelerated infrastructure and deployed as a NIM, an easy-to-use, prebuilt containers that deploy anywhere using a single command on NVIDIA accelerated infrastructure.\n",
    "\n",
    "- https://python.langchain.com/v0.2/docs/integrations/chat/nvidia_ai_endpoints/\n",
    "- https://build.nvidia.com/explore/discover\n",
    "\n",
    "https://pypi.org/project/langchain-nvidia-ai-endpoints/\n",
    "\n",
    "\n",
    "# PHI3 128k\n",
    "\n",
    "Phi-3-Small is a lightweight, state-of-the-art open model built upon datasets used for Phi-2 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family, and the small version comes in two variants 8K and 128K which is the context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. This model is ready for commercial and research use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fb46e-a6fd-4513-8ea9-1032ad058923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m pip install -r requirements.txt --user --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca81034-edb3-405c-8760-ff7dc81f6b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae8325a-3316-45b4-ba90-9ac30106ef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "## API Key can be found by going to NVIDIA NGC -> AI Foundation Models -> (some model) -> Get API Code or similar.\n",
    "## 10K free queries to any endpoint (which is a lot actually).\n",
    "\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110c37d-7beb-4d53-b074-0e591cf387a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvapi_key=  os.getenv(\"NVIDIA_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f08779-54f9-4e31-bf03-39134d1d878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "llm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\", max_tokens=419)\n",
    "[model.id for model in llm.available_models if model.model_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e62a10-9f0b-4d59-b989-c31991f14026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run and see that you can genreate a respond successfully\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(model=\"microsoft/phi-3-small-128k-instruct\", nvidia_api_key=nvapi_key, max_tokens=1024)\n",
    "\n",
    "result = llm.invoke(\"Write a ballad about LangChain.\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b86dfb-8b48-4e5e-925f-086a233e662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-mistral-7b-v2\", model_type=\"passage\")\n",
    "\n",
    "# Alternatively, if you want to specify whether it will use the query or passage type\n",
    "# embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\", model_type=\"passage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec276b3-a3e8-47f6-958a-6a9fb2a1853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.available_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53600c-edc6-438d-887b-e46087326743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import re\n",
    "# Here we read in the text data and prepare them into vectorstore\n",
    "ps = os.listdir(\"./data/\")\n",
    "data = []\n",
    "sources = []\n",
    "for p in ps:\n",
    "    if p.endswith('.txt'):\n",
    "        path2file=\"./data/\"+p\n",
    "        print(path2file)\n",
    "        with open(path2file, encoding=\"utf-8\") as f:\n",
    "            lines=f.readlines()\n",
    "            for line in lines:\n",
    "                text = line.replace(\"\\n\", \"\")\n",
    "                text = text.replace(\" \", \"\")\n",
    "                if len(line)>=1 and len(text) >1:\n",
    "                    data.append(line)\n",
    "                    sources.append(path2file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9575e840-6814-4303-b836-b84bdd8dee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[d for d in data if d != '\\n']\n",
    "len(data), len(documents), data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e592d-a481-416f-b9b0-59d483b149d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997b8e2-4d3c-4681-8fdf-a20bde4c4655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Single Document Embedding: \")\n",
    "s = time.perf_counter()\n",
    "q_embedding  = embedder.embed_documents([documents[0]])\n",
    "elapsed = time.perf_counter() - s\n",
    "print(\"\\033[1m\" + f\"Executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\n",
    "print(\"Shape:\", (len(q_embedding),))\n",
    "\n",
    "print(\"\\nBatch Document Embedding: \")\n",
    "s = time.perf_counter()\n",
    "d_embeddings = embedder.embed_documents(documents[:10])\n",
    "elapsed = time.perf_counter() - s\n",
    "print(\"\\033[1m\" + f\"Executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\n",
    "print(\"Shape:\",len(d_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a8fd0-8435-4046-987c-a3e212464c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a vector store from the documents and save it to disk.\n",
    "from operator import itemgetter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "import faiss\n",
    "# create my own uuid\n",
    "text_splitter = CharacterTextSplitter(chunk_size=400, separator=\" \")\n",
    "docs = []\n",
    "metadatas = []\n",
    "\n",
    "for i, d in enumerate(documents):\n",
    "    splits = text_splitter.split_text(d)\n",
    "    #print(len(splits))\n",
    "    docs.extend(splits)\n",
    "    metadatas.extend([{\"source\": sources[i]}] * len(splits))\n",
    "\n",
    "store = FAISS.from_texts(docs, embedder , metadatas=metadatas)\n",
    "store.save_local('./data/nv_embedding')\n",
    "\n",
    "# you will only need to do this once, later on we will restore the already saved vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a8e3a5-4d73-4965-b601-86b9dc7a8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vectorestore back.\n",
    "\n",
    "store = FAISS.load_local(\"./data/nv_embedding\", embedder, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6edbd-92a8-4897-ae02-9ce7d3ea9818",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = store.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer solely based on the following context:\\n<Documents>\\n{context}\\n</Documents>\",\n",
    "        ),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"Tell me about Great Gatsby.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9cffd1-f970-486e-9ff7-5e5cf370a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Can you explain me the role of Jordan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9146c8-d803-4bea-9b77-534d4ec4f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Who is Rich Draves?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc509b26-562e-4be9-b087-eb05552eca45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sk)",
   "language": "python",
   "name": "sk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
