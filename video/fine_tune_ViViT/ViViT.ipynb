{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4740ba12-621d-423c-8def-e0ee150f3fec",
   "metadata": {},
   "source": [
    "# Fine-tuning for Video Classification with ðŸ¤— Transformers\n",
    "### Abstract\n",
    "We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatio-temporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks. To facilitate further research, we release code at https://github.com/google-research/scenic/tree/main/scenic/projects/vivit\n",
    "\n",
    "https://arxiv.org/pdf/2103.15691\n",
    "\n",
    "![image.png](vivit.png)\n",
    "\n",
    "\n",
    "## Embeddings\n",
    "### Uniform frame sampling \n",
    "straightforward method of tokenising the input video is to uniformly sample nt frames from the input video clip, embed each 2D frame independently using the same method as ViT, and concatenate all these tokens together. Concretely, if nh Â· nw non-overlapping image patches are extracted from each frame, then a total of nt Â·nhÂ·nw tokens will be forwarded through the transformer encoder.Intuitively, this process may be seen as simply constructing a large 2D image to be tokenised following ViT\n",
    "\n",
    "#### Tubelet embedding\n",
    "An alternate method, to extract non-overlapping, spatio-temporal â€œtubesâ€ from the input volume, and to linearly project this to Rd. This method is an extension of ViTâ€™s embedding to 3D,and corresponds to a 3D convolution. \n",
    "\n",
    "### HF Vivit\n",
    "https://huggingface.co/docs/transformers/main/model_doc/vivit\n",
    "\n",
    "# Dataset\n",
    "https://paperswithcode.com/dataset/kinetics-400-1\n",
    "\n",
    "# Download Dataset sayakpaul/ucf101-subset\n",
    "#### Complete UCF101\n",
    "UCF101 is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. This data set is an extension of UCF50 data set which has 50 action categories.\n",
    "\n",
    "With 13320 videos from 101 action categories, UCF101 gives the largest diversity in terms of actions and with the presence of large variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background, illumination conditions, etc, it is the most challenging data set to date. As most of the available action recognition data sets are not realistic and are staged by actors, UCF101 aims to encourage further research into action recognition by learning and exploring new realistic action categories.\n",
    "\n",
    "https://www.crcv.ucf.edu/research/data-sets/ucf101/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b64e8f-95bb-4fc4-a9ee-f8254e5e9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "hf_dataset_identifier = \"sayakpaul/ucf101-subset\"\n",
    "filename = \"UCF101_subset.tar.gz\"\n",
    "file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\", local_dir=\".\")\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d944a0f-bd4d-4433-8b2d-3236efb2e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2683a6-75ef-4dab-b9a3-2af7c44f8b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "with tarfile.open(\"UCF101_subset.tar.gz\") as t:\n",
    "     t.extractall(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feac456-e72d-4f65-b11d-723561752f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer, TrainingArguments, AdamW\n",
    "from model_configuration import *\n",
    "from transformers import Trainer\n",
    "from preprocessing import create_dataset\n",
    "from data_handling import frames_convert_and_create_dataset_dictionary\n",
    "from model_configuration import initialise_model\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54835acc-68f5-4431-8cc7-a20dc01b6c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "env_path =  \".env\"\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d89a96-62be-4451-91bf-16958edc5232",
   "metadata": {},
   "source": [
    "# Base Model\n",
    "\n",
    "https://github.com/google-research/scenic/tree/main/scenic/projects/vivit\n",
    "\n",
    "### google/vivit-f-16x2-kinetics400\n",
    "\n",
    "![image.png](models.png)\n",
    "\n",
    "\n",
    "##### https://huggingface.co/docs/transformers/main/model_doc/vivit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89491e-49f7-4bdf-ac77-a44ac2ebae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_configuration\n",
    "from model_configuration import compute_metrics\n",
    "import cv2\n",
    "import av\n",
    "from data_handling import sample_frame_indices, read_video_pyav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ec3e5-ce10-43f4-99bc-e91758e40790",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = av.open(\"./data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c01.avi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95055c36-4a00-47a9-a708-f09142a35469",
   "metadata": {},
   "outputs": [],
   "source": [
    "container.streams.video[0].frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f514c6-e965-44be-ba98-2264406b88fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install moviepy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c88329-7c33-4004-ba2d-846627bcb698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52eea4a-b495-4628-905f-d64e14d48bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = av.open(\"./data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c01.avi\")\n",
    "indices = sample_frame_indices(clip_len=50, frame_sample_rate=2,seg_len=container.streams.video[0].frames)\n",
    "video = read_video_pyav(container=container, indices=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b9401-805d-41ab-988d-77d32253880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1240ee6-78b9-49b2-a973-59b8f20c24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17abeb-ec48-4e35-860b-d211c3573b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# reload(model_configuration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc228c-6625-40ed-8168-bf6aeb9a583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a6315-5a3d-484d-85db-2a5f092bbca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_files = \"data/UCF101_subset\"\n",
    "video_dict, class_labels = frames_convert_and_create_dataset_dictionary(path_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89cf77-2d91-4bda-9973-1834e679ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(video_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c4762-df21-4ac2-9246-8b880c8f3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dict[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62230e98-5e56-461b-9127-bdfe66c17a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dict[0]['video'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb929df-f9e5-477e-927b-2624ec666b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dict[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230a4a1-0a96-4571-9ebb-123aa4f96b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames, height, width, channels =  video_dict[0]['video'].shape\n",
    "num_frames, height, width, channels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d9aae-e43f-4718-822b-03863d3a4172",
   "metadata": {},
   "source": [
    "# Display Video sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b0c424-558a-4ac9-b7eb-bb9529e4ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"./tmp/saved.mp4\"\n",
    "# codec_id = \"mp4v\" # ID for a video codec.\n",
    "# fourcc = cv2.VideoWriter_fourcc(*codec_id)\n",
    "# out = cv2.VideoWriter(filename, fourcc=fourcc, fps=2, frameSize=(width, height))\n",
    "\n",
    "# for frame in np.split(video_dict[0]['video'], num_frames, axis=0):\n",
    "#     out.write(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e9c3e-3a18-4213-ad63-b4a6a8cc7e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# container2 = av.open(\"./tmp/saved.mp4\")\n",
    "# moviepy.editor.ipython_display(container2.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4931a0-d208-4453-a7c5-33a141124ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = sorted(class_labels)\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1d85a-bbb9-43f6-97d5-afc042e7721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = create_dataset(video_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023d611-596b-4376-86fa-970fc805734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949d2f4-968f-49ec-9216-ba25db2bcf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model_configuration.initialise_model(shuffled_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0157fe-af12-41f2-a196-5837132e541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output_dir = \"/tmp/results\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=training_output_dir,         \n",
    "    num_train_epochs=3,             \n",
    "    per_device_train_batch_size=2,   \n",
    "    per_device_eval_batch_size=2,    \n",
    "    learning_rate=5e-05,            \n",
    "    weight_decay=0.01,              \n",
    "    logging_dir=\"./logs\",           \n",
    "    logging_steps=10,                \n",
    "    seed=42,                       \n",
    "    eval_strategy=\"steps\",    \n",
    "    eval_steps=10,                   \n",
    "    warmup_steps=int(0.1 * 20),      \n",
    "    optim=\"adamw_torch\",          \n",
    "    lr_scheduler_type=\"linear\",      \n",
    "    fp16=True,  \n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49caf955-ac19-4577-a14d-de1266133806",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_key =  os.getenv(\"WANDB_API_KEY\")\n",
    "wandb.login(key=wandb_key)\n",
    "\n",
    "PROJECT = \"ViViT\"\n",
    "MODEL_NAME = \"google/vivit-b-16x2-kinetics400\"\n",
    "DATASET = \"sayakpaul/ucf101-subset\"\n",
    "\n",
    "wandb.init(project=PROJECT, # the project I am working on\n",
    "           tags=[MODEL_NAME, DATASET],\n",
    "           notes =\"Fine tuning ViViT with ucf101-subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c6196-9005-47c6-bdbc-3d5162cf2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-05, betas=(0.9, 0.999), eps=1e-08)\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                      \n",
    "    args=training_args,              \n",
    "    train_dataset=shuffled_dataset[\"train\"],      \n",
    "    eval_dataset=shuffled_dataset[\"test\"],       \n",
    "    optimizers=(optimizer, None),  \n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f3a42-bb1d-4300-a78a-79ca585d4cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=PROJECT, job_type=\"train\", # the project I am working on\n",
    "           tags=[MODEL_NAME, DATASET],\n",
    "           notes =f\"Fine tuning {MODEL_NAME} with {DATASET}.\"):\n",
    "           train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de7874-5757-4619-ba3d-ae8131f26e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"model\")\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5ff29-4c50-42c1-b6e2-367593a1c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_path = \"./model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b0371-dac0-46c6-b2ab-5d91a79331df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=PROJECT, job_type=\"models\"):\n",
    "  artifact = wandb.Artifact(\"ViViT-Fine-tuned\", type=\"model\")\n",
    "  artifact.add_dir(custom_path)\n",
    "  wandb.save(custom_path)\n",
    "  wandb.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788e5d9-9dd6-422a-8ea1-f6ef37b25f08",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d640fec-b0d0-481c-a1fe-28209f00dc2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_files_val = \"data/UCF_101_subset_val\"\n",
    "video_dict_val, class_labels_val = frames_convert_and_create_dataset_dictionary(path_files_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29146a59-a5b6-4b94-9830-1e9cfe8b3f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = create_dataset(video_dict_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e16c89-f1ef-47ef-81f0-a1026b5887e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('olonok69/ViViT/ViViT-Fine-tuned:v0', type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e73cab-255f-4957-a773-314c78eb7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ea59e-bac7-497e-88c4-1e1c7d69c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cefb08b-9b23-42ff-8e7b-ded2c1fb6182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_handling import generate_all_files\n",
    "import os\n",
    "import numpy as np\n",
    "import av\n",
    "from pathlib import Path\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085745f-44db-4fec-998a-3d4b5efb29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = val_dataset['train'].features['labels'].names\n",
    "config = VivitConfig.from_pretrained(artifact_dir)\n",
    "config.num_classes=len(labels)\n",
    "config.id2label = {str(i): c for i, c in enumerate(labels)}\n",
    "config.label2id = {c: str(i) for i, c in enumerate(labels)}\n",
    "config.num_frames=10\n",
    "config.video_size= [10, 224, 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358c15f-150a-4f66-ac20-90ad32d91ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab8041-7364-41f0-aa83-385ebbf0da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VivitImageProcessor, VivitForVideoClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0debd-3e88-4913-9ecb-12e7aa4a352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "fine_tune_model = VivitForVideoClassification.from_pretrained(artifact_dir,config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc9d7ac-3d6f-4f0e-a213-461fe8915f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory =  \"data/UCF_101_subset_val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5967620-32da-4263-8a18-695d9559506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = []\n",
    "true_labels=[]\n",
    "predictions = []\n",
    "predictions_labels = []\n",
    "all_videos=[]\n",
    "video_files= []\n",
    "sizes = []\n",
    "for p in generate_all_files(Path(directory), only_files=True):\n",
    "    set_files = str(p).split(\"/\")[2] # train or test\n",
    "    cls = str(p).split(\"/\")[3] # class\n",
    "    file= str(p).split(\"/\")[4] # file name\n",
    "    #file name path\n",
    "    file_name= os.path.join(directory, set_files, cls, file)\n",
    "    true_labels.append(cls)   \n",
    "    # Process class\n",
    "    if cls not in class_labels:\n",
    "        class_labels.append(cls)\n",
    "    # process video File\n",
    "    container = av.open(file_name)\n",
    "    #print(f\"Processing file {file_name} number of Frames: {container.streams.video[0].frames}\")  \n",
    "    indices = sample_frame_indices(clip_len=10, frame_sample_rate=1,seg_len=container.streams.video[0].frames)\n",
    "    video = read_video_pyav(container=container, indices=indices)\n",
    "    inputs = image_processor(list(video), return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tune_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # model predicts one of the 400 Kinetics-400 classes\n",
    "    predicted_label = logits.argmax(-1).item()\n",
    "    prediction = fine_tune_model.config.id2label[str(predicted_label)]\n",
    "    predictions.append(prediction)\n",
    "    predictions_labels.append(predicted_label)\n",
    "    print(f\"file {file_name} True Label {cls}, predicted label {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09aba34-e797-4d4a-ac6d-6d87c680fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdeaef8-256f-4878-95df-6d9624ee3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(true_labels, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4651eb-4877-4920-b0bf-de23831be7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./tmp/6540601-uhd_2560_1440_25fps.mp4\"\n",
    "container = av.open(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77348ebb-b9cd-4dc8-a449-0b9fd58a821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#moviepy.editor.ipython_display(container.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be39adbc-a8c0-4601-8c8c-d685960b5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = sample_frame_indices(clip_len=10, frame_sample_rate=3,seg_len=container.streams.video[0].frames)\n",
    "print(f\"Processing file {file_name} number of Frames: {container.streams.video[0].frames}\")  \n",
    "video = read_video_pyav(container=container, indices=indices)\n",
    "inputs = image_processor(list(video), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9809f5aa-9724-4be7-a8c3-1f9a42618c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    outputs = fine_tune_model(**inputs)\n",
    "    logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53d3939-f45f-47ce-bee9-d52c3ff9f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = logits.argmax(-1).item()\n",
    "prediction = fine_tune_model.config.id2label[str(predicted_label)]\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eae265-a55c-41de-9688-f54e81310ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
