{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4740ba12-621d-423c-8def-e0ee150f3fec",
   "metadata": {},
   "source": [
    "# Fine-tuning for Video Classification with ðŸ¤— Transformers\n",
    "### Abstract\n",
    "We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatio-temporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks. To facilitate further research, we release code at https://github.com/google-research/scenic/tree/main/scenic/projects/vivit\n",
    "\n",
    "https://arxiv.org/pdf/2103.15691\n",
    "\n",
    "![image.png](vivit.png)\n",
    "\n",
    "\n",
    "## Embeddings\n",
    "### Uniform frame sampling \n",
    "straightforward method of tokenising the input video is to uniformly sample nt frames from the input video clip, embed each 2D frame independently using the same method as ViT, and concatenate all these tokens together. Concretely, if nh Â· nw non-overlapping image patches are extracted from each frame, then a total of nt Â·nhÂ·nw tokens will be forwarded through the transformer encoder.Intuitively, this process may be seen as simply constructing a large 2D image to be tokenised following ViT\n",
    "\n",
    "#### Tubelet embedding\n",
    "An alternate method, to extract non-overlapping, spatio-temporal â€œtubesâ€ from the input volume, and to linearly project this to Rd. This method is an extension of ViTâ€™s embedding to 3D,and corresponds to a 3D convolution. \n",
    "\n",
    "### HF Vivit\n",
    "https://huggingface.co/docs/transformers/main/model_doc/vivit\n",
    "\n",
    "# Dataset\n",
    "https://paperswithcode.com/dataset/kinetics-400-1\n",
    "\n",
    "# Download Dataset sayakpaul/ucf101-subset\n",
    "#### Complete UCF101\n",
    "UCF101 is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. This data set is an extension of UCF50 data set which has 50 action categories.\n",
    "\n",
    "With 13320 videos from 101 action categories, UCF101 gives the largest diversity in terms of actions and with the presence of large variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background, illumination conditions, etc, it is the most challenging data set to date. As most of the available action recognition data sets are not realistic and are staged by actors, UCF101 aims to encourage further research into action recognition by learning and exploring new realistic action categories.\n",
    "\n",
    "https://www.crcv.ucf.edu/research/data-sets/ucf101/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b64e8f-95bb-4fc4-a9ee-f8254e5e9b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UCF101_subset.tar.gz'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "hf_dataset_identifier = \"sayakpaul/ucf101-subset\"\n",
    "filename = \"UCF101_subset.tar.gz\"\n",
    "file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\", local_dir=\".\")\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d944a0f-bd4d-4433-8b2d-3236efb2e369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/d/repos2/video'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a2683a6-75ef-4dab-b9a3-2af7c44f8b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "with tarfile.open(\"UCF101_subset.tar.gz\") as t:\n",
    "     t.extractall(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6feac456-e72d-4f65-b11d-723561752f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer, TrainingArguments, AdamW\n",
    "from model_configuration import *\n",
    "from transformers import Trainer\n",
    "from preprocessing import create_dataset\n",
    "from data_handling import frames_convert_and_create_dataset_dictionary\n",
    "from model_configuration import initialise_model\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54835acc-68f5-4431-8cc7-a20dc01b6c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "env_path =  \".env\"\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d89a96-62be-4451-91bf-16958edc5232",
   "metadata": {},
   "source": [
    "# Base Model\n",
    "\n",
    "https://github.com/google-research/scenic/tree/main/scenic/projects/vivit\n",
    "\n",
    "### google/vivit-f-16x2-kinetics400\n",
    "\n",
    "![image.png](models.png)\n",
    "\n",
    "\n",
    "##### https://huggingface.co/docs/transformers/main/model_doc/vivit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb89491e-49f7-4bdf-ac77-a44ac2ebae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_configuration\n",
    "from model_configuration import compute_metrics\n",
    "import cv2\n",
    "import av\n",
    "from data_handling import sample_frame_indices, read_video_pyav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "556ec3e5-ce10-43f4-99bc-e91758e40790",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = av.open(\"./data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c01.avi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95055c36-4a00-47a9-a708-f09142a35469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container.streams.video[0].frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f04a63fa-f64f-42b0-955c-61185627ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipywidgets import Video, Image\n",
    "# from IPython.display import display\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a55cc64e-b695-4aa5-b848-59d7078b85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(\"./data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c01.avi\")\n",
    "\n",
    "# frames = []\n",
    "\n",
    "# while(1):\n",
    "#     try:\n",
    "#         _, frame = cap.read()\n",
    "\n",
    "#         fgmask = cv2.Canny(frame, 100, 100)\n",
    "\n",
    "#         mask = fgmask > 100\n",
    "#         frame[mask, :] = 0\n",
    "\n",
    "#         frames.append(frame)\n",
    "#     except Exception:\n",
    "#         break\n",
    "\n",
    "# width = int(cap.get(3))\n",
    "# height = int(cap.get(4))\n",
    "\n",
    "# filename = 'tmp/output.mp4'\n",
    "\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "# writer = cv2.VideoWriter(filename, fourcc, 25, (width, height))\n",
    "\n",
    "# for frame in frames:\n",
    "#     writer.write(frame)\n",
    "\n",
    "# cap.release()\n",
    "# writer.release()\n",
    "\n",
    "# with open(filename, 'rb') as f:\n",
    "#     video.value = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d52eea4a-b495-4628-905f-d64e14d48bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = av.open(\"./data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c01.avi\")\n",
    "indices = sample_frame_indices(clip_len=50, frame_sample_rate=1,seg_len=container.streams.video[0].frames)\n",
    "video = read_video_pyav(container=container, indices=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "898b9401-805d-41ab-988d-77d32253880a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1240ee6-78b9-49b2-a973-59b8f20c24c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 224, 224, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb17abeb-ec48-4e35-860b-d211c3573b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# reload(model_configuration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22fc228c-6625-40ed-8168-bf6aeb9a583b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a86a6315-5a3d-484d-85db-2a5f092bbca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c01.avi number of Frames: 209\n",
      "Processing file data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c03.avi number of Frames: 115\n",
      "Processing file data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c05.avi number of Frames: 146\n",
      "Processing file data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g23_c02.avi number of Frames: 131\n",
      "Processing file data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g23_c04.avi number of Frames: 200\n",
      "Processing file data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g23_c06.avi number of Frames: 140\n",
      "Processing file data/UCF101_subset/test/ApplyLipstick/v_ApplyLipstick_g14_c01.avi number of Frames: 176\n",
      "Processing file data/UCF101_subset/test/ApplyLipstick/v_ApplyLipstick_g14_c03.avi number of Frames: 174\n",
      "Processing file data/UCF101_subset/test/ApplyLipstick/v_ApplyLipstick_g16_c02.avi number of Frames: 165\n",
      "Processing file data/UCF101_subset/test/ApplyLipstick/v_ApplyLipstick_g16_c04.avi number of Frames: 173\n",
      "Processing file data/UCF101_subset/test/Archery/v_Archery_g16_c01.avi number of Frames: 151\n",
      "Processing file data/UCF101_subset/test/Archery/v_Archery_g16_c03.avi number of Frames: 206\n",
      "Processing file data/UCF101_subset/test/Archery/v_Archery_g16_c05.avi number of Frames: 262\n",
      "Processing file data/UCF101_subset/test/Archery/v_Archery_g17_c02.avi number of Frames: 94\n",
      "Processing file data/UCF101_subset/test/Archery/v_Archery_g17_c04.avi number of Frames: 66\n",
      "Processing file data/UCF101_subset/test/Archery/v_Archery_g21_c02.avi number of Frames: 175\n",
      "Processing file data/UCF101_subset/test/Archery/v_Archery_g21_c04.avi number of Frames: 205\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g03_c01.avi number of Frames: 215\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g03_c03.avi number of Frames: 198\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g04_c01.avi number of Frames: 127\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g04_c03.avi number of Frames: 159\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g19_c02.avi number of Frames: 170\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g19_c04.avi number of Frames: 144\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g22_c02.avi number of Frames: 197\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g22_c04.avi number of Frames: 193\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g22_c06.avi number of Frames: 92\n",
      "Processing file data/UCF101_subset/test/BalanceBeam/v_BalanceBeam_g11_c02.avi number of Frames: 68\n",
      "Processing file data/UCF101_subset/test/BalanceBeam/v_BalanceBeam_g11_c04.avi number of Frames: 116\n",
      "Processing file data/UCF101_subset/test/BalanceBeam/v_BalanceBeam_g20_c01.avi number of Frames: 84\n",
      "Processing file data/UCF101_subset/test/BalanceBeam/v_BalanceBeam_g20_c03.avi number of Frames: 100\n",
      "Processing file data/UCF101_subset/test/BandMarching/v_BandMarching_g05_c02.avi number of Frames: 84\n",
      "Processing file data/UCF101_subset/test/BandMarching/v_BandMarching_g05_c04.avi number of Frames: 89\n",
      "Processing file data/UCF101_subset/test/BandMarching/v_BandMarching_g05_c06.avi number of Frames: 79\n",
      "Processing file data/UCF101_subset/test/BandMarching/v_BandMarching_g11_c01.avi number of Frames: 296\n",
      "Processing file data/UCF101_subset/test/BandMarching/v_BandMarching_g11_c03.avi number of Frames: 389\n",
      "Processing file data/UCF101_subset/test/BandMarching/v_BandMarching_g11_c05.avi number of Frames: 432\n",
      "Processing file data/UCF101_subset/test/BandMarching/v_BandMarching_g11_c07.avi number of Frames: 247\n",
      "Processing file data/UCF101_subset/test/BandMarching/v_BandMarching_g22_c02.avi number of Frames: 131\n",
      "Processing file data/UCF101_subset/test/BandMarching/v_BandMarching_g22_c04.avi number of Frames: 248\n",
      "Processing file data/UCF101_subset/test/BaseballPitch/v_BaseballPitch_g11_c02.avi number of Frames: 74\n",
      "Processing file data/UCF101_subset/test/BaseballPitch/v_BaseballPitch_g11_c04.avi number of Frames: 58\n",
      "Processing file data/UCF101_subset/test/BaseballPitch/v_BaseballPitch_g20_c02.avi number of Frames: 110\n",
      "Processing file data/UCF101_subset/test/BaseballPitch/v_BaseballPitch_g20_c04.avi number of Frames: 102\n",
      "Processing file data/UCF101_subset/test/BaseballPitch/v_BaseballPitch_g22_c02.avi number of Frames: 104\n",
      "Processing file data/UCF101_subset/test/BaseballPitch/v_BaseballPitch_g22_c04.avi number of Frames: 94\n",
      "Processing file data/UCF101_subset/test/BaseballPitch/v_BaseballPitch_g22_c06.avi number of Frames: 71\n",
      "Processing file data/UCF101_subset/test/BaseballPitch/v_BaseballPitch_g24_c02.avi number of Frames: 119\n",
      "Processing file data/UCF101_subset/test/BaseballPitch/v_BaseballPitch_g24_c04.avi number of Frames: 116\n",
      "Processing file data/UCF101_subset/test/BaseballPitch/v_BaseballPitch_g24_c06.avi number of Frames: 120\n",
      "Processing file data/UCF101_subset/test/Basketball/v_Basketball_g02_c02.avi number of Frames: 56\n",
      "Processing file data/UCF101_subset/test/Basketball/v_Basketball_g02_c04.avi number of Frames: 80\n",
      "Processing file data/UCF101_subset/test/Basketball/v_Basketball_g02_c06.avi number of Frames: 124\n",
      "Processing file data/UCF101_subset/test/Basketball/v_Basketball_g05_c02.avi number of Frames: 111\n",
      "Processing file data/UCF101_subset/test/Basketball/v_Basketball_g05_c04.avi number of Frames: 87\n",
      "Processing file data/UCF101_subset/test/Basketball/v_Basketball_g06_c02.avi number of Frames: 101\n",
      "Processing file data/UCF101_subset/test/Basketball/v_Basketball_g06_c04.avi number of Frames: 77\n",
      "Processing file data/UCF101_subset/test/Basketball/v_Basketball_g18_c02.avi number of Frames: 102\n",
      "Processing file data/UCF101_subset/test/Basketball/v_Basketball_g18_c04.avi number of Frames: 188\n",
      "Processing file data/UCF101_subset/test/Basketball/v_Basketball_g24_c02.avi number of Frames: 120\n",
      "Processing file data/UCF101_subset/test/Basketball/v_Basketball_g24_c04.avi number of Frames: 120\n",
      "Processing file data/UCF101_subset/test/BasketballDunk/v_BasketballDunk_g12_c01.avi number of Frames: 50\n",
      "Processing file data/UCF101_subset/test/BasketballDunk/v_BasketballDunk_g12_c03.avi number of Frames: 70\n",
      "Processing file data/UCF101_subset/test/BasketballDunk/v_BasketballDunk_g12_c05.avi number of Frames: 68\n",
      "Processing file data/UCF101_subset/test/BasketballDunk/v_BasketballDunk_g14_c02.avi number of Frames: 82\n",
      "Processing file data/UCF101_subset/test/BasketballDunk/v_BasketballDunk_g14_c04.avi number of Frames: 88\n",
      "Processing file data/UCF101_subset/test/BasketballDunk/v_BasketballDunk_g14_c06.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/test/BenchPress/v_BenchPress_g05_c02.avi number of Frames: 67\n",
      "Processing file data/UCF101_subset/test/BenchPress/v_BenchPress_g05_c04.avi number of Frames: 298\n",
      "Processing file data/UCF101_subset/test/BenchPress/v_BenchPress_g05_c06.avi number of Frames: 195\n",
      "Processing file data/UCF101_subset/test/BenchPress/v_BenchPress_g09_c02.avi number of Frames: 70\n",
      "Processing file data/UCF101_subset/test/BenchPress/v_BenchPress_g09_c04.avi number of Frames: 68\n",
      "Processing file data/UCF101_subset/test/BenchPress/v_BenchPress_g09_c06.avi number of Frames: 211\n",
      "Processing file data/UCF101_subset/test/BenchPress/v_BenchPress_g25_c02.avi number of Frames: 108\n",
      "Processing file data/UCF101_subset/test/BenchPress/v_BenchPress_g25_c04.avi number of Frames: 216\n",
      "Processing file data/UCF101_subset/test/BenchPress/v_BenchPress_g25_c06.avi number of Frames: 123\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g02_c03.avi number of Frames: 216\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c07.avi number of Frames: 158\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c02.avi number of Frames: 264\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c06.avi number of Frames: 115\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g06_c03.avi number of Frames: 245\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi number of Frames: 142\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi number of Frames: 162\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi number of Frames: 120\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi number of Frames: 258\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi number of Frames: 188\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g10_c01.avi number of Frames: 153\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g10_c05.avi number of Frames: 120\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g11_c02.avi number of Frames: 178\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g11_c04.avi number of Frames: 138\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g12_c05.avi number of Frames: 127\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g13_c05.avi number of Frames: 119\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c06.avi number of Frames: 197\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g16_c03.avi number of Frames: 114\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g17_c04.avi number of Frames: 105\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g18_c01.avi number of Frames: 159\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g18_c03.avi number of Frames: 157\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g19_c02.avi number of Frames: 243\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g21_c04.avi number of Frames: 227\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g22_c01.avi number of Frames: 170\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g22_c05.avi number of Frames: 243\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g24_c02.avi number of Frames: 105\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c01.avi number of Frames: 241\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c03.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c05.avi number of Frames: 93\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c07.avi number of Frames: 145\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g01_c02.avi number of Frames: 160\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g01_c04.avi number of Frames: 369\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g02_c01.avi number of Frames: 127\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g02_c03.avi number of Frames: 98\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g03_c01.avi number of Frames: 180\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g03_c03.avi number of Frames: 154\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g04_c01.avi number of Frames: 98\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g04_c05.avi number of Frames: 169\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g05_c02.avi number of Frames: 154\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g06_c03.avi number of Frames: 153\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g07_c02.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g08_c02.avi number of Frames: 219\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g09_c04.avi number of Frames: 210\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g11_c04.avi number of Frames: 157\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g12_c04.avi number of Frames: 277\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g13_c03.avi number of Frames: 164\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g15_c03.avi number of Frames: 133\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g17_c01.avi number of Frames: 161\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g17_c03.avi number of Frames: 142\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g18_c02.avi number of Frames: 139\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g18_c04.avi number of Frames: 119\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g19_c02.avi number of Frames: 111\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g19_c04.avi number of Frames: 155\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g21_c01.avi number of Frames: 85\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g21_c03.avi number of Frames: 143\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g22_c02.avi number of Frames: 176\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g22_c04.avi number of Frames: 177\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g22_c06.avi number of Frames: 172\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g23_c03.avi number of Frames: 135\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g24_c05.avi number of Frames: 288\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g01_c04.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g02_c01.avi number of Frames: 158\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g02_c05.avi number of Frames: 127\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g03_c02.avi number of Frames: 176\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g04_c01.avi number of Frames: 165\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g04_c03.avi number of Frames: 140\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g05_c04.avi number of Frames: 51\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g06_c01.avi number of Frames: 81\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g06_c03.avi number of Frames: 87\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g06_c05.avi number of Frames: 130\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g07_c01.avi number of Frames: 91\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g07_c05.avi number of Frames: 64\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g08_c01.avi number of Frames: 363\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g09_c02.avi number of Frames: 83\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g10_c03.avi number of Frames: 203\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g10_c05.avi number of Frames: 188\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g11_c04.avi number of Frames: 258\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g11_c06.avi number of Frames: 243\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g13_c03.avi number of Frames: 160\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g13_c07.avi number of Frames: 190\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g14_c04.avi number of Frames: 244\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g15_c02.avi number of Frames: 184\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g19_c03.avi number of Frames: 159\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g20_c05.avi number of Frames: 120\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g22_c02.avi number of Frames: 70\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g22_c04.avi number of Frames: 73\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g23_c05.avi number of Frames: 91\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g24_c04.avi number of Frames: 121\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g24_c06.avi number of Frames: 122\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g25_c04.avi number of Frames: 259\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g01_c03.avi number of Frames: 156\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g02_c01.avi number of Frames: 109\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g02_c03.avi number of Frames: 117\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g02_c05.avi number of Frames: 84\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g05_c01.avi number of Frames: 221\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g06_c04.avi number of Frames: 113\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g07_c02.avi number of Frames: 145\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g07_c06.avi number of Frames: 210\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g08_c02.avi number of Frames: 111\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g08_c04.avi number of Frames: 95\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g10_c04.avi number of Frames: 184\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g11_c01.avi number of Frames: 106\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g11_c03.avi number of Frames: 99\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g12_c03.avi number of Frames: 96\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g14_c01.avi number of Frames: 122\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g14_c03.avi number of Frames: 176\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g15_c01.avi number of Frames: 198\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g15_c03.avi number of Frames: 101\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g15_c05.avi number of Frames: 217\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g16_c05.avi number of Frames: 223\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g17_c01.avi number of Frames: 141\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g17_c05.avi number of Frames: 109\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g18_c02.avi number of Frames: 227\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g20_c03.avi number of Frames: 263\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g20_c05.avi number of Frames: 178\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g20_c07.avi number of Frames: 250\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g23_c01.avi number of Frames: 116\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g24_c01.avi number of Frames: 194\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g24_c05.avi number of Frames: 112\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g25_c01.avi number of Frames: 293\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g01_c03.avi number of Frames: 112\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g02_c03.avi number of Frames: 60\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g03_c01.avi number of Frames: 197\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g03_c03.avi number of Frames: 101\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g04_c01.avi number of Frames: 120\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g04_c03.avi number of Frames: 160\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g05_c01.avi number of Frames: 95\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g05_c03.avi number of Frames: 111\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g06_c01.avi number of Frames: 106\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g06_c03.avi number of Frames: 105\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g06_c05.avi number of Frames: 102\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g07_c02.avi number of Frames: 110\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g08_c02.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g08_c04.avi number of Frames: 116\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g09_c02.avi number of Frames: 79\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g10_c02.avi number of Frames: 69\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g10_c04.avi number of Frames: 122\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g12_c04.avi number of Frames: 96\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g14_c01.avi number of Frames: 145\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g14_c03.avi number of Frames: 201\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g15_c03.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g17_c01.avi number of Frames: 191\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g18_c03.avi number of Frames: 181\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g19_c01.avi number of Frames: 153\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g19_c03.avi number of Frames: 118\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g21_c03.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g21_c05.avi number of Frames: 115\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g23_c04.avi number of Frames: 107\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g24_c03.avi number of Frames: 100\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g25_c01.avi number of Frames: 80\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g01_c01.avi number of Frames: 160\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g02_c02.avi number of Frames: 156\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g02_c06.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g03_c01.avi number of Frames: 405\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g03_c03.avi number of Frames: 284\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g03_c05.avi number of Frames: 270\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g04_c02.avi number of Frames: 210\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g04_c04.avi number of Frames: 167\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g06_c01.avi number of Frames: 68\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g07_c03.avi number of Frames: 105\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g07_c05.avi number of Frames: 117\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g07_c07.avi number of Frames: 146\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g08_c02.avi number of Frames: 145\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g09_c03.avi number of Frames: 102\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g09_c07.avi number of Frames: 132\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g10_c02.avi number of Frames: 139\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g10_c06.avi number of Frames: 138\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g12_c02.avi number of Frames: 429\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g13_c04.avi number of Frames: 303\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g14_c05.avi number of Frames: 216\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g15_c05.avi number of Frames: 471\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g16_c06.avi number of Frames: 140\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g18_c03.avi number of Frames: 310\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g19_c03.avi number of Frames: 307\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g19_c05.avi number of Frames: 331\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g21_c02.avi number of Frames: 323\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g21_c06.avi number of Frames: 336\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g24_c01.avi number of Frames: 340\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g25_c05.avi number of Frames: 105\n",
      "Processing file data/UCF101_subset/train/BandMarching/v_BandMarching_g25_c07.avi number of Frames: 214\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g01_c02.avi number of Frames: 94\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g01_c04.avi number of Frames: 105\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g02_c02.avi number of Frames: 72\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g02_c04.avi number of Frames: 85\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g03_c02.avi number of Frames: 120\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g03_c04.avi number of Frames: 80\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g04_c01.avi number of Frames: 213\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g05_c06.avi number of Frames: 116\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g06_c05.avi number of Frames: 63\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g06_c07.avi number of Frames: 81\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g08_c01.avi number of Frames: 113\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g08_c05.avi number of Frames: 110\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g08_c07.avi number of Frames: 105\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g10_c03.avi number of Frames: 118\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g13_c03.avi number of Frames: 140\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g13_c05.avi number of Frames: 75\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g13_c07.avi number of Frames: 108\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g14_c02.avi number of Frames: 138\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g14_c04.avi number of Frames: 109\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g15_c06.avi number of Frames: 79\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g16_c03.avi number of Frames: 126\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g17_c05.avi number of Frames: 73\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g18_c04.avi number of Frames: 79\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g18_c06.avi number of Frames: 67\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g19_c03.avi number of Frames: 155\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g19_c05.avi number of Frames: 94\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g21_c02.avi number of Frames: 88\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g23_c01.avi number of Frames: 113\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g23_c03.avi number of Frames: 78\n",
      "Processing file data/UCF101_subset/train/BaseballPitch/v_BaseballPitch_g25_c01.avi number of Frames: 133\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g01_c01.avi number of Frames: 141\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g01_c03.avi number of Frames: 206\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g01_c05.avi number of Frames: 141\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g01_c07.avi number of Frames: 100\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g03_c04.avi number of Frames: 80\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g03_c06.avi number of Frames: 86\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g04_c02.avi number of Frames: 104\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g04_c04.avi number of Frames: 72\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g07_c02.avi number of Frames: 101\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g08_c04.avi number of Frames: 101\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g09_c02.avi number of Frames: 104\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g09_c04.avi number of Frames: 101\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g11_c02.avi number of Frames: 143\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g12_c01.avi number of Frames: 278\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g13_c02.avi number of Frames: 330\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g13_c04.avi number of Frames: 300\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g14_c02.avi number of Frames: 131\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g14_c04.avi number of Frames: 88\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g15_c01.avi number of Frames: 120\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g15_c07.avi number of Frames: 164\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g16_c02.avi number of Frames: 518\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g16_c06.avi number of Frames: 346\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g17_c04.avi number of Frames: 84\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g19_c05.avi number of Frames: 151\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g19_c07.avi number of Frames: 137\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g21_c03.avi number of Frames: 182\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g22_c05.avi number of Frames: 88\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g23_c04.avi number of Frames: 120\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g23_c06.avi number of Frames: 97\n",
      "Processing file data/UCF101_subset/train/Basketball/v_Basketball_g25_c02.avi number of Frames: 179\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g01_c02.avi number of Frames: 65\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g01_c04.avi number of Frames: 61\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g02_c03.avi number of Frames: 71\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g03_c03.avi number of Frames: 74\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g04_c01.avi number of Frames: 51\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g06_c01.avi number of Frames: 52\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g07_c03.avi number of Frames: 53\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g07_c05.avi number of Frames: 37\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g08_c05.avi number of Frames: 87\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g09_c02.avi number of Frames: 72\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g10_c01.avi number of Frames: 91\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g11_c04.avi number of Frames: 86\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g13_c02.avi number of Frames: 67\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g15_c01.avi number of Frames: 77\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g15_c03.avi number of Frames: 124\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g15_c05.avi number of Frames: 79\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g16_c02.avi number of Frames: 75\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g16_c04.avi number of Frames: 59\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g16_c06.avi number of Frames: 76\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g17_c02.avi number of Frames: 66\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g18_c05.avi number of Frames: 74\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g19_c03.avi number of Frames: 58\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g20_c05.avi number of Frames: 72\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g21_c01.avi number of Frames: 84\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g21_c03.avi number of Frames: 116\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g22_c03.avi number of Frames: 111\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g23_c01.avi number of Frames: 91\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g23_c05.avi number of Frames: 93\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g24_c04.avi number of Frames: 155\n",
      "Processing file data/UCF101_subset/train/BasketballDunk/v_BasketballDunk_g25_c01.avi number of Frames: 82\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g01_c05.avi number of Frames: 88\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g02_c01.avi number of Frames: 81\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g02_c03.avi number of Frames: 221\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g03_c02.avi number of Frames: 100\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g03_c04.avi number of Frames: 105\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g03_c06.avi number of Frames: 83\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g04_c01.avi number of Frames: 70\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g04_c05.avi number of Frames: 148\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g06_c01.avi number of Frames: 68\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g06_c05.avi number of Frames: 128\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g07_c02.avi number of Frames: 71\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g07_c04.avi number of Frames: 272\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g08_c05.avi number of Frames: 187\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g10_c03.avi number of Frames: 243\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g12_c06.avi number of Frames: 73\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g13_c01.avi number of Frames: 140\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g13_c05.avi number of Frames: 141\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g13_c07.avi number of Frames: 103\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g14_c02.avi number of Frames: 99\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g15_c05.avi number of Frames: 65\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g16_c05.avi number of Frames: 124\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g18_c03.avi number of Frames: 274\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g19_c06.avi number of Frames: 103\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g20_c04.avi number of Frames: 210\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g21_c01.avi number of Frames: 188\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g21_c03.avi number of Frames: 93\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g22_c01.avi number of Frames: 88\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g22_c03.avi number of Frames: 74\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g23_c02.avi number of Frames: 126\n",
      "Processing file data/UCF101_subset/train/BenchPress/v_BenchPress_g24_c05.avi number of Frames: 111\n",
      "Processing file data/UCF101_subset/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi number of Frames: 164\n",
      "Processing file data/UCF101_subset/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g14_c05.avi number of Frames: 160\n",
      "Processing file data/UCF101_subset/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g20_c04.avi number of Frames: 220\n",
      "Processing file data/UCF101_subset/val/ApplyLipstick/v_ApplyLipstick_g10_c04.avi number of Frames: 247\n",
      "Processing file data/UCF101_subset/val/ApplyLipstick/v_ApplyLipstick_g20_c04.avi number of Frames: 140\n",
      "Processing file data/UCF101_subset/val/ApplyLipstick/v_ApplyLipstick_g25_c02.avi number of Frames: 151\n",
      "Processing file data/UCF101_subset/val/Archery/v_Archery_g12_c03.avi number of Frames: 436\n",
      "Processing file data/UCF101_subset/val/Archery/v_Archery_g18_c02.avi number of Frames: 291\n",
      "Processing file data/UCF101_subset/val/Archery/v_Archery_g18_c06.avi number of Frames: 160\n",
      "Processing file data/UCF101_subset/val/BabyCrawling/v_BabyCrawling_g09_c06.avi number of Frames: 119\n",
      "Processing file data/UCF101_subset/val/BabyCrawling/v_BabyCrawling_g13_c05.avi number of Frames: 85\n",
      "Processing file data/UCF101_subset/val/BabyCrawling/v_BabyCrawling_g21_c04.avi number of Frames: 189\n",
      "Processing file data/UCF101_subset/val/BalanceBeam/v_BalanceBeam_g13_c02.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/val/BalanceBeam/v_BalanceBeam_g16_c01.avi number of Frames: 124\n",
      "Processing file data/UCF101_subset/val/BalanceBeam/v_BalanceBeam_g22_c02.avi number of Frames: 96\n",
      "Processing file data/UCF101_subset/val/BandMarching/v_BandMarching_g17_c01.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/val/BandMarching/v_BandMarching_g20_c04.avi number of Frames: 165\n",
      "Processing file data/UCF101_subset/val/BandMarching/v_BandMarching_g23_c03.avi number of Frames: 262\n",
      "Processing file data/UCF101_subset/val/BaseballPitch/v_BaseballPitch_g07_c02.avi number of Frames: 138\n",
      "Processing file data/UCF101_subset/val/BaseballPitch/v_BaseballPitch_g09_c02.avi number of Frames: 75\n",
      "Processing file data/UCF101_subset/val/BaseballPitch/v_BaseballPitch_g12_c01.avi number of Frames: 126\n",
      "Processing file data/UCF101_subset/val/Basketball/v_Basketball_g10_c03.avi number of Frames: 116\n",
      "Processing file data/UCF101_subset/val/Basketball/v_Basketball_g20_c02.avi number of Frames: 175\n",
      "Processing file data/UCF101_subset/val/Basketball/v_Basketball_g20_c04.avi number of Frames: 120\n",
      "Processing file data/UCF101_subset/val/BasketballDunk/v_BasketballDunk_g05_c01.avi number of Frames: 52\n",
      "Processing file data/UCF101_subset/val/BasketballDunk/v_BasketballDunk_g05_c03.avi number of Frames: 56\n",
      "Processing file data/UCF101_subset/val/BasketballDunk/v_BasketballDunk_g05_c05.avi number of Frames: 53\n",
      "Processing file data/UCF101_subset/val/BenchPress/v_BenchPress_g11_c05.avi number of Frames: 114\n",
      "Processing file data/UCF101_subset/val/BenchPress/v_BenchPress_g17_c02.avi number of Frames: 68\n",
      "Processing file data/UCF101_subset/val/BenchPress/v_BenchPress_g17_c06.avi number of Frames: 117\n",
      "Min number frames 37\n"
     ]
    }
   ],
   "source": [
    "path_files = \"data/UCF101_subset\"\n",
    "video_dict, class_labels = frames_convert_and_create_dataset_dictionary(path_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1b89cf77-2d91-4bda-9973-1834e679ad6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "582c4762-df21-4ac2-9246-8b880c8f3d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['video', 'labels'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_dict[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "62230e98-5e56-461b-9127-bdfe66c17a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 224, 224, 3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_dict[0]['video'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffb929df-f9e5-477e-927b-2624ec666b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ApplyEyeMakeup'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_dict[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e230a4a1-0a96-4571-9ebb-123aa4f96b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 224, 224, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_frames, height, width, channels =  video_dict[0]['video'].shape\n",
    "num_frames, height, width, channels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1b0c424-558a-4ac9-b7eb-bb9529e4ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"./tmp/saved.avi\"\n",
    "# codec_id = \"mp4v\" # ID for a video codec.\n",
    "# fourcc = cv2.VideoWriter_fourcc(*codec_id)\n",
    "# out = cv2.VideoWriter(filename, fourcc=fourcc, fps=20, frameSize=(width, height))\n",
    "\n",
    "# for frame in np.split(video_dict[0]['video'], num_frames, axis=0):\n",
    "#     out.write(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "597e9c3e-3a18-4213-ad63-b4a6a8cc7e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Video\n",
    "\n",
    "# Video(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d4931a0-d208-4453-a7c5-33a141124ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress'].\n"
     ]
    }
   ],
   "source": [
    "class_labels = sorted(class_labels)\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3c1d85a-bbb9-43f6-97d5-afc042e7721c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018c368fe1624a939aabc5330432b099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/405 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a0d2e6997e41568926f30a9ad0e1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/405 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olonok/.local/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:142: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.tensor(value)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d5f1fdf3174650965ce24929db1862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/405 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shuffled_dataset = create_dataset(video_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b023d611-596b-4376-86fa-970fc805734b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ClassLabel(names=['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress'], id=None),\n",
       " 'pixel_values': Sequence(feature=Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0949d2f4-968f-49ec-9216-ba25db2bcf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitForVideoClassification were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized because the shapes did not match:\n",
      "- vivit.embeddings.position_embeddings: found shape torch.Size([1, 3137, 768]) in the checkpoint and torch.Size([1, 981, 768]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = model_configuration.initialise_model(shuffled_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a0157fe-af12-41f2-a196-5837132e541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output_dir = \"/tmp/results\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=training_output_dir,         \n",
    "    num_train_epochs=3,             \n",
    "    per_device_train_batch_size=2,   \n",
    "    per_device_eval_batch_size=2,    \n",
    "    learning_rate=5e-05,            \n",
    "    weight_decay=0.01,              \n",
    "    logging_dir=\"./logs\",           \n",
    "    logging_steps=10,                \n",
    "    seed=42,                       \n",
    "    eval_strategy=\"steps\",    \n",
    "    eval_steps=10,                   \n",
    "    warmup_steps=int(0.1 * 20),      \n",
    "    optim=\"adamw_torch\",          \n",
    "    lr_scheduler_type=\"linear\",      \n",
    "    fp16=True,  \n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49caf955-ac19-4577-a14d-de1266133806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molonok\u001b[0m (\u001b[33molonok69\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/olonok/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/repos2/video/wandb/run-20240914_233722-uixrkmeu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olonok69/ViViT/runs/uixrkmeu' target=\"_blank\">youthful-terrain-8</a></strong> to <a href='https://wandb.ai/olonok69/ViViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olonok69/ViViT' target=\"_blank\">https://wandb.ai/olonok69/ViViT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olonok69/ViViT/runs/uixrkmeu' target=\"_blank\">https://wandb.ai/olonok69/ViViT/runs/uixrkmeu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/olonok69/ViViT/runs/uixrkmeu?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff4fb58ca50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb_key =  os.getenv(\"WANDB_API_KEY\")\n",
    "wandb.login(key=wandb_key)\n",
    "\n",
    "PROJECT = \"ViViT\"\n",
    "MODEL_NAME = \"google/vivit-b-16x2-kinetics400\"\n",
    "DATASET = \"sayakpaul/ucf101-subset\"\n",
    "\n",
    "wandb.init(project=PROJECT, # the project I am working on\n",
    "           tags=[MODEL_NAME, DATASET],\n",
    "           notes =\"Fine tuning ViViT with ucf101-subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "396c6196-9005-47c6-bdbc-3d5162cf2bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olonok/.local/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-05, betas=(0.9, 0.999), eps=1e-08)\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                      \n",
    "    args=training_args,              \n",
    "    train_dataset=shuffled_dataset[\"train\"],      \n",
    "    eval_dataset=shuffled_dataset[\"test\"],       \n",
    "    optimizers=(optimizer, None),  \n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b5f3a42-bb1d-4300-a78a-79ca585d4cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:uixrkmeu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">youthful-terrain-8</strong> at: <a href='https://wandb.ai/olonok69/ViViT/runs/uixrkmeu' target=\"_blank\">https://wandb.ai/olonok69/ViViT/runs/uixrkmeu</a><br/> View project at: <a href='https://wandb.ai/olonok69/ViViT' target=\"_blank\">https://wandb.ai/olonok69/ViViT</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240914_233722-uixrkmeu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:uixrkmeu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c829cc8ccf54217ad85cc7041f8c3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113483377913427, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/repos2/video/wandb/run-20240914_233737-hcja52kb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olonok69/ViViT/runs/hcja52kb' target=\"_blank\">decent-bee-9</a></strong> to <a href='https://wandb.ai/olonok69/ViViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olonok69/ViViT' target=\"_blank\">https://wandb.ai/olonok69/ViViT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olonok69/ViViT/runs/hcja52kb' target=\"_blank\">https://wandb.ai/olonok69/ViViT/runs/hcja52kb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 3:45:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.482200</td>\n",
       "      <td>2.530440</td>\n",
       "      <td>0.146341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.176900</td>\n",
       "      <td>2.166158</td>\n",
       "      <td>0.219512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.247900</td>\n",
       "      <td>1.969536</td>\n",
       "      <td>0.268293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.793900</td>\n",
       "      <td>2.041849</td>\n",
       "      <td>0.365854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.721900</td>\n",
       "      <td>2.050001</td>\n",
       "      <td>0.243902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.768900</td>\n",
       "      <td>1.586923</td>\n",
       "      <td>0.414634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.619400</td>\n",
       "      <td>1.292942</td>\n",
       "      <td>0.585366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.153600</td>\n",
       "      <td>1.353015</td>\n",
       "      <td>0.585366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.201400</td>\n",
       "      <td>1.127209</td>\n",
       "      <td>0.609756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.063500</td>\n",
       "      <td>0.945320</td>\n",
       "      <td>0.780488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.980400</td>\n",
       "      <td>0.935971</td>\n",
       "      <td>0.634146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.789500</td>\n",
       "      <td>0.888741</td>\n",
       "      <td>0.682927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.024900</td>\n",
       "      <td>0.789689</td>\n",
       "      <td>0.731707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.858100</td>\n",
       "      <td>0.813368</td>\n",
       "      <td>0.731707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.988400</td>\n",
       "      <td>0.623601</td>\n",
       "      <td>0.804878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.681300</td>\n",
       "      <td>0.603301</td>\n",
       "      <td>0.780488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.612741</td>\n",
       "      <td>0.780488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.547900</td>\n",
       "      <td>0.596175</td>\n",
       "      <td>0.731707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.520104</td>\n",
       "      <td>0.804878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.194700</td>\n",
       "      <td>0.413442</td>\n",
       "      <td>0.878049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.701119</td>\n",
       "      <td>0.707317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.274200</td>\n",
       "      <td>0.446177</td>\n",
       "      <td>0.804878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.187900</td>\n",
       "      <td>0.383754</td>\n",
       "      <td>0.853659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.240700</td>\n",
       "      <td>0.420428</td>\n",
       "      <td>0.804878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.129400</td>\n",
       "      <td>0.553500</td>\n",
       "      <td>0.731707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.274700</td>\n",
       "      <td>0.474501</td>\n",
       "      <td>0.804878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.297527</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.140500</td>\n",
       "      <td>0.292708</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.220600</td>\n",
       "      <td>0.234842</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>0.199403</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.224600</td>\n",
       "      <td>0.222877</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.179400</td>\n",
       "      <td>0.242763</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.414200</td>\n",
       "      <td>0.140479</td>\n",
       "      <td>0.975610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.121700</td>\n",
       "      <td>0.104442</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.102480</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.097100</td>\n",
       "      <td>0.111769</td>\n",
       "      <td>0.975610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.136618</td>\n",
       "      <td>0.975610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.114199</td>\n",
       "      <td>0.975610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.104205</td>\n",
       "      <td>0.975610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.096805</td>\n",
       "      <td>0.975610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.107929</td>\n",
       "      <td>0.975610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.136934</td>\n",
       "      <td>0.951220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.131169</td>\n",
       "      <td>0.951220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.079275</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.059050</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.057312</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.055573</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.055143</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.055049</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.054033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.054124</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.054631</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.055084</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.054942</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.019 MB uploaded\\r'), FloatProgress(value=0.13545091295331546, max=1.â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>â–â–‚â–‚â–‚â–ƒâ–…â–…â–†â–…â–†â–†â–†â–†â–†â–‡â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/loss</td><td>â–ˆâ–‡â–†â–‡â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–„â–â–„â–…â–†â–†â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–†â–…â–„â–„â–ƒâ–„â–‡â–‡â–†â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–‚â–‚â–‚â–‚â–â–â–â–‚</td></tr><tr><td>eval/samples_per_second</td><td>â–…â–ˆâ–„â–„â–ƒâ–ƒâ–â–‚â–â–â–â–ƒâ–„â–„â–…â–…â–…â–‚â–‚â–‚â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡</td></tr><tr><td>eval/steps_per_second</td><td>â–…â–ˆâ–…â–ƒâ–ƒâ–‚â–â–â–â–â–â–ƒâ–„â–„â–…â–†â–…â–‚â–‚â–‚â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡</td></tr><tr><td>train/epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–…â–…â–…â–…â–„â–ƒâ–„â–„â–ˆâ–‡â–„â–…â–„â–‚â–â–„â–‚â–ƒâ–â–â–„â–â–â–„â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–‡â–‡â–†â–†â–†â–„â–„â–„â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>1.0</td></tr><tr><td>eval/loss</td><td>0.05494</td></tr><tr><td>eval/runtime</td><td>127.9668</td></tr><tr><td>eval/samples_per_second</td><td>0.32</td></tr><tr><td>eval/steps_per_second</td><td>0.164</td></tr><tr><td>total_flos</td><td>8.580287827825459e+17</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>546</td></tr><tr><td>train/grad_norm</td><td>0.06288</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.005</td></tr><tr><td>train_loss</td><td>0.51105</td></tr><tr><td>train_runtime</td><td>13540.1435</td></tr><tr><td>train_samples_per_second</td><td>0.081</td></tr><tr><td>train_steps_per_second</td><td>0.04</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decent-bee-9</strong> at: <a href='https://wandb.ai/olonok69/ViViT/runs/hcja52kb' target=\"_blank\">https://wandb.ai/olonok69/ViViT/runs/hcja52kb</a><br/> View project at: <a href='https://wandb.ai/olonok69/ViViT' target=\"_blank\">https://wandb.ai/olonok69/ViViT</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240914_233737-hcja52kb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=PROJECT, job_type=\"train\", # the project I am working on\n",
    "           tags=[MODEL_NAME, DATASET],\n",
    "           notes =f\"Fine tuning {MODEL_NAME} with {DATASET}.\"):\n",
    "           train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49de7874-5757-4619-ba3d-ae8131f26e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         3.0\n",
      "  total_flos               = 799101575GF\n",
      "  train_loss               =      0.5111\n",
      "  train_runtime            =  3:45:40.14\n",
      "  train_samples_per_second =       0.081\n",
      "  train_steps_per_second   =        0.04\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"model\")\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94d5ff29-4c50-42c1-b6e2-367593a1c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_path = \"./model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c0b0371-dac0-46c6-b2ab-5d91a79331df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/repos2/video/wandb/run-20240915_091820-l4vwgih0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olonok69/ViViT/runs/l4vwgih0' target=\"_blank\">divine-field-10</a></strong> to <a href='https://wandb.ai/olonok69/ViViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olonok69/ViViT' target=\"_blank\">https://wandb.ai/olonok69/ViViT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olonok69/ViViT/runs/l4vwgih0' target=\"_blank\">https://wandb.ai/olonok69/ViViT/runs/l4vwgih0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model)... Done. 5.1s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='331.903 MB of 331.909 MB uploaded\\r'), FloatProgress(value=0.999981093708994, max=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-field-10</strong> at: <a href='https://wandb.ai/olonok69/ViViT/runs/l4vwgih0' target=\"_blank\">https://wandb.ai/olonok69/ViViT/runs/l4vwgih0</a><br/> View project at: <a href='https://wandb.ai/olonok69/ViViT' target=\"_blank\">https://wandb.ai/olonok69/ViViT</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240915_091820-l4vwgih0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=PROJECT, job_type=\"models\"):\n",
    "  artifact = wandb.Artifact(\"ViViT-Fine-tuned\", type=\"model\")\n",
    "  artifact.add_dir(custom_path)\n",
    "  wandb.save(custom_path)\n",
    "  wandb.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788e5d9-9dd6-422a-8ea1-f6ef37b25f08",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d640fec-b0d0-481c-a1fe-28209f00dc2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file data/UCF_101_subset_val/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi number of Frames: 164\n",
      "Processing file data/UCF_101_subset_val/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g14_c05.avi number of Frames: 160\n",
      "Processing file data/UCF_101_subset_val/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g20_c04.avi number of Frames: 220\n",
      "Processing file data/UCF_101_subset_val/val/ApplyLipstick/v_ApplyLipstick_g10_c04.avi number of Frames: 247\n",
      "Processing file data/UCF_101_subset_val/val/ApplyLipstick/v_ApplyLipstick_g20_c04.avi number of Frames: 140\n",
      "Processing file data/UCF_101_subset_val/val/ApplyLipstick/v_ApplyLipstick_g25_c02.avi number of Frames: 151\n",
      "Processing file data/UCF_101_subset_val/val/Archery/v_Archery_g12_c03.avi number of Frames: 436\n",
      "Processing file data/UCF_101_subset_val/val/Archery/v_Archery_g18_c02.avi number of Frames: 291\n",
      "Processing file data/UCF_101_subset_val/val/Archery/v_Archery_g18_c06.avi number of Frames: 160\n",
      "Processing file data/UCF_101_subset_val/val/BabyCrawling/v_BabyCrawling_g09_c06.avi number of Frames: 119\n",
      "Processing file data/UCF_101_subset_val/val/BabyCrawling/v_BabyCrawling_g13_c05.avi number of Frames: 85\n",
      "Processing file data/UCF_101_subset_val/val/BabyCrawling/v_BabyCrawling_g21_c04.avi number of Frames: 189\n",
      "Processing file data/UCF_101_subset_val/val/BalanceBeam/v_BalanceBeam_g13_c02.avi number of Frames: 125\n",
      "Processing file data/UCF_101_subset_val/val/BalanceBeam/v_BalanceBeam_g16_c01.avi number of Frames: 124\n",
      "Processing file data/UCF_101_subset_val/val/BalanceBeam/v_BalanceBeam_g22_c02.avi number of Frames: 96\n",
      "Processing file data/UCF_101_subset_val/val/BandMarching/v_BandMarching_g17_c01.avi number of Frames: 125\n",
      "Processing file data/UCF_101_subset_val/val/BandMarching/v_BandMarching_g20_c04.avi number of Frames: 165\n",
      "Processing file data/UCF_101_subset_val/val/BandMarching/v_BandMarching_g23_c03.avi number of Frames: 262\n",
      "Processing file data/UCF_101_subset_val/val/BaseballPitch/v_BaseballPitch_g07_c02.avi number of Frames: 138\n",
      "Processing file data/UCF_101_subset_val/val/BaseballPitch/v_BaseballPitch_g09_c02.avi number of Frames: 75\n",
      "Processing file data/UCF_101_subset_val/val/BaseballPitch/v_BaseballPitch_g12_c01.avi number of Frames: 126\n",
      "Processing file data/UCF_101_subset_val/val/Basketball/v_Basketball_g10_c03.avi number of Frames: 116\n",
      "Processing file data/UCF_101_subset_val/val/Basketball/v_Basketball_g20_c02.avi number of Frames: 175\n",
      "Processing file data/UCF_101_subset_val/val/Basketball/v_Basketball_g20_c04.avi number of Frames: 120\n",
      "Processing file data/UCF_101_subset_val/val/BasketballDunk/v_BasketballDunk_g05_c01.avi number of Frames: 52\n",
      "Processing file data/UCF_101_subset_val/val/BasketballDunk/v_BasketballDunk_g05_c03.avi number of Frames: 56\n",
      "Processing file data/UCF_101_subset_val/val/BasketballDunk/v_BasketballDunk_g05_c05.avi number of Frames: 53\n",
      "Processing file data/UCF_101_subset_val/val/BenchPress/v_BenchPress_g11_c05.avi number of Frames: 114\n",
      "Processing file data/UCF_101_subset_val/val/BenchPress/v_BenchPress_g17_c02.avi number of Frames: 68\n",
      "Processing file data/UCF_101_subset_val/val/BenchPress/v_BenchPress_g17_c06.avi number of Frames: 117\n",
      "Min number frames 52\n"
     ]
    }
   ],
   "source": [
    "path_files_val = \"data/UCF_101_subset_val\"\n",
    "video_dict_val, class_labels_val = frames_convert_and_create_dataset_dictionary(path_files_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29146a59-a5b6-4b94-9830-1e9cfe8b3f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe18235b908a4f91b56e756faca285de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97049bd8ad104704b65b6e5dba2bdbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d793328443c24214ace77c9ab65e7614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_dataset = create_dataset(video_dict_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5e16c89-f1ef-47ef-81f0-a1026b5887e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/repos2/video/wandb/run-20240915_111302-4qjyva62</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olonok69/uncategorized/runs/4qjyva62' target=\"_blank\">light-sun-16</a></strong> to <a href='https://wandb.ai/olonok69/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olonok69/uncategorized' target=\"_blank\">https://wandb.ai/olonok69/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olonok69/uncategorized/runs/4qjyva62' target=\"_blank\">https://wandb.ai/olonok69/uncategorized/runs/4qjyva62</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ViViT-Fine-tuned:v0, 331.90MB. 3 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "Done. 0:0:5.7\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('olonok69/ViViT/ViViT-Fine-tuned:v0', type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b3e73cab-255f-4957-a773-314c78eb7684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/d/repos2/video/artifacts/ViViT-Fine-tuned:v0'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "399ea59e-bac7-497e-88c4-1e1c7d69c542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'pixel_values'],\n",
       "        num_rows: 27\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'pixel_values'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6cefb08b-9b23-42ff-8e7b-ded2c1fb6182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_handling import generate_all_files\n",
    "import os\n",
    "import numpy as np\n",
    "import av\n",
    "from pathlib import Path\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d085745f-44db-4fec-998a-3d4b5efb29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = val_dataset['train'].features['labels'].names\n",
    "config = VivitConfig.from_pretrained(artifact_dir)\n",
    "config.num_classes=len(labels)\n",
    "config.id2label = {str(i): c for i, c in enumerate(labels)}\n",
    "config.label2id = {c: str(i) for i, c in enumerate(labels)}\n",
    "config.num_frames=10\n",
    "config.video_size= [10, 224, 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b358c15f-150a-4f66-ac20-90ad32d91ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9ab8041-7364-41f0-aa83-385ebbf0da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VivitImageProcessor, VivitForVideoClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56e0debd-3e88-4913-9ecb-12e7aa4a352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "fine_tune_model = VivitForVideoClassification.from_pretrained(artifact_dir,config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3fc9d7ac-3d6f-4f0e-a213-461fe8915f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory =  \"data/UCF_101_subset_val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c5967620-32da-4263-8a18-695d9559506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file data/UCF_101_subset_val/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi True Label ApplyEyeMakeup, predicted label ApplyEyeMakeup\n",
      "file data/UCF_101_subset_val/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g14_c05.avi True Label ApplyEyeMakeup, predicted label ApplyEyeMakeup\n",
      "file data/UCF_101_subset_val/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g20_c04.avi True Label ApplyEyeMakeup, predicted label ApplyEyeMakeup\n",
      "file data/UCF_101_subset_val/val/ApplyLipstick/v_ApplyLipstick_g10_c04.avi True Label ApplyLipstick, predicted label ApplyLipstick\n",
      "file data/UCF_101_subset_val/val/ApplyLipstick/v_ApplyLipstick_g20_c04.avi True Label ApplyLipstick, predicted label ApplyLipstick\n",
      "file data/UCF_101_subset_val/val/ApplyLipstick/v_ApplyLipstick_g25_c02.avi True Label ApplyLipstick, predicted label ApplyLipstick\n",
      "file data/UCF_101_subset_val/val/Archery/v_Archery_g12_c03.avi True Label Archery, predicted label Archery\n",
      "file data/UCF_101_subset_val/val/Archery/v_Archery_g18_c02.avi True Label Archery, predicted label Archery\n",
      "file data/UCF_101_subset_val/val/Archery/v_Archery_g18_c06.avi True Label Archery, predicted label Archery\n",
      "file data/UCF_101_subset_val/val/BabyCrawling/v_BabyCrawling_g09_c06.avi True Label BabyCrawling, predicted label BabyCrawling\n",
      "file data/UCF_101_subset_val/val/BabyCrawling/v_BabyCrawling_g13_c05.avi True Label BabyCrawling, predicted label BabyCrawling\n",
      "file data/UCF_101_subset_val/val/BabyCrawling/v_BabyCrawling_g21_c04.avi True Label BabyCrawling, predicted label BabyCrawling\n",
      "file data/UCF_101_subset_val/val/BalanceBeam/v_BalanceBeam_g13_c02.avi True Label BalanceBeam, predicted label BalanceBeam\n",
      "file data/UCF_101_subset_val/val/BalanceBeam/v_BalanceBeam_g16_c01.avi True Label BalanceBeam, predicted label BalanceBeam\n",
      "file data/UCF_101_subset_val/val/BalanceBeam/v_BalanceBeam_g22_c02.avi True Label BalanceBeam, predicted label BalanceBeam\n",
      "file data/UCF_101_subset_val/val/BandMarching/v_BandMarching_g17_c01.avi True Label BandMarching, predicted label BandMarching\n",
      "file data/UCF_101_subset_val/val/BandMarching/v_BandMarching_g20_c04.avi True Label BandMarching, predicted label BandMarching\n",
      "file data/UCF_101_subset_val/val/BandMarching/v_BandMarching_g23_c03.avi True Label BandMarching, predicted label BandMarching\n",
      "file data/UCF_101_subset_val/val/BaseballPitch/v_BaseballPitch_g07_c02.avi True Label BaseballPitch, predicted label BaseballPitch\n",
      "file data/UCF_101_subset_val/val/BaseballPitch/v_BaseballPitch_g09_c02.avi True Label BaseballPitch, predicted label BaseballPitch\n",
      "file data/UCF_101_subset_val/val/BaseballPitch/v_BaseballPitch_g12_c01.avi True Label BaseballPitch, predicted label BaseballPitch\n",
      "file data/UCF_101_subset_val/val/Basketball/v_Basketball_g10_c03.avi True Label Basketball, predicted label Basketball\n",
      "file data/UCF_101_subset_val/val/Basketball/v_Basketball_g20_c02.avi True Label Basketball, predicted label Basketball\n",
      "file data/UCF_101_subset_val/val/Basketball/v_Basketball_g20_c04.avi True Label Basketball, predicted label Basketball\n",
      "file data/UCF_101_subset_val/val/BasketballDunk/v_BasketballDunk_g05_c01.avi True Label BasketballDunk, predicted label BasketballDunk\n",
      "file data/UCF_101_subset_val/val/BasketballDunk/v_BasketballDunk_g05_c03.avi True Label BasketballDunk, predicted label BasketballDunk\n",
      "file data/UCF_101_subset_val/val/BasketballDunk/v_BasketballDunk_g05_c05.avi True Label BasketballDunk, predicted label BasketballDunk\n",
      "file data/UCF_101_subset_val/val/BenchPress/v_BenchPress_g11_c05.avi True Label BenchPress, predicted label BenchPress\n",
      "file data/UCF_101_subset_val/val/BenchPress/v_BenchPress_g17_c02.avi True Label BenchPress, predicted label BenchPress\n",
      "file data/UCF_101_subset_val/val/BenchPress/v_BenchPress_g17_c06.avi True Label BenchPress, predicted label BenchPress\n"
     ]
    }
   ],
   "source": [
    "class_labels = []\n",
    "true_labels=[]\n",
    "predictions = []\n",
    "predictions_labels = []\n",
    "all_videos=[]\n",
    "video_files= []\n",
    "sizes = []\n",
    "for p in generate_all_files(Path(directory), only_files=True):\n",
    "    set_files = str(p).split(\"/\")[2] # train or test\n",
    "    cls = str(p).split(\"/\")[3] # class\n",
    "    file= str(p).split(\"/\")[4] # file name\n",
    "    #file name path\n",
    "    file_name= os.path.join(directory, set_files, cls, file)\n",
    "    true_labels.append(cls)   \n",
    "    # Process class\n",
    "    if cls not in class_labels:\n",
    "        class_labels.append(cls)\n",
    "    # process video File\n",
    "    container = av.open(file_name)\n",
    "    #print(f\"Processing file {file_name} number of Frames: {container.streams.video[0].frames}\")  \n",
    "    indices = sample_frame_indices(clip_len=10, frame_sample_rate=1,seg_len=container.streams.video[0].frames)\n",
    "    video = read_video_pyav(container=container, indices=indices)\n",
    "    inputs = image_processor(list(video), return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tune_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # model predicts one of the 400 Kinetics-400 classes\n",
    "    predicted_label = logits.argmax(-1).item()\n",
    "    prediction = fine_tune_model.config.id2label[str(predicted_label)]\n",
    "    predictions.append(prediction)\n",
    "    predictions_labels.append(predicted_label)\n",
    "    print(f\"file {file_name} True Label {cls}, predicted label {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c09aba34-e797-4d4a-ac6d-6d87c680fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dcdeaef8-256f-4878-95df-6d9624ee3f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "ApplyEyeMakeup       1.00      1.00      1.00         3\n",
      " ApplyLipstick       1.00      1.00      1.00         3\n",
      "       Archery       1.00      1.00      1.00         3\n",
      "  BabyCrawling       1.00      1.00      1.00         3\n",
      "   BalanceBeam       1.00      1.00      1.00         3\n",
      "  BandMarching       1.00      1.00      1.00         3\n",
      " BaseballPitch       1.00      1.00      1.00         3\n",
      "    Basketball       1.00      1.00      1.00         3\n",
      "BasketballDunk       1.00      1.00      1.00         3\n",
      "    BenchPress       1.00      1.00      1.00         3\n",
      "\n",
      "      accuracy                           1.00        30\n",
      "     macro avg       1.00      1.00      1.00        30\n",
      "  weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(true_labels, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4651eb-4877-4920-b0bf-de23831be7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be39adbc-a8c0-4601-8c8c-d685960b5f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
