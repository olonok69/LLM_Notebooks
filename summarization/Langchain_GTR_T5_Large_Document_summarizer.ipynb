{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/summarization/Langchain_GTR_T5_Large_Document_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3LFaHkF9AgK",
        "outputId": "f98c7b1b-57d6-4e73-80b8-55fc0b31aff1"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq torch==2.1 --progress-bar off\n",
        "!pip install -qqq transformers==4.34.0 --progress-bar off\n",
        "!pip install -qqq accelerate==0.23.0 --progress-bar off\n",
        "!pip install -qqq bitsandbytes==0.41.1 --progress-bar off\n",
        "!pip install sentence-transformers spacy langchain trl datasets pypdf -qqq --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip_uhu9L9kof"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "\n",
        "# Loaders\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Splitters\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Model\n",
        "\n",
        "\n",
        "# Embedding Support\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Summarizer we'll use for Map Reduce\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "# Data Science\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "#splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=30, tokens_per_chunk=512, model_name= \"sentence-transformers/gtr-t5-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6rUNtCzKsyR",
        "outputId": "42ce0f05-63b1-4ce0-aa47-a48afebc7f1c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDSb12ivAsnm"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "response = requests.get(\"https://www.gutenberg.org/cache/epub/64317/pg64317.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1X7_iELMA24V"
      },
      "outputs": [],
      "source": [
        "book_complete_text = response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVp1hwM7yGRf"
      },
      "outputs": [],
      "source": [
        "book_complete_text = book_complete_text[5:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKgdWQqCA4fl",
        "outputId": "9a1e0bd0-8eb8-4958-c0cd-052217312c88"
      },
      "outputs": [],
      "source": [
        "len(book_complete_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COC8KgjlA-IF"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/data/book.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB1lTJ1RA7e0"
      },
      "outputs": [],
      "source": [
        "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(book_complete_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxPVavEf9klX"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open(file_path, \"r\",  encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX6MHp-ky-_X"
      },
      "outputs": [],
      "source": [
        "text = text.replace('\\t', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHMtl9KgzVU0",
        "outputId": "94892aa9-3f4a-457d-b347-54abf17d1f5d"
      },
      "outputs": [],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKaIuJi9BKON"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \"\\t\"], chunk_size=12000, chunk_overlap=3000)\n",
        "\n",
        "docs = text_splitter.create_documents([text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaSxJgZWBKK7",
        "outputId": "3529bba3-2e0c-4359-ed13-6885b0dd8ade"
      },
      "outputs": [],
      "source": [
        "num_documents = len(docs)\n",
        "\n",
        "print (f\"Now our book is split up into {num_documents} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj0a9QspBKHH"
      },
      "outputs": [],
      "source": [
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vectors = embeddings.embed_documents([x.page_content for x in docs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnjkLByMBKDF",
        "outputId": "d31c0d51-4e27-4c31-a74a-1cb263c849d3"
      },
      "outputs": [],
      "source": [
        "len(vectors[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9WqZHJGEhJQ",
        "outputId": "4ef4c235-1e05-4a05-a4bd-ac195c2d32bb"
      },
      "outputs": [],
      "source": [
        "num_clusters = int(len(vectors) // 4)\n",
        "num_clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A82raZvaB2NY"
      },
      "outputs": [],
      "source": [
        "# Assuming 'embeddings' is a list or array of 768-dimensional embeddings\n",
        "\n",
        "# Choose the number of clusters, this can be adjusted based on the book's content.\n",
        "# I played around and found ~10 was the best.\n",
        "# Usually if you have 10 passages from a book you can tell what it's about\n",
        "num_clusters = 5 if num_clusters <=5 else num_clusters\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk5VRB78B2KG",
        "outputId": "80a20a2f-81a3-4707-a6f8-95dbbf4eedd3"
      },
      "outputs": [],
      "source": [
        "kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8VSz9TIB2HZ",
        "outputId": "b9109af7-3e2a-4d20-83e4-e8e4ce4544f9"
      },
      "outputs": [],
      "source": [
        "len(kmeans.labels_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "kjl4n0fHB2En",
        "outputId": "dc4205da-3886-4dcb-91b5-a35ae7757f25"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Taking out the warnings\n",
        "import warnings\n",
        "from warnings import simplefilter\n",
        "\n",
        "# Filter out FutureWarnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Perform t-SNE and reduce to 2 dimensions\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "reduced_data_tsne = tsne.fit_transform(np.array(vectors))\n",
        "\n",
        "# Plot the reduced data\n",
        "plt.scatter(reduced_data_tsne[:, 0], reduced_data_tsne[:, 1], c=kmeans.labels_)\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.title('Book Embeddings Clustered')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4vyHnUtB2B7"
      },
      "outputs": [],
      "source": [
        "# Find the closest embeddings to the centroids\n",
        "\n",
        "# Create an empty list that will hold your closest points\n",
        "closest_indices = []\n",
        "\n",
        "# Loop through the number of clusters you have\n",
        "for i in range(num_clusters):\n",
        "\n",
        "    # Get the list of distances from that particular cluster center\n",
        "    distances = np.linalg.norm(vectors - kmeans.cluster_centers_[i], axis=1)\n",
        "\n",
        "    # Find the list position of the closest one (using argmin to find the smallest distance)\n",
        "    closest_index = np.argmin(distances)\n",
        "\n",
        "    # Append that position to your closest indices list\n",
        "    closest_indices.append(closest_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MltvvttCDd48",
        "outputId": "c345d35d-7048-4018-e9f7-838d673b0f11"
      },
      "outputs": [],
      "source": [
        "selected_indices = sorted(closest_indices)\n",
        "selected_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOzbh9bSM4zJ"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import transformers\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "um-qU1-KRO6e",
        "outputId": "27913671-004d-486b-a0d9-4e4a190d0735"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f88848b3dfb049dfa5018b4f4773972e",
            "1b8a272f36904afbae4a3f273870e57b",
            "01db4283a4444469a424140d7c040fff",
            "c86623eac8b94feb9d9c52f7c2cee4cf",
            "21b7a30d2d16413a99a545c9e4633101",
            "2b6024f755084993a5c22b6769d81b3a",
            "b02bf6e580504d13abcd60eafe17ef81",
            "258d6b6592ac4c3883bc49cc5997cee2",
            "5688b7ce530f4d0a93a0592adb77aa80",
            "cfb6e04d18b9421fbd0f5bdb895c3b8e",
            "71f13753fcb244c28e4a21ee70614d72"
          ]
        },
        "id": "ww7zGIvcZznr",
        "outputId": "c38f4e45-d43e-4e07-fb3b-a68cef1a8af1"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", torch_dtype=torch.float16).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M48uBi6kVgKJ",
        "outputId": "92f1e52a-6add-4f65-956f-e5407dd32653"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, GPT2LMHeadModel, AutoTokenizer\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xG8htD3LKxaJ"
      },
      "outputs": [],
      "source": [
        "#import evaluate\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
        "\n",
        "# dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")\n",
        "model = (\n",
        "    LongT5ForConditionalGeneration.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")#,  torch_dtype=torch.float16)\n",
        "    .to(\"cuda\")\n",
        "\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n",
        "\n",
        "\n",
        "# def generate_answers(batch):\n",
        "#     inputs_dict = tokenizer(\n",
        "#         batch[\"article\"], max_length=16384, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "#     )\n",
        "#     input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
        "#     attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n",
        "#     output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=512, num_beams=2)\n",
        "#     batch[\"predicted_abstract\"] = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "#     return batch\n",
        "\n",
        "\n",
        "# result = dataset.map(generate_answer, batched=True, batch_size=2)\n",
        "# rouge = evaluate.load(\"rouge\")\n",
        "# rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwE1DMe9FaEt"
      },
      "outputs": [],
      "source": [
        "# define tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\", model_max_length = 1024)\n",
        "\n",
        "# # define model\n",
        "# # model = AutoModelForSeq2SeqLM.from_pretrained(\"gpt2-medium\")\n",
        "# model = GPT2LMHeadModel.from_pretrained('gpt2-medium', pad_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcvV2EzKV7vM",
        "outputId": "7928a691-42f7-439d-c995-cf7e5fbcc648"
      },
      "outputs": [],
      "source": [
        "text_summarization_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"summarization\",\n",
        "    max_length = 512,\n",
        "    do_sample=True,\n",
        "    top_k =5,\n",
        "    num_beams = 4,\n",
        "    device_map= \"auto\",\n",
        "\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=text_summarization_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLFVjgvIC3P0"
      },
      "outputs": [],
      "source": [
        "map_prompt = \"\"\"\n",
        "You will be given a single passage of a book. This section will be enclosed in triple backticks (```)\n",
        "Your goal is to give a summary of this section so that a reader will have a full understanding of what happened.\n",
        "Your response should be at least three paragraphs and fully encompass what was said in the passage.\n",
        "\n",
        "```{text}```\n",
        "FULL SUMMARY:\n",
        "\"\"\"\n",
        "map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKDpbQvUC3Mi"
      },
      "outputs": [],
      "source": [
        "map_chain = load_summarize_chain(llm=llm,\n",
        "                             chain_type=\"stuff\",\n",
        "                             prompt=map_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJZAmYyADYgo"
      },
      "outputs": [],
      "source": [
        "selected_docs = [docs[doc] for doc in selected_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izaKwo_kDYJ4",
        "outputId": "ce12208c-951d-4695-cd1d-1e3a7ce214bf"
      },
      "outputs": [],
      "source": [
        "# Make an empty list to hold your summaries\n",
        "summary_list = []\n",
        "\n",
        "# Loop through a range of the lenght of your selected docs\n",
        "for i, doc in enumerate(selected_docs):\n",
        "\n",
        "    # Go get a summary of the chunk\n",
        "    chunk_summary = map_chain.run([doc])\n",
        "\n",
        "    # Append that summary to your list\n",
        "    summary_list.append(chunk_summary)\n",
        "\n",
        "    print (f\"Summary #{i} (chunk #{selected_indices[i]}) - Preview: {chunk_summary[:500]} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6tvLvS3DX8Q",
        "outputId": "aae360ab-c1a1-43ad-8985-2406880e8ced"
      },
      "outputs": [],
      "source": [
        "summaries = \"\\n\".join(summary_list)\n",
        "\n",
        "# Convert it back to a document\n",
        "summaries = Document(page_content=summaries)\n",
        "\n",
        "print (f\"Your total summary has {llm.get_num_tokens(summaries.page_content)} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4py-VHMDXY5",
        "outputId": "380bfd8a-0a88-41e5-afa6-6012cab136e9"
      },
      "outputs": [],
      "source": [
        "print(summaries.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g0xINMpC3Iz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U82Wn0PSzrnZ"
      },
      "outputs": [],
      "source": [
        "# Loaders\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Splitters\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Model\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Embedding Support\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "\n",
        "# Summarizer we'll use for Map Reduce\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "# Data Science\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRw_Q90Hzrj3"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \"\\t\"], chunk_size=5000, chunk_overlap=1000)\n",
        "\n",
        "docs = text_splitter.create_documents([text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXUh2i5o0Q82",
        "outputId": "39826636-40dc-4846-ccc9-83660c22003e"
      },
      "outputs": [],
      "source": [
        "num_documents = len(docs)\n",
        "\n",
        "print (f\"Now our book is split up into {num_documents} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7AbxHi70Q25"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L24cF7PW0QzK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miFy9gBUzrf9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1YwAPe_WGVB"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Map\n",
        "map_template = \"\"\"The following is a set of documents\n",
        "{docs}\n",
        "Based on this list of docs, please identify the main themes\n",
        "Helpful Answer:\"\"\"\n",
        "map_prompt = PromptTemplate.from_template(map_template)\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe0xpHrfWGSK"
      },
      "outputs": [],
      "source": [
        "# Reduce\n",
        "reduce_template = \"\"\"The following is set of summaries:\n",
        "{docs}\n",
        "Take these and distill it into a final, consolidated summary of the main themes.\n",
        "Helpful Answer:\"\"\"\n",
        "reduce_prompt = PromptTemplate.from_template(reduce_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDINUCLWWGPP",
        "outputId": "7610c763-e966-43fb-b21f-1a788f0c5812"
      },
      "outputs": [],
      "source": [
        "ChatPromptTemplate(input_variables=['docs'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['docs'], template='The following is a set of documents:\\n{docs}\\nBased on this list of docs, please identify the main themes \\nHelpful Answer:'))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5GGAIXjjV92"
      },
      "outputs": [],
      "source": [
        "# Run chain\n",
        "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
        ")\n",
        "\n",
        "# Combines and iteratively reduces the mapped documents\n",
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    # This is final chain that is called.\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    # If documents exceed context for `StuffDocumentsChain`\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    # The maximum number of tokens to group documents into.\n",
        "    token_max=1024,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOLMHDF7jV6y"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "gGQUP2ZOEvKx",
        "outputId": "23336373-1434-4c41-ec78-843c81047944"
      },
      "outputs": [],
      "source": [
        "text[:1300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn0SwCEwvfAj",
        "outputId": "8e227048-d350-4118-a7f2-4a256ecf93cb"
      },
      "outputs": [],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXwsYrpgctjI",
        "outputId": "03aed8a2-4a90-4261-80db-5acb0cc96e6d"
      },
      "outputs": [],
      "source": [
        "num_tokens = llm.get_num_tokens(text)\n",
        "\n",
        "print (f\"This book has {num_tokens} tokens in it\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReNYi2UG9IUE"
      },
      "outputs": [],
      "source": [
        "#%pip install --upgrade --quiet  tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWw5tszAaidJ"
      },
      "outputs": [],
      "source": [
        "# Loaders\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Splitters\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Model\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Embedding Support\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Summarizer we'll use for Map Reduce\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "# Data Science\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88UiAMT_8sBw"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=1200, chunk_overlap=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m2NyppqaiaF"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \" \"], chunk_size=8192, chunk_overlap=30, length_function=len,)\n",
        "\n",
        "docs = text_splitter.create_documents([text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-05Q_Up1aiWq",
        "outputId": "368b6b65-0f8f-4625-db36-00672c38328e"
      },
      "outputs": [],
      "source": [
        "num_documents = len(docs)\n",
        "\n",
        "print (f\"Now our book is split up into {num_documents} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEjbP96SlI04"
      },
      "outputs": [],
      "source": [
        "# Combining documents by mapping a chain over them, then combining results\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # Map chain\n",
        "    llm_chain=map_chain,\n",
        "    # Reduce chain\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # The variable name in the llm_chain to put the documents in\n",
        "    document_variable_name=\"docs\",\n",
        "    # Return the results of the map steps in the output\n",
        "    return_intermediate_steps=False,\n",
        "    verbose = True,\n",
        ")\n",
        "\n",
        "\n",
        "split_docs = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O4P09jo8lIqs",
        "outputId": "205e1ba1-3d42-40a0-fa8b-1dd0696c04a1"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "output = map_reduce_chain.invoke(split_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "IB5yiJ743p9W",
        "outputId": "ff7e15b4-be10-45b3-9e6b-3bf56f315a51"
      },
      "outputs": [],
      "source": [
        "output['output_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7qTq5p_QcnX"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define prompt\n",
        "prompt_template = \"\"\"Write a concise summary of the following:\n",
        "\"{text}\"\n",
        "CONCISE SUMMARY:\"\"\"\n",
        "prompt = PromptTemplate.from_template(prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PTV6lH-N4sk",
        "outputId": "fd7d8642-efe5-4d61-e7b0-10e95e285a76"
      },
      "outputs": [],
      "source": [
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Define StuffDocumentsChain\n",
        "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
        "\n",
        "\n",
        "print(stuff_chain.run(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu1UAOQRc8ZF",
        "outputId": "33a80dc1-f10d-4729-e3b2-775cb2c53d3c"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "vectors = embeddings.embed_documents([x.page_content for x in docs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwEouf2hdCt4",
        "outputId": "5e69ae60-52e1-4657-e344-153b4366d6da"
      },
      "outputs": [],
      "source": [
        "len(vectors[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO5Ublwm6XbE",
        "outputId": "d4d26551-dce4-4399-a48e-ed76c5588416"
      },
      "outputs": [],
      "source": [
        "vectors[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwzwlQKddCqJ"
      },
      "outputs": [],
      "source": [
        "# Assuming 'embeddings' is a list or array of 768-dimensional embeddings\n",
        "\n",
        "# Choose the number of clusters, this can be adjusted based on the book's content.\n",
        "# I played around and found ~10 was the best.\n",
        "# Usually if you have 10 passages from a book you can tell what it's about\n",
        "num_clusters = 21\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgH9qi4ydCmz",
        "outputId": "13a98789-ca08-4129-ca09-9920c1fdc040"
      },
      "outputs": [],
      "source": [
        "kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGJ0_stQ-aVF",
        "outputId": "54dc73dc-5ecb-445c-d534-58c925bb29ac"
      },
      "outputs": [],
      "source": [
        "len(kmeans.labels_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "OZGydweGeAlv",
        "outputId": "6a9538b9-a344-48f8-bce7-646875c644f4"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Taking out the warnings\n",
        "import warnings\n",
        "from warnings import simplefilter\n",
        "\n",
        "# Filter out FutureWarnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Perform t-SNE and reduce to 2 dimensions\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "reduced_data_tsne = tsne.fit_transform(np.array(vectors))\n",
        "\n",
        "# Plot the reduced data\n",
        "plt.scatter(reduced_data_tsne[:, 0], reduced_data_tsne[:, 1], c=kmeans.labels_)\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.title('Book Embeddings Clustered')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6DOKSe1eRaF"
      },
      "outputs": [],
      "source": [
        "# Find the closest embeddings to the centroids\n",
        "\n",
        "# Create an empty list that will hold your closest points\n",
        "closest_indices = []\n",
        "\n",
        "# Loop through the number of clusters you have\n",
        "for i in range(num_clusters):\n",
        "\n",
        "    # Get the list of distances from that particular cluster center\n",
        "    distances = np.linalg.norm(vectors - kmeans.cluster_centers_[i], axis=1)\n",
        "\n",
        "    # Find the list position of the closest one (using argmin to find the smallest distance)\n",
        "    closest_index = np.argmin(distances)\n",
        "\n",
        "    # Append that position to your closest indices list\n",
        "    closest_indices.append(closest_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5KD4VvreZiQ",
        "outputId": "32e192d5-cd86-4279-e50e-b7fcb2b86a46"
      },
      "outputs": [],
      "source": [
        "selected_indices = sorted(closest_indices)\n",
        "selected_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7VRN1gJeZfK"
      },
      "outputs": [],
      "source": [
        "map_prompt = \"\"\"\n",
        "You will be given a single passage of a book. This section will be enclosed in triple backticks (```)\n",
        "Your response should be at least three paragraphs and fully encompass what was said in the passage.\n",
        "\n",
        "```{text}```\n",
        "FULL SUMMARY:\n",
        "\"\"\"\n",
        "map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyS0OUyLeZb8"
      },
      "outputs": [],
      "source": [
        "map_chain = load_summarize_chain(llm=llm,\n",
        "                             chain_type=\"stuff\",\n",
        "                             prompt=map_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0DVEitpevuN"
      },
      "outputs": [],
      "source": [
        "selected_docs = [docs[doc] for doc in selected_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0bvP4DFIVUr",
        "outputId": "1de93c27-a80d-41e3-aefc-e5e3c0132c6d"
      },
      "outputs": [],
      "source": [
        "selected_docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "mJER-0Y5evrs",
        "outputId": "9ce033dd-3b4f-457b-b40c-8eb3135282ae"
      },
      "outputs": [],
      "source": [
        "# Make an empty list to hold your summaries\n",
        "summary_list = []\n",
        "\n",
        "# Loop through a range of the lenght of your selected docs\n",
        "for i, doc in enumerate(selected_docs):\n",
        "\n",
        "    # Go get a summary of the chunk\n",
        "    chunk_summary = map_chain.run([doc])\n",
        "\n",
        "    # Append that summary to your list\n",
        "    summary_list.append(chunk_summary)\n",
        "\n",
        "    print (f\"Summary #{i} (chunk #{selected_indices[i]}) - Preview: {chunk_summary[-250:]} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aV-1Zg39EKwW",
        "outputId": "00200262-5680-4edd-b52b-289318b8b930"
      },
      "outputs": [],
      "source": [
        "summary_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc74I-PFEOCm",
        "outputId": "764af432-b485-4df0-8773-4338a5d6465c"
      },
      "outputs": [],
      "source": [
        "len(summary_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jZZILejevoi",
        "outputId": "c6d0a933-20e4-4da5-a246-ebd36005cb90"
      },
      "outputs": [],
      "source": [
        "summaries = \"\\n\".join(summary_list)\n",
        "\n",
        "# Convert it back to a document\n",
        "summaries = Document(page_content=summaries)\n",
        "\n",
        "print (f\"Your total summary has {llm.get_num_tokens(summaries.page_content)} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RB2eHR0khaRN"
      },
      "outputs": [],
      "source": [
        "text_summarization_pipeline2 = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"summarization\",\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "      max_new_tokens=1000,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "\n",
        ")\n",
        "llm2 = HuggingFacePipeline(pipeline=text_summarization_pipeline2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGymo8I-evlc"
      },
      "outputs": [],
      "source": [
        "combine_prompt = \"\"\"\n",
        "You will be given a series of summaries from a book. The summaries will be enclosed in triple backticks (```)\n",
        "Your goal is to give a verbose summary of what happened in the story.\n",
        "The reader should be able to grasp what happened in the book.\n",
        "\n",
        "```{text}```\n",
        "VERBOSE SUMMARY:\n",
        "\"\"\"\n",
        "combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGygLaEphyB5"
      },
      "outputs": [],
      "source": [
        "reduce_chain = load_summarize_chain(llm=llm2,\n",
        "                             chain_type=\"stuff\",\n",
        "                             prompt=combine_prompt_template,\n",
        "                             verbose=True # Set this to true if you want to see the inner workings\n",
        "                                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbc9kBYDhx95",
        "outputId": "814928db-ae5b-492e-afc6-e081f54cbcd4"
      },
      "outputs": [],
      "source": [
        "output = reduce_chain.run([summaries])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdEWkrN9hx6l",
        "outputId": "160367d3-b798-4daa-87d3-692b20b96271"
      },
      "outputs": [],
      "source": [
        "print (output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMizxwy2T5BwVLKA5Q0OGwQ",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "mount_file_id": "1eZ5X_ZyoqE3tO1WRAGZkIoojBLchKWar",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01db4283a4444469a424140d7c040fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_258d6b6592ac4c3883bc49cc5997cee2",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5688b7ce530f4d0a93a0592adb77aa80",
            "value": 3
          }
        },
        "1b8a272f36904afbae4a3f273870e57b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b6024f755084993a5c22b6769d81b3a",
            "placeholder": "​",
            "style": "IPY_MODEL_b02bf6e580504d13abcd60eafe17ef81",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "21b7a30d2d16413a99a545c9e4633101": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "258d6b6592ac4c3883bc49cc5997cee2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b6024f755084993a5c22b6769d81b3a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5688b7ce530f4d0a93a0592adb77aa80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71f13753fcb244c28e4a21ee70614d72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b02bf6e580504d13abcd60eafe17ef81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c86623eac8b94feb9d9c52f7c2cee4cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfb6e04d18b9421fbd0f5bdb895c3b8e",
            "placeholder": "​",
            "style": "IPY_MODEL_71f13753fcb244c28e4a21ee70614d72",
            "value": " 3/3 [00:53&lt;00:00, 17.75s/it]"
          }
        },
        "cfb6e04d18b9421fbd0f5bdb895c3b8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f88848b3dfb049dfa5018b4f4773972e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b8a272f36904afbae4a3f273870e57b",
              "IPY_MODEL_01db4283a4444469a424140d7c040fff",
              "IPY_MODEL_c86623eac8b94feb9d9c52f7c2cee4cf"
            ],
            "layout": "IPY_MODEL_21b7a30d2d16413a99a545c9e4633101"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
